---

layout:     post
title:      损失函数
subtitle:   Loss Function
date:       2020-02-21
author:     bjmsong
header-img: img/machineLearning/machineLearning.png
catalog: true
tags:
    - 机器学习
---



### 监督学习学习过程

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/loss/High-level-training-process.jpg) 
</li> 
</ul>

- Forward pass 
  - the training data is fed into the machine learning model 
- Loss :
  - compare between some actual targets and predicted targets
  - the lower the loss, the more the set of targets and the set of predictions resemble each other
  - the more they resemble each other, the better the machine learning model performs.
- Backward pass
  - propagating the error backwards to the model structure, such as the model’s weights
  - many ways for optimizing the mode
    - gradient descent based methods
      - back propagation ：neural networks
    - quadratic optimization ：SVM





### 损失函数

- 模型优化的目标函数，通过优化损失函数得到模型的参数
- 针对不同的问题需要选取合适的损失函数

#### 分类问题

- 0-1 loss
  
  $$
  L(Y,f(X)) = \left\{
  \begin{aligned}
  1, Y\neq f(X) \\
  0, Y= f(X)
  \end{aligned}
  \right.
  $$
  
  - 非凸，非光滑，很难对其进行优化


- Hinge loss
  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/loss/hinge loss.png) 
  </li>
  </ul>
  
  - 通常被用于最大间隔算法，最大间隔算法是SVM用到的重要算法
  
  - 当
    $$
    Yf(X)\geq 1
    $$
  
    该函数不对其做任何惩罚。
  
  - Hinge loss在Yf(X)=1处不可导，因此不能用梯度下降法进行优化，而是使用次梯度下降法。因为其图像像打开的一本书，所以也叫合页损失。
  
  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/loss/hinge loss.jpeg) 
  </li> 
  </ul> 
  
- Squared hinge

  - The squared hinge loss is like the hinge formula displayed above, but then the max() function output is squared
  - more sensitive to outliers
  - differentiable

- Categorical / multiclass hinge

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/loss/multiclass hinge.png) 
  </li> 
  </ul> 

- Binary cross entropy(交叉熵)，也叫logloss

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/loss/binary cross entropy.png) 
  </li> 
  </ul> 

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/loss/bce-1.png) 
  </li> 
  </ul> 

- Categorical cross entropy

- Kullback-Leibler divergence(KL散度)

  - comparison between two probability distributions

- Logistic loss

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/loss/logistic loss.png) 
  </li> 
  </ul> 
  
  - 该函数处处光滑，因此可以用梯度下降法进行优化。但是，该损失函数对所有的样本点都有所惩罚，因此对异常值更敏感。

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/loss/二分类问题的损失函数.png) 
</li> 
</ul> 



#### 回归问题

- Mean Absolute Error (MAE，L1 Loss)

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/loss/mae.png) 
  </li> 
  </ul> 

  - 缺点
    - 在x=0处不可导，梯度计算复杂，需要借助线性规划求解
    - gradients are continuously large (Grover, 2019)，easy to overshoot the minimum continously, finding a suboptimal model
      - Consider Huber loss (more below) if you face this problem
    - if your average error is very small, it may be better to use the Mean Squared Error
      - 误差接近0的时候不平滑，较少使用
    - 只关注误差大小，不关注误差方向

- **Mean Squared Error (MSE)**

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/loss/mse.png) 
  </li> 
  </ul> 

  - 处处可导：优化更容易
  - large errors introduce a much larger cost than smaller errors
    - 优点：when your errors are small, because optimization is then advanced
    - 缺点：对异常值过于敏感，只关注误差大小，不关注误差方向

- Mean Absolute Percentage Error（MAPE）

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/loss/mape.png) 
  </li> 
  </ul> 

- Root Mean Squared Error(RMSE，L2 Loss)

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/loss/rmse.png) 
  </li> 
  </ul> 

- **Logcosh**

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/loss/logcosh.png) 
  </li> 
  </ul> 

  - log(cosh(x)) is approximately equal to (x ** 2) / 2 for small x and to abs(x) - log(2) for large x
  - This means that `logcosh` works mostly like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect prediction

- **Huber loss**

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/loss/huber loss.png) 
  </li> 
  </ul> 

  - Huber loss approaches MSE when 𝛿 ~ 0 and MAE when 𝛿 ~ ∞ (large numbers.)

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/loss/回归问题的损失函数.png) 
</li> 
</ul> 





### loss function 和 metrcis 的区别

- loss是模型优化的目标
  - 可导 ：做梯度下降
- metrics 不需要可导，更接近业务目标



### 参考资料

- https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/

- 《百面机器学习》

- https://www.analyticsvidhya.com/blog/2019/08/detailed-guide-7-loss-functions-machine-learning-python-code/

- https://www.cnblogs.com/wkslearner/p/9833191.html

- [线性回归采用平方误差作为loss的解释](https://mp.weixin.qq.com/s?__biz=MzU0NzAxNTYyMQ==&mid=2247483916&idx=1&sn=96ea01f6bc4c018a11dfc677b1c2ed81&chksm=fb559ca6cc2215b03263442a7cccc39b5afd5c1a3f61ecc188e653686f9d77ecabdc2e4d338c&mpshare=1&scene=1&srcid=1217RBU9Qs1IY8OtPTXkMQvF#rd)

- https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23

- https://www.jianshu.com/p/ac26866e81bc

- https://algorithmia.com/blog/introduction-to-loss-functions

- http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/

- https://mp.weixin.qq.com/s?__biz=MzI1MzY0MzE4Mg==&mid=2247485541&idx=1&sn=b914a4ba2250597a4bc48f45401b0c45&chksm=e9d018a4dea791b20ce2a6752a2f4bb2ce222713d7cade58a40974a3ddd78e78adc53a73ff86&mpshare=1&scene=1&srcid=&sharer_sharetime=1572829222282&sharer_shareid=49581f7bdbef8664715f595bc62d7044&key=8a4bbb55c6c79ce68e2a634f845f562fd0b6fbf82bb629c5d5df10d62d40127c3f82024cec1aeb41435cfc5df399eaad37653eccd4de1ee6f89c9e333590441d827e165572701cb224ede4157035ac41&ascene=1&uin=MjM1OTMwMzkwMA%3D%3D&devicetype=Windows+7&version=62070152&lang=zh_CN&pass_ticket=LP%2BeJbVSoUnpFGrFF0jkMhYuRBFEXQ4jgRZzE3Lc3uQG2Hh9YJp6FpIsB6GCp5dU

  

