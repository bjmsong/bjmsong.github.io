---

layout:     post
title:      æŸå¤±å‡½æ•°
subtitle:   Loss Function
date:       2020-02-21
author:     bjmsong
header-img: img/machineLearning/machineLearning.png
catalog: true
tags:
    - æœºå™¨å­¦ä¹ 
---



## å­¦ä¹ è¿‡ç¨‹



## ç›‘ç£å­¦ä¹ å­¦ä¹ è¿‡ç¨‹

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/High-level-training-process.jpg) 
</li> 
</ul>

- Forward pass 
  - the training data is fed into the machine learning model 
- Loss :
  - compare between some actual targets and predicted targets
  - the lower the loss, the more the set of targets and the set of predictions resemble each other
  - the more they resemble each other, the better the machine learning model performs.
- Backward pass
  - propagating the error backwards to the model structure, such as the modelâ€™s weights
  - many ways for optimizing the mode
    - gradient descent based methods
      - back propagation ï¼šneural networks
    - quadratic optimization ï¼šSVM





## æŸå¤±å‡½æ•°

- æ¨¡å‹ä¼˜åŒ–çš„ç›®æ ‡å‡½æ•°ï¼Œé€šè¿‡ä¼˜åŒ–æŸå¤±å‡½æ•°å¾—åˆ°æ¨¡å‹çš„å‚æ•°
- é’ˆå¯¹ä¸åŒçš„é—®é¢˜éœ€è¦é€‰å–åˆé€‚çš„æŸå¤±å‡½æ•°

### äºŒåˆ†ç±»é—®é¢˜

- 0-1 loss
  $$
  L(Y,f(X)) = \left\{
  \begin{aligned}
  1, Y\neq f(X) \\
  0, Y= f(X)
  \end{aligned}
  \right.
  $$
  

  - éå‡¸ï¼Œéå…‰æ»‘ï¼Œå› æ­¤éœ€è¦æ‰¾ä»£ç†æŸå¤±å‡½æ•°

- Hinge loss
  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/hinge loss.png) 
</li> 
  </ul> 
  
  - é€šå¸¸è¢«ç”¨äºæœ€å¤§é—´éš”ç®—æ³•ï¼Œæœ€å¤§é—´éš”ç®—æ³•æ˜¯SVMç”¨åˆ°çš„é‡è¦ç®—æ³•
  
  - å½“
  $$
  Yf(X)\geq 1
  $$
  è¯¥å‡½æ•°ä¸å¯¹å…¶åšä»»ä½•æƒ©ç½šã€‚Hinge lossåœ¨Yf(X)=1å¤„ä¸å¯å¯¼ï¼Œå› æ­¤ä¸èƒ½ç”¨æ¢¯åº¦ä¸‹é™æ³•è¿›è¡Œä¼˜åŒ–ï¼Œè€Œæ˜¯ä½¿ç”¨æ¬¡æ¢¯åº¦ä¸‹é™æ³•ã€‚å› ä¸ºå…¶å›¾åƒåƒæ‰“å¼€çš„ä¸€æœ¬ä¹¦ï¼Œæ‰€ä»¥ä¹Ÿå«åˆé¡µæŸå¤±ã€‚
  
  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/hinge loss.jpeg) 
  </li> 
  </ul> 
  
- Squared hinge

  - The squared hinge loss is like the hinge formula displayed above, but then the max() function output is squared
  - more sensitive to outliers
  - differentiable

- Categorical / multiclass hinge

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/multiclass hinge.png) 
  </li> 
  </ul> 

- Binary cross entropy(äº¤å‰ç†µ)

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/binary cross entropy.png) 
  </li> 
  </ul> 

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/bce-1.png) 
  </li> 
  </ul> 

- Categorical cross entropy

- Kullback-Leibler divergence(KLæ•£åº¦)

  - comparison between two probability distributions

  

- Logistic loss
  $$
  L(Y,f(X)) =
  $$

  è¯¥å‡½æ•°å¤„å¤„å…‰æ»‘ï¼Œå› æ­¤å¯ä»¥ç”¨æ¢¯åº¦ä¸‹é™æ³•è¿›è¡Œä¼˜åŒ–ã€‚ä½†æ˜¯ï¼Œè¯¥æŸå¤±å‡½æ•°å¯¹æ‰€æœ‰çš„æ ·æœ¬ç‚¹éƒ½æœ‰æ‰€æƒ©ç½šï¼Œå› æ­¤å¯¹å¼‚å¸¸å€¼æ›´æ•æ„Ÿã€‚

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/äºŒåˆ†ç±»é—®é¢˜çš„æŸå¤±å‡½æ•°.png) 
</li> 
</ul> 



### å›å½’é—®é¢˜

- Mean Absolute Error (MAEï¼ŒL1 Loss)

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/mae.png) 
  </li> 
  </ul> 

  - ç¼ºç‚¹
    - åœ¨x=0å¤„ä¸å¯å¯¼
    - gradients are continuously large (Grover, 2019)ï¼Œeasy to overshoot the minimum continously, finding a suboptimal model
      - Consider Huber loss (more below) if you face this problem
    - if your average error is very small, it may be better to use the Mean Squared Error

- **Mean Squared Error (MSE)**

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/mse.png) 
  </li> 
  </ul> 

  - å¤„å¤„å¯å¯¼ï¼šä¼˜åŒ–æ›´å®¹æ˜“
  - large errors introduce a much larger cost than smaller errors
    - ä¼˜ç‚¹ï¼šwhen your errors are small, because optimization is then advanced
    - ç¼ºç‚¹ï¼šå¯¹å¼‚å¸¸å€¼è¿‡äºæ•æ„Ÿ

- Mean Absolute Percentage Errorï¼ˆMAPEï¼‰

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/mape.png) 
  </li> 
  </ul> 

- Root Mean Squared Error(RMSEï¼ŒL2 Loss)

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/rmse.png) 
  </li> 
  </ul> 

- **Logcosh**

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/logcosh.png) 
  </li> 
  </ul> 

  - log(cosh(x)) is approximately equal to (x ** 2) / 2 for small x and to abs(x) - log(2) for large x
  - This means that `logcosh` works mostly like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect prediction

- **Huber loss**

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/huber loss.png) 
  </li> 
  </ul> 

  - Huber loss approaches MAE when ğ›¿ ~ 0 and MSE when ğ›¿ ~ âˆ (large numbers.)

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/å›å½’é—®é¢˜çš„æŸå¤±å‡½æ•°.png) 
</li> 
</ul> 



## å‚è€ƒèµ„æ–™

- https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/

- ã€Šç™¾é¢æœºå™¨å­¦ä¹ ã€‹

- https://www.analyticsvidhya.com/blog/2019/08/detailed-guide-7-loss-functions-machine-learning-python-code/

- https://www.cnblogs.com/wkslearner/p/9833191.html

  

  

