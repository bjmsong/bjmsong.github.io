---
layout:     post
title:      Caffe源码解析之二
subtitle:   
date:       2023-01-05
author:     bjmsong
header-img: img/caffe/coffee.jpg
catalog: true
tags:
    - 深度学习框架
---

## Layer

### 源码文件

- `include/caffe: layer_factory.hpp,layer.hpp, layer/*.hpp`
- `src/caffe: layer_factory.cpp,layer.cpp, layer/*.cpp,layer/*.cu` 



### 工厂模式

<ul> 
<li markdown="1">
layer.hpp中的Layer类是总的产品标准，使用virtual修饰函数，layer文件夹中的PoolingLayer、ConcatLayer等类继承Layer类，是Layer类的不同实现。然后layer_factory.hpp中的LayerRegistry类实现了Layer的注册，根据层的名称然后new对应的类返回Layer类型。至此，工厂模式流程完成。优点是实例化不同的Layer类时，具有统一的接口。
![]({{site.baseurl}}/img/caffe/factory.png) 
</li> 
</ul> 



### `LayerRegisterer`

- 定义了两个类型别名：
  - `Creator`: 函数指针，输入是`LayerParameter`，返回是`shared_ptr<Layer<Dtype> >`
  - `CreatorRegistry`:  即注册表，是一个字典，key是Layer的名称，value是`Creator`

```c++
 public:
  typedef shared_ptr<Layer<Dtype> > (*Creator)(const LayerParameter&);
  typedef std::map<string, Creator> CreatorRegistry;
```



#### 注册`Layer`

有两种方式，一种是`layer`可以由其构造函数直接生成，那么可以在具体`layer`的源文件中（例如`clip.cpp`）,  使用宏：

```
REGISTER_LAYER_CLASS(clip)
```

`REGISTER_LAYER_CLASS`定义了一个`Creator_clipLayer`的函数，然后调用宏`REGISTER_LAYER_CREATOR`。

```c++
#define REGISTER_LAYER_CLASS(type)                                             \
  template <typename Dtype>                                                    \
  shared_ptr<Layer<Dtype> > Creator_##type##Layer(const LayerParameter& param) \
  {                                                                            \
    return shared_ptr<Layer<Dtype> >(new type##Layer<Dtype>(param));           \
  }                                                                            \
  REGISTER_LAYER_CREATOR(type, Creator_##type##Layer)
```

`REGISTER_LAYER_CREATOR`实例化`LayerRegisterer`对象。`LayerRegisterer`的构造函数调用`LayerRegistry`的`AddCreator`方法。

```c++
#define REGISTER_LAYER_CREATOR(type, creator)                                  \
  static LayerRegisterer<float> g_creator_f_##type(#type, creator<float>);     \
  static LayerRegisterer<double> g_creator_d_##type(#type, creator<double>) 

template <typename Dtype>
class LayerRegisterer {
 public:
  LayerRegisterer(const string& type,
                  shared_ptr<Layer<Dtype> > (*creator)(const LayerParameter&)) {
    // LOG(INFO) << "Registering layer type: " << type;
    LayerRegistry<Dtype>::AddCreator(type, creator);
  }
};
```

`AddCreator`方法调用`Registry`方法，`Registry`方法会返回一个静态成员变量`CreatorRegistry`(即注册表)的指针，最终把(`clip`，`Creator_clipLayer`）注册到注册表。

```c++
 static CreatorRegistry& Registry() {
    static CreatorRegistry* g_registry_ = new CreatorRegistry();
    return *g_registry_;
  }

  // Adds a creator.
  static void AddCreator(const string& type, Creator creator) {
    CreatorRegistry& registry = Registry();
    CHECK_EQ(registry.count(type), 0)
        << "Layer type " << type << " already registered.";
    registry[type] = creator;
  }
```



第二种情况，`layer`需要通过一个`creator function`来生成。那么可以在`layer_factory.cpp`中调用宏`REGISTER_LAYER_CREATOR`，来进行注册。

```c++
REGISTER_LAYER_CREATOR(Convolution, GetConvolutionLayer);
```



#### 生成`Layer`

输入`LayerParameter`，查找注册表，返回`shared_ptr<Layer<Dtype> > `

```c++
static shared_ptr<Layer<Dtype> > CreateLayer(const LayerParameter& param) {
    if (Caffe::root_solver()) {
        LOG(INFO) << "Creating layer " << param.name();
    }
    const string& type = param.type();
    CreatorRegistry& registry = Registry();
    CHECK_EQ(registry.count(type), 1) << "Unknown layer type: " << type
        << " (known types: " << LayerTypeListString() << ")";
    return registry[type](param);
}
```



#### 单例模式

构造函数设为private，`LayerRegisterer`不能通过构造函数进行构造，从而保证只能有一个实例

```c++
 private:
  LayerRegistry() {}
```



### Layer初始化

#### 成员变量

- `layer_param_`:  `LayerParameter`是用于描述`Layer`的 `protobuf` 消息类型，包含的主要字段有：
  - `type`: The type of layer, such as `Convolution`, `Pooling`, `ReLU`, etc.
  - `name`: The name of the layer, which can be used to identify it in the network.
  - `bottom`: The name(s) of the input blobs for the layer.
  - `top`: The name(s) of the output blobs for the layer.
  - `include`: Specifies which phases (TRAIN or TEST) the layer should be included in.
  - `param`: A repeated field containing the hyperparameters for the layer.
  - `blobs`: A repeated field containing the weight and bias parameters for the layer.
  - `Layer type-specific parameters`： e.g. `ConvolutionParameter`,`BatchNormParameter`,`InnerProductParameter`...
- `phase_`:  标识网络的运行阶段：`TRAIN` or `TEST`。
  - `TRAIN Phase` 用于训练网络，它会根据损失函数计算的梯度来更新网络的权重。在 `TRAIN Phase` 中，通常会使用一些数据增强技术（如随机裁剪、随机翻转等）来扩充训练数据集。
  - `TEST Phase` 用于测试和评估网络的性能。在 `TEST Phase`中，网络通常不进行权重更新，并且使用整个测试数据集进行测试，以计算网络在给定任务上的精度和损失。
- `blobs_`:  `Layer`的权重和偏置，也就是模型要学习的参数，**weight和bias分开保存在两个blob中**
- `param_propagate_down_`:   存储参数更新的开关，即是否对层的参数进行反向传播更新。它是一个` bool` 类型的向量，长度等于` blobs_` 向量的长度，每个元素表示对应的 `Blob` 是否需要进行参数更新。
- `loss_`: vector类型，每个top blob是否有非零的权重weight

```c++
 protected:
  /** The protobuf that stores the layer parameters */
  LayerParameter layer_param_;
  /** The phase: TRAIN or TEST */
  Phase phase_;
  /** The vector that stores the learnable parameters as a set of blobs. */
  vector<shared_ptr<Blob<Dtype> > > blobs_;
  /** Vector indicating whether to compute the diff of each param blob. */
  vector<bool> param_propagate_down_;

  /** The vector that indicates whether each top blob has a non-zero weight in
   *  the objective function. */
  vector<Dtype> loss_;
```



#### 构造函数

构造函数初始化`layer_param_`，`phase_`, `blobs_`

构造函数在基类实现，子类只是继承父类的构造函数。

```c++
  explicit Layer(const LayerParameter& param)
    : layer_param_(param) {
      // Set phase and copy blobs (if there are any).
      phase_ = param.phase();
      if (layer_param_.blobs_size() > 0) {
        blobs_.resize(layer_param_.blobs_size());
        for (int i = 0; i < layer_param_.blobs_size(); ++i) {
          blobs_[i].reset(new Blob<Dtype>());
          blobs_[i]->FromProto(layer_param_.blobs(i));
        }
      }
    }
```



#### `SetUp()`

- 校验输入&输出的`Blob`数量是否正确
- 初始化`blobs_`,`param_propagate_down_`
- 调整输出`Blob`的形状
- 设置loss的权重

```c++
  void SetUp(const vector<Blob<Dtype>*>& bottom,
      const vector<Blob<Dtype>*>& top) {
    CheckBlobCounts(bottom, top);
    LayerSetUp(bottom, top);
    Reshape(bottom, top);
    SetLossWeights(top);
  }
```



### 前向传播

由各层实现具体的前向传播算法，有CPU、GPU两种实现。

- 例如全连接层，前向传播过程如下：
  - 输入数据（一批大小为 N×C×H×W 的数据）首先被展开成 N×D 的二维矩阵，其中 D=C×H×W。
  - 该二维矩阵与一个大小为 D×M 的权重矩阵 W 相乘，得到一个大小为 N×M 的矩阵。
  - 该矩阵的每一行都加上一个大小为 M 的偏置向量 b。
  - 最后输出一个大小为 N×M 的矩阵。

其中，矩阵乘法分别调用的是`cblas_sgemm()`,`cublasSgemv()`

```c++
template <typename Dtype>
void InnerProductLayer<Dtype>::Forward_cpu(const vector<Blob<Dtype>*>& bottom,
    const vector<Blob<Dtype>*>& top) {
  const Dtype* bottom_data = bottom[0]->cpu_data();
  Dtype* top_data = top[0]->mutable_cpu_data();
  const Dtype* weight = this->blobs_[0]->cpu_data();
  caffe_cpu_gemm<Dtype>(CblasNoTrans, transpose_ ? CblasNoTrans : CblasTrans,
      M_, N_, K_, (Dtype)1.,
      bottom_data, weight, (Dtype)0., top_data);
  if (bias_term_) {
    caffe_cpu_gemm<Dtype>(CblasNoTrans, CblasNoTrans, M_, N_, 1, (Dtype)1.,
        bias_multiplier_.cpu_data(),
        this->blobs_[1]->cpu_data(), (Dtype)1., top_data);
  }
}
```



### 反向传播

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/caffe/backward.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
通过SGD等方法优化神经网络参数，需要计算神经网络参数关于误差的梯度。这可以通过反向传播得到，也就是沿着前向传播相反的方向，乘上局部导数，起始点是误差关于输出值的偏导数。反向传播是基于链式法则的，计算过程需要用到前向传播得到的中间结果。
![]({{site.baseurl}}/img/caffe/chain_rule.png) 
</li> 
</ul> 

`Backward()`实现了反向传播，也就是给定top blob和error gradient计算得到bottom的error gradient。其输入是 top blobs ，在top blobs里面的diff存储的就是其相应的error gradients。其中propagate_down这个参数跟Bottom的长度是一样的，每一个Index用来指定是否需要反向传播error gradients 到对应的bottom blob。而bottom 这里面的diff 区域存放的就是BackWard计算出来相应的gradient error。

如果自己你要实现一个自己的Layer，主要实现的就是Forward_cpu和Backward_cpu 以及gpu（可选）。

可以参考何凯明的深度残差网络https://github.com/KaimingHe/deep-residual-networks





## 参考资料

- https://github.com/BVLC/caffe
- https://hqli.github.io/doc/experience/caffe_code/index.html