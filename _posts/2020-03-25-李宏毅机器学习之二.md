---
layout:     post
title:      李宏毅机器学习之二
subtitle:   
date:       2020-03-25
author:     bjmsong
header-img: img/machineLearning/machineLearning.png
catalog: true
tags:
    - 机器学习
---

> 本文将介绍CNN、Semi-supervised Learning、Unsupervised Learning

### CNN

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/why_cnn.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/cnn_for_image.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/cnn_for_image2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/cnn_for_image3.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/cnn.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/convolution.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
convolution背后的运算仍然是全连接网络，只不过每一层的神经元只跟上一层的一小部分神经元关联，并且不同神经元之间会共享参数，这样参数数量大大减少
![]({{site.baseurl}}/img/machineLearning/convolution2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/max_pooling.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/flatten.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/cnn_in_keras.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
调节输入的图片，使得某个filter的输出最大，这样就可以看到这个filter主要是在找什么样的pattern
![]({{site.baseurl}}/img/machineLearning/cnn_learn.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/fooled.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
CNN除了图像、语音识别，也可以应用在文本。将语句看成是词×词向量的矩阵，只不过做convolution的时候只沿着词的方向移动，而不沿着词向量的方向移动（因为没有意义）
![]({{site.baseurl}}/img/machineLearning/cnn_text.png) 
</li> 
</ul> 



### Why Deep

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/why_deep.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/end_to_end.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/end_to_end2.png) 
</li> 
</ul> 



### Semi-supervised Learning

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/semi.png) 
</li> 
</ul> 



#### Semi-supervised Learning for Generative Model

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/semi_generative.png) 
</li> 
</ul> 



#### Low-density Separation

- 想法是，数据是“非黑即白”，界限分明的，因此两个类别之间的数据密度很低

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/self-training.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
self-training和generative model的半监督学习的不同在于：hard label or soft label。self-training是hard label，即直接将unlabeled data归于某一类，而generative model是soft label，只是给出unlabeled data属于某一类的概率。
![]({{site.baseurl}}/img/machineLearning/self-training-vs-generative.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
hard label比较粗暴，可以加上熵的约束，熵可以衡量分布是否集中，因此可以得到分布集中的label
![]({{site.baseurl}}/img/machineLearning/entropy_based.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
通过遍历unlabeled data。将它们指派到任意一个类别，然后训练SVM，挑选出训练得到最好的模型 -- 效率很低下？
![]({{site.baseurl}}/img/machineLearning/semi-svm.png) 
</li> 
</ul> 



#### Smoothness Assumption

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/smoothness_assumption.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/smoothness_assumption2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/graph-based.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/graph-based2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/graph-based3.png) 
</li> 
</ul> 



### Unsupervised Learning：Linear Method

<ul> 
<li markdown="1">
unsupervised_learning分为两大类：1. Clustering，Dimension Reduction ； 2. Generation
![]({{site.baseurl}}/img/machineLearning/unsupervised_learning.png) 
</li> 
</ul> 



#### Clustering

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/clustering.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/hac.png) 
</li> 
</ul> 



#### Dimension Reduction

<ul> 
<li markdown="1">
通过聚类把某个样本直接归为某一类略显粗暴，大部分情况下，用一个distributed representation来表示一个样本更合理，这就引出了降维的概念
![]({{site.baseurl}}/img/machineLearning/distributed_representation.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/dimension_reduction.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/pca.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/pca2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/pca3.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
SVD分解，可以近似得到矩阵，PCA的结果就是矩阵U的对角线（前k个）参数
![]({{site.baseurl}}/img/machineLearning/pca4.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/pca_nn.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
缺点1：无监督，缺点2：无法做非线性变换
![]({{site.baseurl}}/img/machineLearning/pca_weak.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/矩阵分解.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
LSA
![]({{site.baseurl}}/img/machineLearning/矩阵分解for主题分析.png) 
</li> 
</ul> 



### Unsupervised Learning：Word Embedding

[可以参照这一篇](https://bjmsong.github.io/2020/03/03/%E4%BB%8EWord-Embedding%E5%88%B0Bert%E6%A8%A1%E5%9E%8B-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E4%B8%AD%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%8A%80%E6%9C%AF%E5%8F%91%E5%B1%95%E5%8F%B2/)



### Unsupervised Learning：Neighbor Embedding

- 非线性降维
- 在Manifold里面，只有距离很近的点，欧式几何才成立



#### LLE

<ul> 
<li markdown="1">
LLE先从原始数据中学出wij，通过最小化下面的式子
![]({{site.baseurl}}/img/machineLearning/lle.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
精妙的比喻！
![]({{site.baseurl}}/img/machineLearning/lle2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
降维后得到的zij要满足以下的关系
![]({{site.baseurl}}/img/machineLearning/lle3.png) 
</li> 
</ul> 



#### Laplacian Eigenmaps

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/laplacian.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/laplacian2.png) 
</li> 
</ul> 



#### tSNE

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/tsne.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/tsne2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/tsne3.png) 
</li> 
</ul> 



### Unsupervised Learning：Auto-encoder

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/auto-encoder.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/auto-encoder-text.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/auto-encoder-text2.png) 
</li> 
</ul> 



### Unsupervised Learning：Deep Generative Model

#### PixelRNN

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/pixelrnn.png) 
</li> 
</ul> 



#### VAE

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/vae.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/write_poetry.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/vae缺点.png) 
</li> 
</ul> 



#### GAN

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/gan.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/discriminator.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/generator.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/gan_example.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
GAN比较困难的地方是没有一个指标来判断discriminator的好坏
![]({{site.baseurl}}/img/machineLearning/GAN实操比较困难.png) 
</li> 
</ul> 