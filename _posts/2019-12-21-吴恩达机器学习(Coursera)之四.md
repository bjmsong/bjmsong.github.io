---
layout:     post
title:      吴恩达机器学习(Coursera)之四
subtitle:   
date:       2019-12-21
author:     bjmsong
header-img: img/machineLearning/machineLearning.png
catalog: true
tags:
    - 机器学习
---
> 本文是课程的第13~15章，将介绍无监督学习算法：聚类、降维、异常检测



## 十三、聚类(**Clustering**) 

13.1 无监督学习：简介 

13.2 K-均值算法 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/kmeans算法.png) 
</li> 
</ul> 

13.3 优化目标 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/kmeans优化目标.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/kmeans优化目标2.png) 
</li> 
</ul> 

13.4 随机初始化

为避免陷入局部最优，可以尝试多次随机初始化

13.5 选择聚类数 

<ul> 
<li markdown="1"> 
可以尝试elbow method，但是很多时候，elbow不是那么明显（如右图）
![]({{site.baseurl}}/img/CourseraML/elbowmethod.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
更合理的方法是，根据聚类之后的应用场景，确定聚类的数量。
![]({{site.baseurl}}/img/CourseraML/选择k值.png) 
</li> 
</ul> 



## 十四、降维(**Dimensionality Reduction**) 

14.1 动机一：数据压缩 

14.2 动机二：数据可视化 

14.3 主成分分析（PCA）问题 

<ul> 
<li markdown="1"> 
在做PCA之前，需要将特征先进行归一化
![]({{site.baseurl}}/img/CourseraML/pca.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
1. 线性回归最小化点到直线上点的“竖直”距离，PCA最小化点到直线的垂直距离
2. 线性回归中有一个预测变量y，而PCA所有特征都是同等地位的
![]({{site.baseurl}}/img/CourseraML/PCAvsLinear.png) 
</li> 
</ul> 

14.4 主成分分析算法 

<ul> 
<li markdown="1"> 
取矩阵U的前k个向量    
![]({{site.baseurl}}/img/CourseraML/pca算法.png) 
</li> 
</ul> 

14.5 选择主成分的数量 

<ul> 
<li markdown="1">     
![]({{site.baseurl}}/img/CourseraML/选择k.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">     
右边的方法比较高效    
![]({{site.baseurl}}/img/CourseraML/选择k2.png) 
</li> 
</ul> 

14.6 重建的压缩表示 

14.7 主成分分析法的应用建议 



## 十五、异常检测(**Anomaly Detection**) 

15.1 问题的动机 

15.2 高斯分布 

15.3 算法 

15.4 开发和评价一个异常检测系统 

15.5 异常检测与监督学习对比 

15.6 选择特征 

15.7 多元高斯分布（选修） 

15.8 使用多元高斯分布进行异常检测（选修） 




## 参考资料
- https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes




