---
layout:     post
title:      微软-AI-System
subtitle:   
date:       2022-12-15
author:     bjmsong
header-img: 
catalog: true
tags:
    - ML System
---
## 学习资料
- https://github.com/microsoft/AI-System
- ppt
    - https://github.com/microsoft/AI-System/tree/main/Lectures
- 中文教材  
    - https://github.com/microsoft/AI-System/tree/main/Textbook
    - 更详细
- lab
    - https://github.com/microsoft/AI-System/tree/main/Labs
- 大量的参考文献

## 第1章-人工智能系统概述
- 本书介绍的人工智能系统（Artificial Intelligence System）主要是指深度学习系统（Deep Learning System），但是这些系统设计原则大部分也适合于机器学习系统（Machine Learning System）
- 深度学习算法本身无论从模型设计，训练方式也借鉴了很多传统机器学习算法的经典理论与实践方式，但是深度学习系统本身相比机器学习系统从硬件到软件层有更多新的挑战和演化，数据与问题规模变得更大，应用场景与部署也更加广泛。
- 综合起来深度学习系统会综合考虑和借鉴机器学习系统（Machine Learning System），大数据系统（Big Data System）和高性能计算（High Performance Computing）领域和社区中的经典的系统设计，优化与问题解决方法，并演化出针对深度学习算法特点的新的系统设计。
- 深度学习模型变化趋势
    - 更大的模型
        - 以Transformer为基本结构的代表性预训练神经语言模型（Neural Language Model），例如，BERT，GPT-3 等，在自然语言处理和计算机视觉等场景应用越来越广泛。其不断增加的层数和参数量，对底层系统内存管理，分布式训练算子放置，通信，以及硬件设计提出了很大的挑战。
    - 更灵活的结构和建模能力
        - 图神经网络等网络不断抽象多样且灵活的数据结构（例如：图（Graph），树（Tree）等），应对更为复杂的建模需求。进而衍生了新的算子（例如：图卷积等）与计算框架（例如：图神经网络框架等）。
    - 更稀疏的模型结构与模型融合（Model Ensemble）
        - 以多专家模型（Mixture of Experts， MoE） 和 Pathways 模型结构为代表的融合模型结构让运行时的模型更加动态（Dynamic）和稀疏（Sparse），进而提升模型的训练效率减少训练代价，支持更多的任务。给系统设计中以往的静态分析方式带来了不小的挑战，同时驱动运用即时编译（Just In Time Compiling）和运行期（Runtime）更加高效的调度与优化。
    - 更大规模的搜索空间
        - 用户定义更大规模的超参数与模型结构搜索空间，通过超参数搜索优化（HPO）与神经网络结构搜索（NAS）自动化找到最优的模型结构。自动化机器学习（AutoML）为代表的训练方式，衍生出多作业（Multi-Jobs）执行与编排的优化需求。
    - 更多样的训练方式
        - 强化学习（Reinforcement Learning）为代表的算法有比传统训练方式更为复杂的过程。其衍生出训练，推理，数据处理混合部署与协同优化的系统设计需求。
- PyTorch和 TensorFlow等框架应对自动化机器学习，强化学习等多样执行方式，以及细分的应用场景显得不够灵活，需要用户手工做特定的一些优化，没有好的工具和系统的支撑，这些问题一定程度上会拖慢和阻碍算法工程师研发效率，影响算法本身的发展。
- 开源社区中不断涌现针对特定应用领域而设计的框架和工具
    - Hugging Face: 面向语言预训练模型构建的Model Zoo和库框架
    - FairSeq: 面向自然语言处理中场景的序列到序列模型
    - MMDetection: 针对物体检测
    - NNI: 针对自动化机器学习
- 计算框架的进步
    - 第一代框架
        - Theano，Caffe，DisBelief
        - 为数值计算或特定机器学习问题或算法而设计
    - 第二代框架
        - 声明式编程（Declarative Programming）: TensorFlow
        - 命令式编程（Imperative Programming）: PyTorch
        - 存在问题
            - 控制流，数据预处理等其他语言层的逻辑与深度学习模型计算图的割裂造成不便于统一编译与优化
            - 除深度学习模型之外的库不方便卸载计算和利用 GPU 等专有硬件进而造成低效数据流水线
            - 没有侧重面向方面设计造成作业调试诊断困难，运维负担较大
            - Python 语言本身特点是简单，但是并发支持效率不高，不利于静态优化与错误检测等，对大规模工程化实践不友好
- 深度学习系统的设计目标
    - 提供更加高效的编程语言、框架和工具
    - 提供全面多样的深度学习任务需求的系统支持
    - 探索并解决新挑战下的系统设计、实现和演化的问题
    - 提供在更大规模的企业级环境的部署需求
- 深度学习系统的大致组成
    - 开发体验层
    - 框架层
    - 运行时
    - 资源管理与硬件体系结构
- 深度学习框架一般会提供以下功能
    - 以 Python API 供开发者编写复杂的模型计算图（Computation Graph）结构，调用基本算子实现（例如，卷积的 cuDNN 实现），大幅降低开发代码量
    - 自动化内存管理，不暴露指针和内存管理给用户
    - 自动微分（Automatic Differentiation）的功能，并能自动构建反向传播计算图，与前向传播图拼接成统一计算图
    - 调用或生成运行期优化代码（静态优化）
    - 调度算子在指定设备的执行，并在运行期应用并行算子，提升设备利用率等优化（动态优化）
- TensorFlow训练全流程
    - 前端程序转换为数据流图
        - 使用 Python 编写的深度学习模型，通过预先定义的接口，翻译为中间表达（Intermediate Representation），并且构建算子直接的依赖关系，形成前向数据流图（Data-Flow Graph）
    - 反向求导
        - 分析形成前向数据流图，通过算子之前定义的反向传播函数，构建反向传播数据流图，并和前向传播数据流图一起形成整体的数据流图
    - 产生运行期代码
        - 根据运行时部署所在的设备（CPU，GPU 等），将算子中间表达替换为算子针对特定设备的运行期的代码，例如CPU 的 C++ 算子实现或者针对 NVIDIA GPU 的 CUDA 算子实现
    - 调度并运行代码
        - 框架会将算子及其运行期的代码实现抽象为“任务”，依次根据“任务”依赖关系，调度到计算设备上进行执行
- 影响深度学习系统设计的理论，原则与假设
    - 抽象-层次化表示
        - 深度学习系统也遵循层次化的设计。系统会在各个层次抽象不同的表示，在高层方便用户表达算子，在底层则被转换为指令被芯片执行。这样搭积木的方式让整个工具链快速协同发展且能复用，大为加速了开发效率与自动化。
    - 摩尔定律（Moore's Law）与算力发展趋势
        - 摩尔定律已死
        - 芯片和系统性能还会受到其他约束（Constraint）所限制
            - 功耗墙约束
            - 暗硅 (Dark Silicon）与异构硬件（Heterogeneous Hardware） 
            - 内存墙约束
                - “内存墙”是芯片与芯片外的内存之间越来越大的速度差距
    - 局部性原则（Priciple of Locality）与内存层次结构（Memory Hierarchy）
        - 局部性原理是处理器在短时间内重复访问同一组内存位置的趋势。局部性有两种基本类型——时间局部性和空间局部性。时间局部性是指在相对较短的时间内重复使用特定数据和资源。空间局部性（也称为数据局部性）是指在相对较近的存储位置内使用数据元素。
    - 线性代数（Linear Algebra）计算与模型缺陷容忍（Defect Tolerance）特性
        - 线性代数：大部分的深度学习算子可以抽象为线性代数运算，如通用矩阵乘（GEMM）
    - 并行（Parallel）加速与阿姆达尔定律（Amdahl's Law）优化上限
    - 冗余（Redundancy）与可靠性（Dependability）

## 第2章-神经网络基础

## 第3章-深度学习框架基础
### 基于数据流图的深度学习框架
- 深度学习框架设计目标
    - 可编程性
    - 性能
    - 可扩展性
- 编程范式
    - 声明式
        - 前端语言中的表达式不直接执行，而是首先构建起一个完整前向计算过程表示，这个计算过程的表示经过序列化发送给后端系统，后端对计算过程表示优化后再执行，又被称作先定义后执行（Define-and-Run）或是静态图
        - 对数据和控制流的静态性限制更强，由于能够在执行之前得到全程序描述，从而有机会进行运行前编译（Ahead-Of-Time）优化
    - 命令式
        - 后端高性能可复用模块以跨语言绑定（Language Binding）方式与前端深度集成，前端语言直接驱动后端算子执行，用户表达式会立即被求值，又被称作边执行边定义(Define-by-Run)或者动态图
        - 方便调试，灵活性高，但由于在执行前缺少对算法的统一描述，也失去了编译期优化（例如，对数据流图进行全局优化等）
    - 多阶段（Multi-Stage ）编程和即时编译（Just-In-Time, JIT)技术能够实现两种编程模式的混合。随着TensorFlow Eager和PyTorch JIT的加入，主流深度学习框架都选择了通过支持混合式编程以兼顾两者的优点
- 数据流图用统一的方式描述出了复杂神经网络训练的全过程，使得在运行程序之前后端系统有机会对整个计算过程的数据依赖关系进行分析，通过数据流图化简、内存优化、预先计算算子间的静态调度策略等方式，改善运行时的性能。
- 基于数据流图描述，深度学习框架在设计上切分出了三个解耦的优化层：数据流图优化，运行时调度策略，以及算子优化。当遇到新的神经网络模型结构或是训练算法时，通过以下三步进行扩展：
    （1）添加新的算子；
    （2）对算子的内核函数在不同设备，不同超参数下进行计算优化；
    （3）注册算子和内核函数，由运行时系统在在运行时派发到所需的实现上。
- 在基于数据流图的深度学习框架设计之初，希望通过对三个优化层之间的解耦来加速深度学习软件栈的迭代，然而，随着神经网络模型计算规模的增大，出现了越来越多的定制化算子，多设备支持需求增加，这三个抽象层之间的抽象边界也在被频繁地打破。在后续的章节我们会进一步讨论。

### 神经网络计算中的控制流
- 随着神经网络算法研究的发展，一些新颖的神经网络结构很难自然地表示为纯数据流图。为了能够支持如自定义循环神经网络这类计算过程中天生就含有控制流结构的神经网络计算，主流深度学习框架不约而同的引入了对动态控制流这一语言结构（Language Construct）的支持。
- 主流框架采用了两类设计思路：
    - 静态图：向数据流图中添加控制流原语
    - 动态图：复用宿主语言控制流语句
- 动态图转换为静态图
    - 为基于追踪（Tracing）
    - 基于源代码解析（Parsing）

## 第4章-矩阵运算与计算机体系结构
### 深度学习的计算模式
- 全连接层
- 卷积层
- 循环网络层
- 注意力层

### 计算机体系结构与矩阵运算
- 

### GPU体系结构与矩阵运算
- 



## 第5章-深度学习框架的编译与优化

## 第6章-分布式训练算法与系统


## 第7章-异构计算集群调度与资源管理系统

## 第8章-深度学习推理系统