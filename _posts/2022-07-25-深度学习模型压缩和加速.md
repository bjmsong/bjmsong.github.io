---
layout:     post
title:      深度学习模型压缩和加速
subtitle:   
date:       2022-07-25
author:     bjmsong
header-img: 
catalog: true
tags:
    - 深度学习
---
>模型压缩和加速是两个不同的话题，有时候压缩并不一定能带来加速的效果，有时候又是相辅相成的。压缩重点在于减少网络参数量，加速则侧重在降低计算复杂度、提升并行能力等。

## 背景
- AI模型越来越复杂，训练和预测需要大量算力，大量时间
- 很多场景下算力、存储、电量等有限，比如移动端AI落地
- 目标：模型尺寸小、计算复杂度低、电池耗电量低、下发更新部署灵活


## 算法层压缩加速：算法工程师的工作范畴
- 李宏毅：模型压缩系列讲解
https://www.bilibili.com/video/BV1LE411Z76J?spm_id_from=333.337.search-card.all.click&vd_source=7798c62f92ce545f56fd00d4daf55e26
http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20
- Architecture Design
    - 矩阵分解
        - ALBERT
    - 权值共享
    - 分组卷积
    - 分解卷积
    - 全局平均池化代替全连接层
- Network Pruning 模型剪枝
    - prune redundant weights or neurons
    - 突触剪枝、神经元剪枝、权重矩阵剪枝
    - 将权重矩阵中不重要的参数设置为0，结合稀疏矩阵来进行存储和计算
    - 为了保证performance，需要一小步一小步地进行迭代剪枝
    - 般是prune neuron
        - prune weight的问题：权重设为0，而不是拿掉，容易实现，方便加速，但是模型没有真正压缩，并没有得到加速
- Knowledge Distillation 蒸馏
    - student模型学习teacher模型的输出
    - 没太有用？
- Parameter Quantization 量化
    - 伪量化
        - 利用低精度来保存每一个网络参数，同时保存拉伸比例scale和零值对应的浮点数zero_point。推理阶段，利用公式来还原参数为32bit浮点
    - 聚类与伪量化
        - 进阶：霍夫曼编码
    - 定点化
        - 与伪量化不同的是，定点化在推理时，不需要还原为浮点数
- Dynamic Computation


## 框架层加速
- 移动端AI框架: 谷歌的tf-lite，腾讯的NCNN，阿里的MNN，百度的PaddleLite, 小米的MACE
https://easyai.tech/blog/10-mobil-deeplearning-frame/
https://www.cnblogs.com/LXP-Never/p/14840535.html
- 端侧AI框架加速优化方法
    - 编译优化、缓存优化、多线程、稀疏存储和计算、NEON指令应用、算子优化等
- 系统优化: 指在特定系统平台上，通过Runtime层面性能优化，以提升AI模型的计算效率
    - Op-level的算子优化：FFT Conv2d (7x7, 9x9), Winograd Conv2d (3x3, 5x5) 等；
    - Layer-level的快速算法：Sparse-block net [1] 等；
    - Graph-level的图优化：BN fold、Constant fold、Op fusion和计算图等价变换等；
    - 优化工具与库（手工库、自动编译）：TensorRT (Nvidia), MNN (Alibaba), TVM (Tensor Virtual Machine), Tensor Comprehension (Facebook) 和OpenVINO (Intel) 等；


### CUDA
- https://zhuanlan.zhihu.com/p/532397444
- CUDA 是 NVIDIA 发明的一种并行计算平台和编程模型。它通过利用图形处理器 (GPU) 的处理能力，可大幅提升计算性能
- https://www.nvidia.cn/geforce/technologies/cuda/
- https://www.bilibili.com/video/BV15Q4y1i7Bp?p=2&spm_id_from=pageDriver&vd_source=7798c62f92ce545f56fd00d4daf55e26
- https://www.bilibili.com/video/BV1kx411m7Fk?spm_id_from=333.337.search-card.all.click&vd_source=7798c62f92ce545f56fd00d4daf55e26

### cuDNN
- CUDA神经网络库，将一些基本层结构进行封装，实现了在GPU上的高效并行计算
- 为了避免每个使用者都需要自己实现底层的CUDA编程
- torch在此之上又实现了一层封装：torch.cuda

### ONNX
作为开放神经网络交换标准，对不同框架和不同的移动端、边缘设备 Runtime 进行了标准化，从而降低了模型在不同框架和不同 Runtime 之间转换的成本，现在主流的框架和设备都支持 ONNX。
https://github.com/onnx/onnx


## 硬件层加速,异构计算
- 有GPU、FPGA、ASIC等多种方案，各种TPU、NPU就是ASIC这种方案，通过专门为深度学习进行芯片定制，大大加速模型运行速度
- GPU
    - https://zhuanlan.zhihu.com/p/413145211
    - tensor core
    https://zhuanlan.zhihu.com/p/75753718
    https://zhuanlan.zhihu.com/p/400351564

## Tensorflow加速技巧: 《Hands on ml2》Chapter 19
https://tf.wiki/zh_hans/appendix/distributed.html
- 利用好GPU，CPU资源
- 数据并行：更容易实现
https://www.pythonf.cn/read/100405
- 模型并行
    - 比较难
    - https://www.tensorflow.org/tutorials/distribute/keras?hl=zh-cn
    - https://ai.facebook.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/
        - 对Embedding部分采用模型并行
        - 对MLP部分采用数据并行
    - https://medium.com/analytics-vidhya/speeding-up-inference-using-parallel-model-runs-d76dcf393567   
- 分布式训练
https://zhuanlan.zhihu.com/p/56991108
https://www.tensorflow.org/guide/distributed_training?hl=zh-cn
https://github.com/yahoo/TensorFlowOnSpark

## 其它
- 读取训练文件
    https://tech.meituan.com/2018/06/07/searchads-dnn.html
    https://www.cnblogs.com/wj-1314/p/11211333.html
    https://www.tensorflow.org/tutorials/load_data/csv?hl=zh-cn
    https://www.tensorflow.org/tutorials/load_data/tfrecord?hl=zh-cn
    - spark输出训练文件（TensorFlow Record格式） 
        https://zhuanlan.zhihu.com/p/352025069
        https://cloud.tencent.com/developer/news/639997  
        - 归一化这步local来完成 
    - 数据预读：用多进程的方式，将HDFS上预处理好的数据拉取到本地磁盘（使用joblib库+shell将HDFS数据用多进程的方式拉取到本地，基本可以打满节点带宽2.4GB/s，所以，拉取数据也可以在10分钟内完成）
    - 程序通过TensorFlow提供的TFrecordReader的方式读取本地磁盘上的数据，这部分的性能提升是最为明显的。原有的程序处理数据的性能大概是1000条/秒，而通过TFrecordReader读取数据并且处理，性能大概是18000条/秒，性能大概提升了18倍
- tensorflow单机版，数据在分布式集群上，交互流程麻烦
- tensorflow serving
    - 存在哪些问题 






## 参考资料
- https://zhuanlan.zhihu.com/p/138059904
- https://towardsdatascience.com/the-deep-learning-inference-acceleration-blog-series-part-1-introduction-668e39b8b14b
- https://blog.csdn.net/nature553863/article/details/81083955
- https://github.com/scutan90/DeepLearning-500-questions/blob/master/ch17_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/%E7%AC%AC%E5%8D%81%E4%B8%83%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2.md
- https://mp.weixin.qq.com/s/VdmukMxAFTJYQmqXP7rrcg
- https://blog.csdn.net/zeusee/article/details/89601634
- https://www.cnblogs.com/LXP-Never/p/14833772.html