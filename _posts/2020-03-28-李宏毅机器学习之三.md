---
layout:     post
title:      李宏毅机器学习之三
subtitle:   
date:       2020-03-28
author:     bjmsong
header-img: img/machineLearning/machineLearning.png
catalog: true
tags:
    - 机器学习
---

> 本文将介绍Transfer Learning、SVM、RNN、Ensemble



### Transfer Learning

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/transfer-learning.png) 
</li> 
</ul> 



#### Fine-tuning

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/fine-tune.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
为了得到比较好的结果，可以对Fine-tuning的output加一些约束，比如跟source data的output结果越接近越好
![]({{site.baseurl}}/img/machineLearning/fine-tune-tips.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
也可以只fine-tuning其中几层的参数，其它层的参数保持不变
![]({{site.baseurl}}/img/machineLearning/layer-transfer.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/layer-transfer2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/layer-transfer3.png) 
</li> 
</ul> 



####  Multi-task Learning

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/multitask.png) 
</li> 
</ul> 



#### Domain-adversarial training

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/Domain-adversarial-training.png) 
</li> 
</ul>

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/Domain-adversarial-training2.png) 
</li> 
</ul>



#### Zero-shot learning

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/zero-shot.png) 
</li> 
</ul>





### Support Vector Machine

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/svm.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
SVM和deep learning一样，都是feature transformation +　linear classifier。所不同的是SVM的feature transformation相对较少（被kernel method种类限制），linear classifier采用hinge loss
![]({{site.baseurl}}/img/machineLearning/svm2.png) 
</li> 
</ul> 



#### Hinge Loss

- 图中横轴为
  $$
  y^nf(x^n)
  $$
  纵轴为loss，那么应该是横轴值越大，纵轴的loss越小。

<ul> 
<li markdown="1">
从图中也能看出，Square loss不适合作为二分类问题的loss function。Hinge Loss和cross entrophy都可以看成是Ideal loss的upper bond。
![]({{site.baseurl}}/img/machineLearning/几种loss.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
function不一定要是linear，比如也可以是deep network。
![]({{site.baseurl}}/img/machineLearning/linearSVM.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
linearSVM的loss function另一种表达方式
![]({{site.baseurl}}/img/machineLearning/linearSVM2.png) 
</li> 
</ul>



####  Kernel Method

<ul> 
<li markdown="1">
可以将参数表示成特征的线性组合（对偶表示），只有少数的点会被选为support vector（系数不为零），影响参数的选择，这是SVM得名的由来。这跟其它算法不太一样，SVM模型更鲁棒（outlier如果不是support vector，对参数没有影响）。
![]({{site.baseurl}}/img/machineLearning/对偶表示.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
可以将function表示成核函数的形式，核函数代表特征间的内积
![]({{site.baseurl}}/img/machineLearning/对偶表示2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/对偶表示3.png) 
</li> 
</ul> 

- 通常将feature做非线性变换（特征工程），可使模型得到更好的效果。kernel method可以很高效地做这件事情。不只是SVM可以使用kernel method，其它模型，如逻辑回归，也可以使用kernel method。

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/kernel.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/kernel2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
RBF kernel相当于将feature投影到无限维的空间，再做inner product
![]({{site.baseurl}}/img/machineLearning/kernel3.png) 
</li> 
</ul> 



### RNN

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/rnn.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/rnn2.png) 
</li> 
</ul> 



#### LSTM

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/lstm.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/lstm2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
GRU：简化版本，参数会更少
![]({{site.baseurl}}/img/machineLearning/lstm3.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/lstm_uneasy_tolearn.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/lstm4.png) 
</li> 
</ul> 



#### Application

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/rnn_application.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/rnn_application2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/rnn_application3.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/rnn_application4.png) 
</li> 
</ul> 



#### Attention

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/attention.png) 
</li> 
</ul> 



#### RNN vs Structured Learning

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/rnn_vs_hmm.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/rnn+hmm.png) 
</li> 
</ul> 



### Ensemble

#### Bagging

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/bagging.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/bagging2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/bagging3.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/rf.png) 
</li> 
</ul> 



#### Boosting

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/boosting.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/boosting2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/adaboost.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/adaboost2.png) 
</li> 
</ul> 



##### Gradient Boosting

<ul> 
<li markdown="1">
Boosting算法的通用版本
![]({{site.baseurl}}/img/machineLearning/gradient_boosting.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
可以变换loss
![]({{site.baseurl}}/img/machineLearning/gradient_boosting2.png) 
</li> 
</ul> 



#### Stacking

- 把训练数据分成两个部分
  - 一部分用来训练各个模型
  - 剩下一部分用来训练一个（简单的）分类器，把上一步得到的每个模型的输出作为feature

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/stacking.png) 
</li> 
</ul> 

