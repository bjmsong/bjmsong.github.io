---
layout:     post
title:      吴恩达机器学习(Coursera)之二
subtitle:   
date:       2019-12-04
author:     bjmsong
header-img: img/machineLearning/machineLearning.png
catalog: true
tags:
    - 机器学习
---
> 本文是课程的第6~9章，将介绍分类算法逻辑回归、正则化、神经网络

## 六、逻辑回归(**Logistic Regression**) 

6.1 分类问题 

6.2 假说表示 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/逻辑回归.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/逻辑回归输出的解释.jpg) 
</li> 
</ul> 

6.3 判定边界 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/决策边界1.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/决策边界2.jpg) 
</li> 
</ul> 

6.4 代价函数 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/逻辑回归损失函数.jpg) 
</li> 
</ul> 

6.5 简化的代价函数和梯度下降 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/逻辑回归损失函数2.jpg) 
</li> 
</ul> 

6.6 高级优化 

- Gradient descent
- Conjugate gradient
- BFGS
- L-BFGS

6.7 多类别分类





## 七、正则化(**Regularization**) 

7.1 过拟合的问题 

7.2 代价函数 

7.3 正则化线性回归 

7.4 正则化的逻辑回归模型 





## 八、神经网络：表述(**Neural Networks: Representation**) 

8.1 非线性假设 

8.2 神经元和大脑 

8.3 模型表示1 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/nn.jpg) 
</li> 
</ul> 

8.4 模型表示2 

向量化表示

8.5 样本和直观理解1 

8.6 样本和直观理解II 

8.7 多类分类 





## 九、神经网络的学习(**Neural Networks: Learning**) 

9.1 代价函数 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/神经网络损失函数.jpg) 
</li> 
</ul> 

9.2 反向传播算法 

<ul> 
<li markdown="1"> 
痛点：神经网络参数太多了，为了在计算梯度下降时更高效，于是发明了BP。
![]({{site.baseurl}}/img/CourseraML/bp.png) 
</li> 
</ul> 

- BP算法将损失对参数的偏微分分解为两项，第一项可以通过前向传播得到，第二项可以通过反向传播得到

<ul> 
<li markdown="1"> 
反向传播相当于也构造了一个神经网络，输入是最后一层的误差，通过这个神经网络进行传导。
![]({{site.baseurl}}/img/CourseraML/bp2.png) 
</li> 
</ul> 

9.3 反向传播算法的直观理解 

9.4 实现注意：展开参数 

9.5 梯度检验 

9.6 随机初始化 

9.7 总结 

9.8 自动驾驶 






## 参考资料
- https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes

- https://www.bilibili.com/video/av9912938/

- [李宏毅 机器学习 第七课 反向传播](https://www.bilibili.com/video/av35932863?from=search&seid=14077284449651622223)


