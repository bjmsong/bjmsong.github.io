---
layout:     post
title:      加速你的python代码之一
subtitle:   向量化
date:       2019-10-28
author:     bjmsong
header-img: img/language/python.jpg
catalog: true
tags:
    - Python
---
>python是处理数据的利器，然而在面对一些比较大的数据集时，python有时会感觉运行很慢，这大大影响了工作的效率。那么有哪些方法可以提升python的效率呢，下面就来介绍几种方法，抛砖引玉。



### 先来讲讲如何性能监控及分析

- `time.time()`：以浮点形式返回自Linux新世纪以来经过的秒数

- [timeit](https://wxnacy.com/2019/03/26/python-timeit/)：提供了一种简单的方法来计算一小段Python代码。 它既有命令行界面，也有可调用界面。 它避免了许多用于测量执行时间的常见陷阱

  ```
  $ python -m timeit '"-".join(str(n) for n in range(100))'
  10000 loops, best of 3: 40.3 usec per loop
  ```

  ```
  >>> import timeit
  >>> # attribute is missing
  >>> s = """\
  ... try:
  ...     str.__nonzero__
  ... except AttributeError:
  ...     pass
  ... """
  >>> timeit.timeit(s)
  0.5832341330096824
  >>> timeit.timeit(s, number=1000)     # number 不指定时，默认为 default_number = 1000000
  0.000628526002401486
  
  # 增加重复次数，使用 repeat() 方法，不指定 repeat 时，默认 default_repeat = 3
  >>> timeit.repeat(s)
  [0.5949273270089179, 0.6405833000026178, 0.5868908820120851]
  >>> timeit.repeat(s, repeat=4)
  [0.5963048749981681, 0.5834796829876723, 0.5749933830084046, 0.5814367970015155]
  ```

- cProfile

  ```
  python -m cProfile myscript.py
  ```

  ```python
  import cProfile
  import pstats
  from contextlib import contextmanager
  
  @contextmanager
  def profiling(sortby='cumulative',limit=20):
      pr = cProfile.Profile()
      pr.enable()
      yield
      pr.disable()
      ps = pstats.Stats(pr).sort_stats(sortby)
      ps.print_stats(limit)
  
  with profiling(sortby='tottime',limit=10):
      # execute code
  ```

- Line Profiler ： 告诉你一个函数中每一行的耗时

  ```python
  from line_profiler import LineProfiler
  import random
  
  def do_stuff(numbers):
      s = sum(numbers)
      l = [numbers[i]/43 for i in range(len(numbers))]
      m = ['hello'+str(numbers[i]) for i in range(len(numbers))]
  
  numbers = [random.randint(1,100) for i in range(1000)]
  lp = LineProfiler()
  lp_wrapper = lp(do_stuff)
  lp_wrapper(numbers)
  lp.print_stats()
  ```

  

### 向量化

当我们要进行向量间的运算，比如矩阵乘法时。常规的做法是需要用for循环去遍历向量里面的每一个值
```
import numpy as np
import time
# Number of features
n = 1000
# Number of training examples
m = 10000
# Initialize X and W
X = np.random.rand(n,m)
W = np.random.rand(n,1)

# Non Vectorized code
Z1 = np.zeros((1,m))
t2 = time.time()
for i in range(X.shape[1]):
    for j in range(X.shape[0]):
        Z[0][i] += W[j]*X[j][i]
print("Time taken for non vectorized code is : ",(time.time()-t2)*1000,"ms")
```
python的向量化可以帮助我们高效地实现这个过程
```
# Vectorized code
t1=time.time()
Z = np.dot(W.T,X)
print("Time taken for vectorized code is : ",(time.time()-t1)*1000,"ms")
```



### 拒绝for循环

#### 一个典型的for循环
```
import seaborn as sns
import numpy as np
import pandas as pd
import time

data = sns.load_dataset('iris')

def compute_class(petal_length):
    if petal_length <= 2:
        return 1
    elif 2 < petal_length < 5:
        return 2
    else:
        return 3

# for loop
start = time.time()

class_list = list()
for i in range(len(data)):
    petal_length = data.iloc[i]['petal_length']
    class_num = compute_class(petal_length)
    class_list.append(class_num)

end = time.time()
print("For-loop run time = {}".format(end - start))
```



#### iterrows方法

for循环是非常耗时的，因此需要想办法抛弃掉for循环。可以采用.iterrows()方法，该方法实现了一个生成器函数，它将在每次迭代中生成一行数据，缺点是iterrows()不能保留行之间的dtype。
```
start = time.time()
class_list = list()
for index, data_row in data.iterrows():
    petal_length = data_row['petal_length']
    class_num = compute_class(petal_length)
    class_list.append(class_num)

end = time.time()
print("Iterrows run time = {}".format(end - start))
```



#### apply是更优方案

apply本身并不快，但与DataFrame结合使用时，它具有很大的优势。这取决于apply表达式的内容。如果它可以在Cython中执行，那么apply要快得多。
```
start = time.time()
class_list = data.apply(lambda row: compute_class(row['petal_length']), axis=1)
end = time.time()
print("apply() run time = {}".format(end - start))
```



#### 使用内置的函数来替代循环的语句

```
start = time.time()
class_list = pd.cut(x=data.petal_length, bins=[0, 2, 5, 100], include_lowest=True, labels=[1, 2, 3]).astype(int)
end = time.time()
print("cut() run time = {}".format(end - start))
```



### pandas向量化

需要改写计算的函数，将series作为输入
```
def compute_class(series):
    data['class'] = 3
    data.loc[series <= 2, 'class'] = 1
    data.loc[(series < 5) & (series > 2), 'class'] = 2

start = time.time()
compute_class(data["petal_length"])
end = time.time()
print("pandas vectorization  run time = {}".format(end - start))
```



#### numpy向量化

Numpy array会更快
```
start = time.time()
compute_class(data["petal_length"].values)
end = time.time()
print("numpy vectorization  run time = {}".format(end - start))
```



#### 比apply再快10倍

```python
import numpy as np
import pandas as pd
import timeit


def divide(a, b):
    return a / b


def test():
    return df["A"] / df["B"]


if __name__ == '__main__':
    np.random.seed(0)
    N = 10 ** 5
    A_list = np.random.randint(1, 100, N)
    B_list = np.random.randint(1, 100, N)
    df = pd.DataFrame({'A': A_list, 'B': B_list})

    print(timeit.timeit("test()", setup="from __main__ import test", number=1))

    timeit.timeit(list(map(divide, df['A'], df['B'])))  # 43.9 ms
    timeit.timeit(np.vectorize(divide)(df['A'], df['B']))  # 48.1 ms
    timeit.timeit([divide(a, b) for a, b in zip(df['A'], df['B'])])  # 49.4 ms
    timeit.timeit([divide(a, b) for a, b in df[['A', 'B']].itertuples(index=False)])  # 112 ms
    timeit.timeit(df.apply(lambda row: divide(*row), axis=1, raw=True))  # 760 ms
    timeit.timeit(df.apply(lambda row: divide(row['A'], row['B']), axis=1))  # 4.83 s
    timeit.timeit([divide(row['A'], row['B']) for _, row in df[['A', 'B']].iterrows()])  # 11.6 s
```

- 基于tuple的方法（前四个）要远快于基于Series的方法（后三个）
- apply方法设置raw=True，可以大大改善性能

- np.vectorize：可以比上面案例中的for循环提升百倍以上性能

```
start = time.time()
np.vectorize(compute_class)(data["petal_length"])
end = time.time()
print("np.vectorize  run time = {}".format(end - start))
```



### numba：就像坐上了火箭

- numba是一个用于编译Python数组和数值计算函数的编译器，这个编译器能够大幅提高直接使用Python编写的函数的运算速度。

- numba使用LLVM编译器架构将纯Python代码生成优化过的机器码，通过一些添加简单的注解，将面向数组和使用大量数学的python代码优化到与c，c++和Fortran类似的性能，而无需改变Python的解释器。

```
from numba import njit

# 在jit装饰器装饰的函数中，不可以有第三方的package
@njit
def divide(a, b):
    res = np.empty(a.shape)
    for i in range(len(a)):
        if b[i] != 0:
            res[i] = a[i] / b[i]
        else:
            res[i] = 0
    return res

%timeit divide(df['A'].values, df['B'].values)  # 717 µs
```



### Cython
- https://blog.csdn.net/a19990412/article/details/93130293
- https://towardsdatascience.com/use-cython-to-get-more-than-30x-speedup-on-your-python-code-f6cb337919b6


### 其它

- [CuPy](https://towardsdatascience.com/heres-how-to-use-cupy-to-make-numpy-700x-faster-4b920dda1f56) 
- pandas DataFrame数据选取
  - loc：通过index值获取行数据
  - iloc：通过index位置获取行数据
  - ix：结合了前两种的数据获取方式
  - at：类似loc，数据选取更快
  - iat：类似iloc，数据选取更快
-  《python-parallel-programmning-cookbook》：concurrent.futures。。。。
- https://www.ibm.com/developerworks/cn/linux/l-cn-python-optim/index.html
- https://www.cnblogs.com/xybaby/p/6510941.html
- https://towardsdatascience.com/10-python-pandas-tricks-that-make-your-work-more-efficient-2e8e483808ba
- 内存优化
  - https://www.dataquest.io/blog/pandas-big-data/
  - https://www.cnblogs.com/xybaby/p/7488216.html



#### Modin

通过在**系统所有可用的 CPU 核上**自动分配计算来加速 pandas。有了它，对于任何尺寸的 pandas 数据数据集，Modin 声称能够以 CPU 内核的数量得到近乎线性的加速。



#### 用C重写关键计算部分





## 参考资料

- https://realpython.com/fast-flexible-pandas/#pandas-apply
- https://stackoverflow.com/questions/52673285/performance-of-pandas-apply-vs-np-vectorize-to-create-new-column-from-existing-c
- https://towardsdatascience.com/how-to-make-your-pandas-loop-71-803-times-faster-805030df4f06
- https://juejin.im/post/5c62bb92e51d45407f057b45
