---
layout:     post
title:      深度学习框架的加速方法
subtitle:   
date:       2022-11-26
author:     bjmsong
header-img: 
catalog: true
tags:
    - 深度学习框架
---
## Pytorch
- https://pytorch.org/tutorials/distributed/home.html
    + Distributed Data-Parallel Training(DDP)
        * https://blog.csdn.net/fs1341825137/article/details/123233295
    + RPC-Based Distributed Training(RPC)
    + Collective Communication(c10d)
- https://pytorch.org/docs/stable/multiprocessing.html


###
https://github.com/tczhangzhi/pytorch-distributed.git
https://zhuanlan.zhihu.com/p/98535650
https://www.bilibili.com/video/BV1bd4y1X7hb/?spm_id_from=333.337.search-card.all.click&vd_source=7798c62f92ce545f56fd00d4daf55e26

### 
- https://www.bilibili.com/video/BV1yt4y1e7sZ/?spm_id_from=333.788.recommend_more_video.2&vd_source=7798c62f92ce545f56fd00d4daf55e26
- https://zhuanlan.zhihu.com/p/113694038
- https://zhuanlan.zhihu.com/p/361314953


## Horovod
https://cloud.tencent.com/developer/article/1117910
https://eng.uber.com/horovod/
https://zhuanlan.zhihu.com/p/40578792

## 
- 多线程
- 分布式
- GPU加速
- 《动手学深度学习》里面讲到的方法
- ...

## TensorFlow
### 《Hands on ml2》Chapter 19
https://www.tensorflow.org/guide/distributed_training?hl=zh-cn

### 官方文档
- 六个并行计算框架
    - MirroredStrategy：适合一台机器上有多个GPU
        - 同步随机梯度下降
        - 原理：类似MapReduce
            - Ring All-Reduce
    - TPUStrategy
    - MultiWorkerMirroredStrategy
    - CentralStorageStrategy
    - ParameterServerStrategy
    - OneDeviceStrategy
- code
    - learn/MachineLearning/tf_distribute.py
- 业界实践
https://zhuanlan.zhihu.com/p/430383324
- 默认使用全部CPU
https://stackoverflow.com/questions/42845261/does-tensorflow-job-use-multiple-cores-by-default
https://stackoverflow.com/questions/38836269/does-tensorflow-view-all-cpus-of-one-machine-as-one-device
- 数据并行：更容易实现，tensorflow目前只支持这个
https://www.pythonf.cn/read/100405
https://cloud.tencent.com/developer/article/1453361
https://tf.wiki/zh_hans/appendix/distributed.html
- 模型并行
    - 比较难
    - https://www.tensorflow.org/tutorials/distribute/keras?hl=zh-cn
    - https://medium.com/analytics-vidhya/speeding-up-inference-using-parallel-model-runs-d76dcf393567   
- 分布式训练
https://zhuanlan.zhihu.com/p/469541810
https://zhuanlan.zhihu.com/p/56991108
https://www.tensorflow.org/guide/distributed_training?hl=zh-cn
https://github.com/yahoo/TensorFlowOnSpark
https://towardsdatascience.com/distribute-your-pytorch-model-in-less-than-20-lines-of-code-61a786e6e7b0
https://blog.csdn.net/weixin_39589455/article/details/120759372


