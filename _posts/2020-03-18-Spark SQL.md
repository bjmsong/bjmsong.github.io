---
layout:     post
title:      Spark SQL
subtitle:   
date:       2020-03-18
author:     bjmsong
header-img: img/spark/Spark_logo.png
catalog: true
tags:
    - Spark
---

> Spark SQL是Spark的基本组件之一，主要用于结构化数据处理。可以通过sql和Dataset API和Spark SQL进行交互。



### SQL

Spark SQL的一个用途是可以执行sql语句，可以直接和Hive交互，或者通过JDBC/ODBC和mysql、oracle等数据库交互。如果在其它语言中使用sql会返回Dataset/DataFrame。



### Dataset,DataFrame,RDD

- Dataset

  - Dataset是Spark 1.6引入的，具备RDD强类型、可以使用匿名函数等优点，同时又获得了Spark SQL的优化执行引擎
  - Dataset API目前只有Scala和Java，尚不支持Python

- DataFrame

  - A DataFrame is a Dataset organized into named columns

  - DataFrame的概念是和R/Python里面的DataFrame的概念一致的

  - 它提供的更像是一个传统的数据库里面的表，他除了数据之外还能够知道更多的信息，比如说列名、列值和列的属性，这一点就和hive很类似了，而且他也能够支持一些复杂的数据格式

  - 从API应用的角度来说DataFrame提供的API他的层次更高，比RDD编程还要方便，学习的门槛更低

  - 在Scala API中，DataFame仅仅只是Dataset[Row]的别名（即每一行的类型是Row）

    <ul> 
    <li markdown="1"> 
    ![]({{site.baseurl}}/img/spark/rdd-DataFrame.png) 
    </li> 
    </ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/spark/rdd_dataframe_dataset.png) 
</li> 
</ul> 

- 相同：
	- 惰性机制
	- 根据内存情况自动缓存运算？
	- partition概念，如mappartition对每一个分区进行操作，数据量小，而且可以将运算结果拿出来，map中对外面变量的操作是无效的
	- 有共同的方法，如filter、排序等
	
- 不同：
    - RDD: 不支持spark sql，支持spark mllib
    - DataFrame、Dataset：支持spark ml、spark sql
	- DataFrame、Dataset 比 RDD做了更多的优化 
	
- [互相转换](https://stackoverflow.com/questions/29383578/how-to-convert-rdd-object-to-dataframe-in-spark/42469625#42469625)

  - DataFrame/Dataset转RDD: `df.rdd`

  - RDD转DataFrame

    ```scala
    import spark.implicits._
    val testDF = rdd.map(line=> (line._1,line._2)).toDF("col1","col2")
    ```

  - RDD转DataSet

      ```scala
      import spark.implicits._
      case class Coltest(col1:String,col2:Int) extends Serializable //定义字段名和类型
      val testDS = rdd.map(line=> Coltest(line._1,line._2)).toDS
      ```

  - Dataset转DataFrame

      ```scala
      import spark.implicits._
      val testDF = testDS.toDF	
      ```

  - DataFrame转Dataset

      ```scala
      case class Coltest(col1:String,col2:Int)extends Serializable  	//定义字段名和类型
      val testDS = testDF.as[Coltest]
      ```



### UDF

- UDF(User-defined functions, UDFs),即用户自定义函数，在Spark Sql的开发中十分常用，UDF对表中的每一行进行函数处理，返回新的值，有些类似与RDD编程中的Map()方法，pandas中的apply方法

```scala
import spark.implicits._

//生成一个DataFrame
val df = Seq(
    (1, "boy", "裤子"),
    (2, "girl", "裤子"),
    (3, "boy", "裙子"),
    (4, "girl", "裙子"),
    (5, "girl", "裙子")
).toDF("id", "sex", "dressing")
df.createOrReplaceTempView("boys_and_girls")

//找变态函数
def findHentai(sex:String,dressing:String): String ={
    if(sex =="boy" && dressing == "裙子") "变态" else "正常"
}
```

- register方法

  ```scala
  //常用写法
  spark.udf.register("find_hentai",findHentai _ )
  //这种写法也行
  spark.udf.register("findHentai",findHentai(_:String,_:String))
  //这个也是一样的，spark2之前的都是用sqlContext去注册
  spark.sqlContext.udf.register("find_the_one",findHentai _)
  
  //调用udf
  spark.sql(s"select id,sex,dressing,find_the_one(sex,dressing) as tag from boys_and_girls").show()
  ```

- udf方法

  ```scala
  import org.apache.spark.sql.functions.{udf,col}
  //注册函数
  val who_is_hentai = udf(findHentai(_:String,_:String))
  //调用函数
  df.select(col("id"),col("sex"),col("dressing"),who_is_hentai(col("sex"),col("dressing"))).show()
  ```

  



### 参考资料

- https://spark.apache.org/docs/latest/sql-programming-guide.html
- https://medium.zenika.com/a-comparison-between-rdd-dataframe-and-dataset-in-spark-from-a-developers-point-of-view-a539b5acf734
- https://www.infoq.cn/article/three-apache-spark-apis-rdds-dataframes-and-datasets
- https://zhuanlan.zhihu.com/p/29830732
- https://www.jianshu.com/p/b1e9d5cc6193