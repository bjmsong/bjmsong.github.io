---

layout:     post
title:      æŸå¤±å‡½æ•°
subtitle:   Loss Function
date:       2020-02-21
author:     bjmsong
header-img: img/machineLearning/machineLearning.png
catalog: true
tags:
    - æœºå™¨å­¦ä¹ 
---



### ç›‘ç£å­¦ä¹ å­¦ä¹ è¿‡ç¨‹

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/loss/High-level-training-process.jpg) 
</li> 
</ul>

- Forward pass 
  - the training data is fed into the machine learning model 
- Loss :
  - compare between some actual targets and predicted targets
  - the lower the loss, the more the set of targets and the set of predictions resemble each other
  - the more they resemble each other, the better the machine learning model performs.
- Backward pass
  - propagating the error backwards to the model structure, such as the modelâ€™s weights
  - many ways for optimizing the mode
    - gradient descent based methods
      - back propagation ï¼šneural networks
    - quadratic optimization ï¼šSVM





### æŸå¤±å‡½æ•°

- æ¨¡å‹ä¼˜åŒ–çš„ç›®æ ‡å‡½æ•°ï¼Œé€šè¿‡ä¼˜åŒ–æŸå¤±å‡½æ•°å¾—åˆ°æ¨¡å‹çš„å‚æ•°
- é’ˆå¯¹ä¸åŒçš„é—®é¢˜éœ€è¦é€‰å–åˆé€‚çš„æŸå¤±å‡½æ•°

#### åˆ†ç±»é—®é¢˜

- 0-1 loss
  
  $$
  L(Y,f(X)) = \left\{
  \begin{aligned}
  1, Y\neq f(X) \\
  0, Y= f(X)
  \end{aligned}
  \right.
  $$
  
  - éå‡¸ï¼Œéå…‰æ»‘ï¼Œå¾ˆéš¾å¯¹å…¶è¿›è¡Œä¼˜åŒ–


- Hinge loss
  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/loss/hinge loss.png) 
  </li>
  </ul>
  
  - é€šå¸¸è¢«ç”¨äºæœ€å¤§é—´éš”ç®—æ³•ï¼Œæœ€å¤§é—´éš”ç®—æ³•æ˜¯SVMç”¨åˆ°çš„é‡è¦ç®—æ³•
  
  - å½“
    $$
    Yf(X)\geq 1
    $$
  
    è¯¥å‡½æ•°ä¸å¯¹å…¶åšä»»ä½•æƒ©ç½šã€‚
  
  - Hinge lossåœ¨Yf(X)=1å¤„ä¸å¯å¯¼ï¼Œå› æ­¤ä¸èƒ½ç”¨æ¢¯åº¦ä¸‹é™æ³•è¿›è¡Œä¼˜åŒ–ï¼Œè€Œæ˜¯ä½¿ç”¨æ¬¡æ¢¯åº¦ä¸‹é™æ³•ã€‚å› ä¸ºå…¶å›¾åƒåƒæ‰“å¼€çš„ä¸€æœ¬ä¹¦ï¼Œæ‰€ä»¥ä¹Ÿå«åˆé¡µæŸå¤±ã€‚
  
  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/loss/hinge loss.jpeg) 
  </li> 
  </ul> 
  
- Squared hinge

  - The squared hinge loss is like the hinge formula displayed above, but then the max() function output is squared
  - more sensitive to outliers
  - differentiable

- Categorical / multiclass hinge

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/loss/multiclass hinge.png) 
  </li> 
  </ul> 

- Binary cross entropy(äº¤å‰ç†µ)ï¼Œä¹Ÿå«logloss

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/loss/binary cross entropy.png) 
  </li> 
  </ul> 

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/loss/bce-1.png) 
  </li> 
  </ul> 

- Categorical cross entropy

- Kullback-Leibler divergence(KLæ•£åº¦)

  - comparison between two probability distributions

- Logistic loss

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/loss/logistic loss.png) 
  </li> 
  </ul> 
  
  - è¯¥å‡½æ•°å¤„å¤„å…‰æ»‘ï¼Œå› æ­¤å¯ä»¥ç”¨æ¢¯åº¦ä¸‹é™æ³•è¿›è¡Œä¼˜åŒ–ã€‚ä½†æ˜¯ï¼Œè¯¥æŸå¤±å‡½æ•°å¯¹æ‰€æœ‰çš„æ ·æœ¬ç‚¹éƒ½æœ‰æ‰€æƒ©ç½šï¼Œå› æ­¤å¯¹å¼‚å¸¸å€¼æ›´æ•æ„Ÿã€‚

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/loss/äºŒåˆ†ç±»é—®é¢˜çš„æŸå¤±å‡½æ•°.png) 
</li> 
</ul> 



#### å›å½’é—®é¢˜

- Mean Absolute Error (MAEï¼ŒL1 Loss)

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/loss/mae.png) 
  </li> 
  </ul> 

  - ç¼ºç‚¹
    - åœ¨x=0å¤„ä¸å¯å¯¼ï¼Œæ¢¯åº¦è®¡ç®—å¤æ‚ï¼Œéœ€è¦å€ŸåŠ©çº¿æ€§è§„åˆ’æ±‚è§£
    - gradients are continuously large (Grover, 2019)ï¼Œeasy to overshoot the minimum continously, finding a suboptimal model
      - Consider Huber loss (more below) if you face this problem
    - if your average error is very small, it may be better to use the Mean Squared Error
      - è¯¯å·®æ¥è¿‘0çš„æ—¶å€™ä¸å¹³æ»‘ï¼Œè¾ƒå°‘ä½¿ç”¨
    - åªå…³æ³¨è¯¯å·®å¤§å°ï¼Œä¸å…³æ³¨è¯¯å·®æ–¹å‘

- **Mean Squared Error (MSE)**

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/loss/mse.png) 
  </li> 
  </ul> 

  - å¤„å¤„å¯å¯¼ï¼šä¼˜åŒ–æ›´å®¹æ˜“
  - large errors introduce a much larger cost than smaller errors
    - ä¼˜ç‚¹ï¼šwhen your errors are small, because optimization is then advanced
    - ç¼ºç‚¹ï¼šå¯¹å¼‚å¸¸å€¼è¿‡äºæ•æ„Ÿï¼Œåªå…³æ³¨è¯¯å·®å¤§å°ï¼Œä¸å…³æ³¨è¯¯å·®æ–¹å‘

- Mean Absolute Percentage Errorï¼ˆMAPEï¼‰

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/loss/mape.png) 
  </li> 
  </ul> 

- Root Mean Squared Error(RMSEï¼ŒL2 Loss)

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/loss/rmse.png) 
  </li> 
  </ul> 

- **Logcosh**

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/loss/logcosh.png) 
  </li> 
  </ul> 

  - log(cosh(x)) is approximately equal to (x ** 2) / 2 for small x and to abs(x) - log(2) for large x
  - This means that `logcosh` works mostly like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect prediction

- **Huber loss**

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/loss/huber loss.png) 
  </li> 
  </ul> 

  - Huber loss approaches MSE when ğ›¿ ~ 0 and MAE when ğ›¿ ~ âˆ (large numbers.)

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/loss/å›å½’é—®é¢˜çš„æŸå¤±å‡½æ•°.png) 
</li> 
</ul> 





### loss function å’Œ metrcis çš„åŒºåˆ«

- lossæ˜¯æ¨¡å‹ä¼˜åŒ–çš„ç›®æ ‡
  - å¯å¯¼ ï¼šåšæ¢¯åº¦ä¸‹é™
- metrics ä¸éœ€è¦å¯å¯¼ï¼Œæ›´æ¥è¿‘ä¸šåŠ¡ç›®æ ‡



### å‚è€ƒèµ„æ–™

- https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/

- ã€Šç™¾é¢æœºå™¨å­¦ä¹ ã€‹

- https://www.analyticsvidhya.com/blog/2019/08/detailed-guide-7-loss-functions-machine-learning-python-code/

- https://www.cnblogs.com/wkslearner/p/9833191.html

- [çº¿æ€§å›å½’é‡‡ç”¨å¹³æ–¹è¯¯å·®ä½œä¸ºlossçš„è§£é‡Š](https://mp.weixin.qq.com/s?__biz=MzU0NzAxNTYyMQ==&mid=2247483916&idx=1&sn=96ea01f6bc4c018a11dfc677b1c2ed81&chksm=fb559ca6cc2215b03263442a7cccc39b5afd5c1a3f61ecc188e653686f9d77ecabdc2e4d338c&mpshare=1&scene=1&srcid=1217RBU9Qs1IY8OtPTXkMQvF#rd)

- https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23

- https://www.jianshu.com/p/ac26866e81bc

- https://algorithmia.com/blog/introduction-to-loss-functions

- http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/

- https://mp.weixin.qq.com/s?__biz=MzI1MzY0MzE4Mg==&mid=2247485541&idx=1&sn=b914a4ba2250597a4bc48f45401b0c45&chksm=e9d018a4dea791b20ce2a6752a2f4bb2ce222713d7cade58a40974a3ddd78e78adc53a73ff86&mpshare=1&scene=1&srcid=&sharer_sharetime=1572829222282&sharer_shareid=49581f7bdbef8664715f595bc62d7044&key=8a4bbb55c6c79ce68e2a634f845f562fd0b6fbf82bb629c5d5df10d62d40127c3f82024cec1aeb41435cfc5df399eaad37653eccd4de1ee6f89c9e333590441d827e165572701cb224ede4157035ac41&ascene=1&uin=MjM1OTMwMzkwMA%3D%3D&devicetype=Windows+7&version=62070152&lang=zh_CN&pass_ticket=LP%2BeJbVSoUnpFGrFF0jkMhYuRBFEXQ4jgRZzE3Lc3uQG2Hh9YJp6FpIsB6GCp5dU

  

