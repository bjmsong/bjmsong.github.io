---

layout:     post
title:      损失函数
subtitle:   Loss Function
date:       2020-02-21
author:     bjmsong
header-img: img/machineLearning/machineLearning.png
catalog: true
tags:
    - 机器学习
---



## 学习过程



## 监督学习学习过程

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/High-level-training-process.jpg) 
</li> 
</ul>

- Forward pass 
  - the training data is fed into the machine learning model 
- Loss :
  - compare between some actual targets and predicted targets
  - the lower the loss, the more the set of targets and the set of predictions resemble each other
  - the more they resemble each other, the better the machine learning model performs.
- Backward pass
  - propagating the error backwards to the model structure, such as the model’s weights
  - many ways for optimizing the mode
    - gradient descent based methods
      - back propagation ：neural networks
    - quadratic optimization ：SVM





## 损失函数

- 模型优化的目标函数，通过优化损失函数得到模型的参数
- 针对不同的问题需要选取合适的损失函数

### 二分类问题

- 0-1 loss
  $$
  L(Y,f(X)) = \left\{
  \begin{aligned}
  1, Y\neq f(X) \\
  0, Y= f(X)
  \end{aligned}
  \right.
  $$
  

  - 非凸，非光滑，因此需要找代理损失函数

- Hinge loss
  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/hinge loss.png) 
</li> 
  </ul> 
  
  - 通常被用于最大间隔算法，最大间隔算法是SVM用到的重要算法
  
  - 当
  $$
  Yf(X)\geq 1
  $$
  该函数不对其做任何惩罚。Hinge loss在Yf(X)=1处不可导，因此不能用梯度下降法进行优化，而是使用次梯度下降法。因为其图像像打开的一本书，所以也叫合页损失。
  
  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/hinge loss.jpeg) 
  </li> 
  </ul> 
  
- Squared hinge

  - The squared hinge loss is like the hinge formula displayed above, but then the max() function output is squared
  - more sensitive to outliers
  - differentiable

- Categorical / multiclass hinge

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/multiclass hinge.png) 
  </li> 
  </ul> 

- Binary cross entropy(交叉熵)

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/binary cross entropy.png) 
  </li> 
  </ul> 

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/bce-1.png) 
  </li> 
  </ul> 

- Categorical cross entropy

- Kullback-Leibler divergence(KL散度)

  - comparison between two probability distributions

  

- Logistic loss
  $$
  L(Y,f(X)) =
  $$

  该函数处处光滑，因此可以用梯度下降法进行优化。但是，该损失函数对所有的样本点都有所惩罚，因此对异常值更敏感。

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/二分类问题的损失函数.png) 
</li> 
</ul> 



### 回归问题

- Mean Absolute Error (MAE，L1 Loss)

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/mae.png) 
  </li> 
  </ul> 

  - 缺点
    - 在x=0处不可导
    - gradients are continuously large (Grover, 2019)，easy to overshoot the minimum continously, finding a suboptimal model
      - Consider Huber loss (more below) if you face this problem
    - if your average error is very small, it may be better to use the Mean Squared Error

- **Mean Squared Error (MSE)**

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/mse.png) 
  </li> 
  </ul> 

  - 处处可导：优化更容易
  - large errors introduce a much larger cost than smaller errors
    - 优点：when your errors are small, because optimization is then advanced
    - 缺点：对异常值过于敏感

- Mean Absolute Percentage Error（MAPE）

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/mape.png) 
  </li> 
  </ul> 

- Root Mean Squared Error(RMSE，L2 Loss)

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/rmse.png) 
  </li> 
  </ul> 

- **Logcosh**

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/logcosh.png) 
  </li> 
  </ul> 

  - log(cosh(x)) is approximately equal to (x ** 2) / 2 for small x and to abs(x) - log(2) for large x
  - This means that `logcosh` works mostly like the mean squared error, but will not be so strongly affected by the occasional wildly incorrect prediction

- **Huber loss**

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machineLearning/huber loss.png) 
  </li> 
  </ul> 

  - Huber loss approaches MAE when 𝛿 ~ 0 and MSE when 𝛿 ~ ∞ (large numbers.)

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/回归问题的损失函数.png) 
</li> 
</ul> 



## 参考资料

- https://www.machinecurve.com/index.php/2019/10/04/about-loss-and-loss-functions/

- 《百面机器学习》

- https://www.analyticsvidhya.com/blog/2019/08/detailed-guide-7-loss-functions-machine-learning-python-code/

- https://www.cnblogs.com/wkslearner/p/9833191.html

  

  

