---
layout:     post
title:      大模型与分布式训练
subtitle:   
date:       2022-11-23
author:     bjmsong
header-img: 
catalog: true
tags:
    - ML System
---
- b站 ZOMI酱

## 背景：大规模分布式训练系统
- 深度学习迎来大模型(Foundation Models)
    + 优点
        * 解决模型碎片化问题(针对不同应用场景需要定制化开发不同模型的问题)，提供统一的预训练+下游任务微调的方案
        * 具备自监督学习功能，可以减少数据标注，降低训练研发成本
        * 模型参数规模越大，有望进一步突破现有模型结构的精度局限
- 分布式深度学习的意义
    + 深度学习训练耗时 = 训练数据规模 \* 单步计算量/计算速率
    + 计算速率 = 单设备计算速率 \* 设备数 \* 多设备并行效率(加速比)

## 大模型训练的挑战
- 内存墙
    + 200B参数，参数内存占用754GB，训练过程需要3500GB内存(权重+激活+优化器状态),需要100多张32GB显存的卡才能装下
    + 训练过程中内存占用会逐渐增大到峰值，然后逐渐减少到一个固定值
    + 静态内存：模型自身的权重参数、优化器状态信息，比较固定
    + 动态内存：训练过程中产生的前向输出、梯度输出、算子计算临时变量，会在反向传播时逐渐释放掉的内存
- 通信墙
    + 通讯过程，需要综合考虑参数量、计算量、计算类型、样本量、集群拓扑和通讯策略等不同因素，才能设计出性能较优的切分策略，最大化利用通讯效率，提高通讯比
- 性能墙：计算资源的利用率
    + 大规模训练技术中，不仅要求AI芯片的计算性能足够强悍，同时也依赖于AI框架的大规模分布式训练的运行和调度效率，以及分布式并行等各种优化手段的权衡 
- 调优墙
    + 在数千节点的集群上，要保证计算的正确性/性能/可用性，手工分布式难以全面兼顾

## 分布式训练系统
- 并行处理硬件架构
    + SISD
        * 每个指令部件每次仅译码一条指令，而且在执行时仅为操作部件提供一份数据
        * 串行计算，硬件不支持并行计算；在时钟周期内，CPU只能处理一个数据流
    + SIMD
        * 
    + MISD
    + MIMD
- AI框架中的分布式训练

## 分布式并行总体架构
- 参数服务器模式
- 集合通讯模式

## 算法结构/分类
- 从Transformer到预训练
- 多模态大模型
- MOE万亿大模型

## 分布式并行与同步策略
- 数据并行(DP)
    + 梯度累积(Gradient Accumulation)
        * 同步（业界采用）: Next round of local computing cannot continue all working nodes having completed this commnuication
            - 机器互相等待，造成资源浪费，效率不高
            - master-slave方式
            - ring-allreduce方式
        * 异步：After the current batch iteration，commnuicate with other servers to transmit network parameters
            - 模型很难收敛  
    + 数据并行(DP)
        * automatically splits training data and send model jobs to multiple GPUs. After each model completed, Data Parallel will Accumulate Gradients(All-Reduce)，然后不断迭代
    + 分布式数据并行(DDP)
        * 多进程
        * 梯度通信和梯度计算同时进行
    + 全切片数据并行(FSDP)
        * shards all of model's parameters,gradients and optimizer states across data-parallel workers and can optionally offload the shared model parameters to CPUs
- 模型并行
    + 张量并行
    + 流水线并行
- 混合并行

## 内存和计算优化


## 通讯原语与协调


## 参考资料
- https://github.com/chenzomi12/DeepLearningSystem



