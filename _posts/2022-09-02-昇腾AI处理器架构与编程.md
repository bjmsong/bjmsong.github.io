---
layout:     post
title:      昇腾AI处理器架构与编程
subtitle:   
date:       2022-09-02
author:     bjmsong
header-img: 
catalog: true
tags:
    - 深度模型加速
---
## Chapter1.基础理论
### 多层感知机
- 参数太多
- 难以捕捉图像局部丰富的特征
### 卷积神经网络
- LeNet5(1998) -> AlexNet(2012) -> Resnet,VGG,GoogLeNet
- 输入层处理
    - 长*宽*像素数量
    - 归一化
- 卷积层
    - 卷积核
        - 三维矩阵
        - 同一层有多个卷积核，用来提取不同的特征(如颜色、形状等)
        - 同一层卷积核大小相同，不同层的卷积核大小可以不同
        - 感受野：卷积核作用的卷积窗口对应于原始输入特征图的部分
    - 卷积核操作完成输出后，输出特征图中的每一个特征值还需要通过激活函数进行处理
        - 引入非线性，增强了对于非线性特征的拟合能力
    - 特性
        - 局部连接
            - 每个神经元只针对输入特征图中感受野范围内的数据进行处理，使得权重参数锐减
            - 在特征图像中，每一个像素和相邻像素的联系比较紧密，和距离较远的像素相关性较弱
        - 权重共享
            - 同一层卷积层中不同神经元使用了同一个卷积核进行卷积计算，极大精简了参数数量
    - 卷积层计算方法
        - 直接卷积
            - 大多数卷积神经网络加速器都是采用直接卷积的计算模式
                - TPU：脉动阵列的计算方式
            - 步骤
                - 先对输入特征图补零(padding)
                    - 输出特征图和输入特征图大小保持一致
                    - 保护输入特征图的边缘特征信息
                - 卷积核在输入特征图中滑动，进行点积计算
        - 矩阵乘法实现卷积
            - 应用在CPU、GPU等具有通用编程性的计算芯片上
                - 昇腾AI处理器
            - 步骤
                - 通过Img2Col将卷积层中的输入特征图和卷积核权重矩阵展开
                    - 在计算过程中将需要计算的特征子矩阵存放在连续的内存中，有利于一次将所需要计算的数据直接按照需要的格式取出进行计算，减少了内存的访问次数
                - 将卷积中输入特征子矩阵和权重矩阵对应元素的点积操作转换成矩阵运算中行与列向量的乘加运算
                    - CPU和GPU都提供专门的基本线性代数程序集(BLAS)来高效地实现向量和矩阵运算
        - 快速傅里叶变换(FFT)和Winograd算法
            - 将复杂的卷积运算等价变换成另一个空间的简单运算，从而降低了计算的复杂度
                - cuDNN部分使用了Winograd算法
- 池化层
    - 在相邻卷积层之间规律性地插入池化层，通过池化层进一步减少每一层特征值，保留为主要特征，减少了参数量
    - 池化滤波器
        - 最大值、平均值
        - 尺寸F，步长S
- 全连接层输出
- 优化加速
    - 突触并行
        - 同一个卷积窗口中的乘法运算可以并行执行
    - 神经元并行
        - 每个卷积窗口的计算过程没有数据依赖，可以并行计算
    - 输入特征图并行
        - 通道级别的并行处理
    - 输出特征图并行
        - 多个卷积核的操作可以实现并行计算
    - 批处理并行
        - 一次输入一个批次的图像进行批处理
### 应用示例


## Chapter2.行业背景
### 神经网络芯片现状
- CPU
    - 性能提升遇到瓶颈
    - 侧重于指令执行中的逻辑控制
- GPU
    - 大规模硬件并行
    - 擅长大规模密集型数据并行计算
    - CUDA
        - 提供了GPU硬件的直接访问接口，可以直接使用一种类C语言的方式对GPU编程
    - 张量核(Tensor Core)
    - TFLOPS: 每秒浮点数计算次数
    - TOPS: 每秒整型数计算次数 
    - 缺点：功耗高
- TPU
    - 采用低精度计算
- FPGA
### 神经网络芯片加速理论
- GPU加速理论
    - 并行化，矢量化
    - 最常见的GPU加速神经网络的模式为通用矩阵相乘(GEMM)
        - 对于各类神经网络，将其核心计算展开成矩阵运算的形式
        - 矩阵相乘：单指令多线程，一个线程计算结果矩阵的一个点(对应着输入左矩阵的一行与输入右矩阵的一列相运算后得到的结果)
    - 现代GPU架构:Turing
        - 流处理器
            - CUDA核
                - 负责单精度浮点数FP32的计算
            - 张量核
                - 为深度神经网络算法提供更强大、更专用、更高效的算力
                - 支持FP16,INT8,INT4等多精度计算: 精度较低，但满足绝大多数神经网络的需求
        - 多层级的片上存储器结构
        - 网络互联结构
- TPU加速理论
    - 脉动阵列
### 深度学习框架
- Mindspore
- Caffe
- TensorFlow
    - 静态图：定义完计算图后，运行计算图时才能获取数据
    - 静态数据流图来表示神经网络中数据的计算过程
        - 用户不关心底层硬件调度以及计算实现细节
        - 框架能够在运行时知道计算图的所有细节，能够对计算图进行更深层次的优化
    - 支持分布式计算
    - 不是一个普通的Python库，本质上是一种新的特定域语言(DSL)
- PyTorch
    - 动态图：每执行一条语句，系统变构建出相应的计算图，且数据在实时进行计算
### 深度学习编译框架：TVM
- 痛点：不同的软件编程方法和框架在不同的硬件架构上的优化实现差异很大
- 编译语言中间表示框架LLVM
    - 所有的软件框架都不直接映射到具体的硬件上，而是首先通过前端编译器编译成一种中间格式的指令表达(IR)
    - 硬件供应商可以分别提供对接到中间指令表达的后端编译器，实现从IR到具体硬件指令的通路
- 专门用于深度学习的中间表达形式、编译器和执行器
    - TVM

## Chapter3.硬件架构
### 昇腾AI处理器总览
- 
### 达芬奇架构
- 
### 卷积加速原理
- 

## Chapter4.软件架构
### 昇腾AI软件栈总览
- 
### 神经网络软件流
- 
### 开发工具链
- 

## Chapter5.编程方法
### 深度学习开发基础
- 
### 昇腾AI软件栈中的技术
- 
### 自定义算法开发
- 
### 自定义应用开发
- 

## Chapter6.实战案例
### 评价标准
- 
### 图像识别
- 
### 目标检测
- 

## 参考资料
《昇腾AI处理器架构与编程》