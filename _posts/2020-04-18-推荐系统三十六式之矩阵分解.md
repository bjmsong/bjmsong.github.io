---
layout:     post
title:      推荐系统三十六式之
subtitle:   矩阵分解
date:       2020-04-18
author:     bjmsong
header-img: img/Recommendation System/th.jpg
catalog: true
tags:
    - 推荐系统
---

> [关于矩阵分解，可以参考我的另外一篇博客](https://bjmsong.github.io/2019/09/04/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%8F%AC%E5%9B%9E%E7%AE%97%E6%B3%95%E4%B9%8B%E9%9A%90%E8%AF%AD%E4%B9%89%E6%A8%A1%E5%9E%8B/)



### 矩阵分解

- **近邻模型的缺点**
    - 物品之间存在相关性，信息量并不随着向量维度增加而线性增加
    - 矩阵元素稀疏，计算结果不稳定，增减一个向量维度，导致近邻结果差异很大的情况存在
- **矩阵分解，就是把原来的大矩阵，近似分解成两个小矩阵的乘积，在实际推荐计算时不再使用大矩阵，而是使用分解得到的两个小矩阵：**
    - 用户偏好矩阵
    - 物品主题矩阵



#### SVD

- Yehuda Koren在Netflix Prize中提出

- 优化方法
    - 随机梯度下降：SGD
    - 交替最小二乘：ALS
      - 步骤
        - 初始化随机矩阵 Q 里面的元素值；
        - 把 Q 矩阵当做已知的，直接用线性代数的方法求得矩阵 P；
        - 得到了矩阵 P 后，把 P 当做已知的，故技重施，回去求解矩阵 Q；
        - 上面两个过程交替进行，一直到误差可以接受为止
      - 优点
        - 在交替的其中一步，也就是假设已知其中一个矩阵求解另一个时，要优化的参数是很容易并行化的
        - 在不那么稀疏的数据集合上，交替最小二乘通常比随机梯度下降要更快地得到结果

- 算法优化

    - 增加偏置信息
    - 增加隐式反馈和个人属性 ： SVD++
    - 考虑时间因素

    

#### 预测行为（隐式反馈）

- **One-Class问题：只有正样本**
    
- `weighted-ALS`

    - **如果用户对物品无隐式反馈则认为评分是 0；**
        - **不能一股脑儿使用所有的缺失值作为负类别，挑一部分缺失值作为负类别样本即可**
            - 按照物品的热门程度采样：热门内容往往有更高的曝光率，用户反复看到却没产生点击，就潜在的表达了一种厌恶
    - **如果用户对物品有至少一次隐式反馈则认为评分是 1，次数作为该评分的置信度。**

    <ul> 
    <li markdown="1">
    ![]({{site.baseurl}}/img/Recommendation System/36/weighted-ALS.png) 
    </li> 
    </ul> 

- 推荐计算
    
    - 用户和物品的隐因子向量两两相乘
    - 用户和物品数量巨大的情况下，计算开销很大
    - 工程优化
        - **利用专门设计的数据结构存储所有物品的隐因子向量**，**从而实现通过一个用户向量可以返回最相似的 K 个物品**
            - 开源实现：**Faiss、Annoy、KGraph、NMSLIB**
        - **先对物品的隐因子向量做聚类**，海量的物品会减少为少量的聚类。然后再逐一计算用户和每个聚类中心的推荐分数，给用户推荐物品就变成了给用户推荐物品聚类



### Learning to Rank

- 矩阵分解的不足
    
    - 本质上都是在预测用户对一个物品的偏好程度
    - 得到这样的矩阵分解结果后，常常在实际使用时，又是用这个预测结果来排序  
    - **为何不直接预测排序结果？**

- **point-wise**：**预测单个用户对单个物品的偏好程度，得到结果后再排序**
    
    - 如：矩阵分解
    - **存在问题：只有正样本，没有负样本，认为缺失值就是负样本，再以预测误差为评判标准去使劲逼近这些样本。逼近正样本没问题，但是同时逼近的负样本只是缺失值而已，还不知道真正呈现在用户面前，到底是不喜欢还是喜欢呢？**
    
- **pair-wise：直接预测物品两两之间相对顺序**
    
    - **贝叶斯个性化排序(BPR)**
        
        - 构造样本：用户、物品 1、物品 2、两个物品相对顺序
        
            - “两个物品的相对顺序”，取值是：
            - 如果物品 1 是消费过的，而物品 2 不是，那么相对顺序取值为 1，是正样本；
            - 如果物品 1 和物品 2 刚好相反，则是负样本；
            - 样本中不包含其他情况：物品 1 和物品 2 都是消费过的，或者都是没消费过的
        
        - 目标函数
            - 先假装矩阵分解结果已经有了，于是就计算出用户对于每个物品的推荐分数，只不过这个推荐分数可能并不满足均方根误差最小，而是满足物品相对排序最佳。
            - 得到了用户和物品的推荐分数后，就可以计算四元组的样本中，物品 1 和物品 2 的分数差（Xu12）。希望的情况是：如果物品 1 和物品 2 相对顺序为 1，那么希望两者分数之差是个正数，而且越大越好；如果物品 1 和物品 2 的相对顺序是 0，则希望分数之差是负数，且越小越好。
            - 然后再用 sigmoid 函数把这个分数差压缩到 0 到 1 之间
            
            <ul> 
            <li markdown="1">
            ![]({{site.baseurl}}/img/Recommendation System/36/bpr目标函数.png) 
            </li> 
            </ul> 
        
        - 训练模型：结合重复抽样的梯度下降
            
        - 评价指标：AUC
        
            - 数学上等价于：模型把关心的那一类样本排在其他样本前面的概率，最大是1，完美结果，而 0.5 就是随机排列，0 就是完美地全部排错
    
<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/Recommendation System/36/矩阵分解.jpg) 
</li> 
</ul> 


### 深度学习
- **Neural Collaborative Filtering （NCF）：用非线性的神经网络结构泛化矩阵分解模型**
- Variational Autoencoders for Collaborative Filtering


### 其它Model-based算法
- Bayesian networks ： https://en.wikipedia.org/wiki/Bayesian_network
- clustering models
- latent semantic models ： singular value decomposition
- probabilistic latent semantic analysis
- multiple multiplicative factor
- latent Dirichlet allocation 
- Markov decision process based models

### Hybrid算法
混合Memory-based算法和Model-based算法


### 参考资料
- Matrix Factorization and Collaborative Filtering ， 演示文稿，Daryl Lim
- Factorization Meets the Neighborhood: a Multifaceted Collaborative Filtering Model
- BPR- Bayesian Personalized Ranking from Implicit Feedback
- Collaborative Filtering for Implicit Feedback Datasets
    - 处理点击等隐式反馈数据的矩阵分解模型
- Matrix Factorization Techniques For Recommender Systems
    - 大神 Yehuda Koren 对矩阵分解在推荐系统中的应用做的一个普及性介绍，值得一读
- The BellKor Solution to the Netflix Grand Prize