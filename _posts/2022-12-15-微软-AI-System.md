---
layout:     post
title:      微软-AI-System
subtitle:   
date:       2022-12-15
author:     bjmsong
header-img: 
catalog: true
tags:
    - ML System
---
## 学习资料
- https://github.com/microsoft/AI-System
- 中文教材  
    - https://github.com/microsoft/AI-System/tree/main/Textbook
    - 更详细
- ppt
    - https://github.com/microsoft/AI-System/tree/main/Lectures
- lab
    - https://github.com/microsoft/AI-System/tree/main/Labs
- 大量的参考文献

## 第1章-人工智能系统概述
- 本书介绍的人工智能系统（Artificial Intelligence System）主要是指深度学习系统（Deep Learning System），但是这些系统设计原则大部分也适合于机器学习系统（Machine Learning System）
- 深度学习算法本身无论从模型设计，训练方式也借鉴了很多传统机器学习算法的经典理论与实践方式，但是深度学习系统本身相比机器学习系统从硬件到软件层有更多新的挑战和演化，数据与问题规模变得更大，应用场景与部署也更加广泛。
- 综合起来深度学习系统会综合考虑和借鉴机器学习系统（Machine Learning System），大数据系统（Big Data System）和高性能计算（High Performance Computing）领域和社区中的经典的系统设计，优化与问题解决方法，并演化出针对深度学习算法特点的新的系统设计。
- 深度学习模型变化趋势
    - 更大的模型
        - 以Transformer为基本结构的代表性预训练神经语言模型（Neural Language Model），例如，BERT，GPT-3 等，在自然语言处理和计算机视觉等场景应用越来越广泛。其不断增加的层数和参数量，对底层系统内存管理，分布式训练算子放置，通信，以及硬件设计提出了很大的挑战。
    - 更灵活的结构和建模能力
        - 图神经网络等网络不断抽象多样且灵活的数据结构（例如：图（Graph），树（Tree）等），应对更为复杂的建模需求。进而衍生了新的算子（例如：图卷积等）与计算框架（例如：图神经网络框架等）。
    - 更稀疏的模型结构与模型融合（Model Ensemble）
        - 以多专家模型（Mixture of Experts， MoE） 和 Pathways 模型结构为代表的融合模型结构让运行时的模型更加动态（Dynamic）和稀疏（Sparse），进而提升模型的训练效率减少训练代价，支持更多的任务。给系统设计中以往的静态分析方式带来了不小的挑战，同时驱动运用即时编译（Just In Time Compiling）和运行期（Runtime）更加高效的调度与优化。
    - 更大规模的搜索空间
        - 用户定义更大规模的超参数与模型结构搜索空间，通过超参数搜索优化（HPO）与神经网络结构搜索（NAS）自动化找到最优的模型结构。自动化机器学习（AutoML）为代表的训练方式，衍生出多作业（Multi-Jobs）执行与编排的优化需求。
    - 更多样的训练方式
        - 强化学习（Reinforcement Learning）为代表的算法有比传统训练方式更为复杂的过程。其衍生出训练，推理，数据处理混合部署与协同优化的系统设计需求。
- PyTorch和 TensorFlow等框架应对自动化机器学习，强化学习等多样执行方式，以及细分的应用场景显得不够灵活，需要用户手工做特定的一些优化，没有好的工具和系统的支撑，这些问题一定程度上会拖慢和阻碍算法工程师研发效率，影响算法本身的发展。
- 开源社区中不断涌现针对特定应用领域而设计的框架和工具
    - Hugging Face: 面向语言预训练模型构建的Model Zoo和库框架
    - FairSeq: 面向自然语言处理中场景的序列到序列模型
    - MMDetection: 针对物体检测
    - NNI: 针对自动化机器学习
- 计算框架的进步
    - 第一代框架
        - Theano，Caffe，DisBelief
        - 为数值计算或特定机器学习问题或算法而设计
    - 第二代框架
        - 声明式编程（Declarative Programming）: TensorFlow
        - 命令式编程（Imperative Programming）: PyTorch
        - 存在问题
            - 控制流，数据预处理等其他语言层的逻辑与深度学习模型计算图的割裂造成不便于统一编译与优化
            - 除深度学习模型之外的库不方便卸载计算和利用 GPU 等专有硬件进而造成低效数据流水线
            - 没有侧重面向方面设计造成作业调试诊断困难，运维负担较大
            - Python 语言本身特点是简单，但是并发支持效率不高，不利于静态优化与错误检测等，对大规模工程化实践不友好
- 深度学习系统的设计目标
    - 提供更加高效的编程语言、框架和工具
    - 提供全面多样的深度学习任务需求的系统支持
    - 探索并解决新挑战下的系统设计、实现和演化的问题
    - 提供在更大规模的企业级环境的部署需求
- 深度学习系统的大致组成
    - 开发体验层
    - 框架层
    - 运行时
    - 资源管理与硬件体系结构
- 深度学习框架一般会提供以下功能
    - 以 Python API 供开发者编写复杂的模型计算图（Computation Graph）结构，调用基本算子实现（例如，卷积的 cuDNN 实现），大幅降低开发代码量
    - 自动化内存管理，不暴露指针和内存管理给用户
    - 自动微分（Automatic Differentiation）的功能，并能自动构建反向传播计算图，与前向传播图拼接成统一计算图
    - 调用或生成运行期优化代码（静态优化）
    - 调度算子在指定设备的执行，并在运行期应用并行算子，提升设备利用率等优化（动态优化）
- TensorFlow训练全流程
    - 前端程序转换为数据流图
        - 使用 Python 编写的深度学习模型，通过预先定义的接口，翻译为中间表达（Intermediate Representation），并且构建算子直接的依赖关系，形成前向数据流图（Data-Flow Graph）
    - 反向求导
        - 分析形成前向数据流图，通过算子之前定义的反向传播函数，构建反向传播数据流图，并和前向传播数据流图一起形成整体的数据流图
    - 产生运行期代码
        - 根据运行时部署所在的设备（CPU，GPU 等），将算子中间表达替换为算子针对特定设备的运行期的代码，例如CPU 的 C++ 算子实现或者针对 NVIDIA GPU 的 CUDA 算子实现
    - 调度并运行代码
        - 框架会将算子及其运行期的代码实现抽象为“任务”，依次根据“任务”依赖关系，调度到计算设备上进行执行
- 影响深度学习系统设计的理论，原则与假设
    - 抽象-层次化表示
        - 深度学习系统也遵循层次化的设计。系统会在各个层次抽象不同的表示，在高层方便用户表达算子，在底层则被转换为指令被芯片执行。这样搭积木的方式让整个工具链快速协同发展且能复用，大为加速了开发效率与自动化。
    - 摩尔定律（Moore's Law）与算力发展趋势
        - 摩尔定律已死
        - 芯片和系统性能还会受到其他约束（Constraint）所限制
            - 功耗墙约束
            - 暗硅 (Dark Silicon）与异构硬件（Heterogeneous Hardware） 
            - 内存墙约束
                - “内存墙”是芯片与芯片外的内存之间越来越大的速度差距
    - 局部性原则（Priciple of Locality）与内存层次结构（Memory Hierarchy）
        - 局部性原理是处理器在短时间内重复访问同一组内存位置的趋势。局部性有两种基本类型——时间局部性和空间局部性。时间局部性是指在相对较短的时间内重复使用特定数据和资源。空间局部性（也称为数据局部性）是指在相对较近的存储位置内使用数据元素。
    - 线性代数（Linear Algebra）计算与模型缺陷容忍（Defect Tolerance）特性
        - 线性代数：大部分的深度学习算子可以抽象为线性代数运算，如通用矩阵乘（GEMM）
    - 并行（Parallel）加速与阿姆达尔定律（Amdahl's Law）优化上限
    - 冗余（Redundancy）与可靠性（Dependability）

## 第2章-神经网络基础

## 第3章-深度学习框架基础
### 基于数据流图的深度学习框架
- 深度学习框架设计目标
    - 可编程性
    - 性能
    - 可扩展性
- 编程范式
    - 声明式
        - 前端语言中的表达式不直接执行，而是首先构建起一个完整前向计算过程表示，这个计算过程的表示经过序列化发送给后端系统，后端对计算过程表示优化后再执行，又被称作先定义后执行（Define-and-Run）或是静态图
        - 对数据和控制流的静态性限制更强，由于能够在执行之前得到全程序描述，从而有机会进行运行前编译（Ahead-Of-Time）优化
    - 命令式
        - 后端高性能可复用模块以跨语言绑定（Language Binding）方式与前端深度集成，前端语言直接驱动后端算子执行，用户表达式会立即被求值，又被称作边执行边定义(Define-by-Run)或者动态图
        - 方便调试，灵活性高，但由于在执行前缺少对算法的统一描述，也失去了编译期优化（例如，对数据流图进行全局优化等）
    - 多阶段（Multi-Stage）编程和即时编译（Just-In-Time, JIT)技术能够实现两种编程模式的混合。随着TensorFlow Eager和PyTorch JIT的加入，主流深度学习框架都选择了通过支持混合式编程以兼顾两者的优点
- 数据流图用统一的方式描述出了复杂神经网络训练的全过程，使得在运行程序之前后端系统有机会对整个计算过程的数据依赖关系进行分析，通过数据流图化简、内存优化、预先计算算子间的静态调度策略等方式，改善运行时的性能。
- 基于数据流图描述，深度学习框架在设计上切分出了三个解耦的优化层：数据流图优化，运行时调度策略，以及算子优化。当遇到新的神经网络模型结构或是训练算法时，通过以下三步进行扩展：
    （1）添加新的算子；
    （2）对算子的内核函数在不同设备，不同超参数下进行计算优化；
    （3）注册算子和内核函数，由运行时系统在在运行时派发到所需的实现上。
- 在基于数据流图的深度学习框架设计之初，希望通过对三个优化层之间的解耦来加速深度学习软件栈的迭代，然而，随着神经网络模型计算规模的增大，出现了越来越多的定制化算子，多设备支持需求增加，这三个抽象层之间的抽象边界也在被频繁地打破。在后续的章节我们会进一步讨论。

### 神经网络计算中的控制流
- 随着神经网络算法研究的发展，一些新颖的神经网络结构很难自然地表示为纯数据流图。为了能够支持如自定义循环神经网络这类计算过程中天生就含有控制流结构的神经网络计算，主流深度学习框架不约而同的引入了对动态控制流这一语言结构（Language Construct）的支持。
- 主流框架采用了两类设计思路：
    - 静态图：向数据流图中添加控制流原语
    - 动态图：复用宿主语言控制流语句
- 动态图转换为静态图
    - 为基于追踪（Tracing）
    - 基于源代码解析（Parsing）

## 第4章-矩阵运算与计算机体系结构
### 深度学习的计算模式
- 全连接层
    - 一层全连接层的计算就变成一个矩阵乘以向量的运算，输入层还可以是多个样本的批量输入，这时候一层的计算就可以转化成一个矩阵乘以矩阵的运算。
    - 矩阵乘法是一个经典的计算，无论是在CPU还是GPU中等有非常成熟的软件库，如MKL，CUBLAS等，因此其可以直接高效的被各种硬件支持。
- 卷积层
    - img2col: 通过对输入矩阵的重组而等价变化成一个矩阵相乘的形式
    - 这样的矩阵实现在实际中并不一定是最高效的，因为重组的输入矩阵的元素个数比原始矩阵变多了，也就意味着计算过程中要读取更多的数据，同时重组的过程也会引入一次内存复制的开销。为了优化后者，一种隐式矩阵乘法的实现就是在计算过程中在高级存储层中重组矩阵，从而减少对低级内存的访问量。
- 循环网络层
    - 循环神经网络层中的主要计算部分也是矩阵乘法，例如一个GRU单元里的就有6个矩阵乘算子，其它均为一些轻量的point-wise算子
- 注意力层
    - 核心的算子也是矩阵乘算子

### 计算机体系结构与矩阵运算
- CPU体系结构
    - CPU体系结构主要支持的计算是如何高效地执行通用程序，为了能够灵活地支持不用类型的计算任务，一个CPU核上需要引入较为复杂的控制单元来优化针对动态程序的调度，由于一般的计算程序的计算密度低，因此，CPU核上只有较少量的算术逻辑单元（ALU）来处理数值类型的计算
    - 在CPU核上执行一条计算执行需要经过一个完整的执行流水线，包括读取指令、译码、在ALU上计算结果、和写回
        - 在整个流水线上，大量的时间和功耗都花在了ALU之外的非计算逻辑上
    - 由于需要计算的数据一般存储在主存中，其访存延时往往和计算延时有数量级的差距，直接从主存中访问数据并计算会导致计算单元利用率极低，因此，为了加快访存速度，一般会在主存和寄存器之间加上多层缓存层(L1/L2/L3)。
    - 为了提升CPU的性能，新的CPU主要从以下几个方面提升性能：
        - 在单核上增加指令并发执行能力：通过乱序执行互不依赖的指令，重叠不同指令的流水线，从而增加指令发射吞吐；
        - 增加多核并发处理能力：通过多核并发执行增加并行处理能力，这需要依赖操作系统将应用程序调度到多核上，或者依赖用户的程序中显示使用多线程进行计算；
        - 在单核上增加向量化处理能力：允许CPU在向量数据上执行相同的指令，也就是针对一条指令的取指和译码可以对多个数据同时执行
- CPU实现高效计算矩阵乘
    - 通过更好的利用缓存来增加访存效率
    - 通过使用向量化指令来增加计算吞吐

### GPU体系结构与矩阵运算
- GPU体系结构
    - 大量的运算单元ALU: 通常GPU的一个处理器（也叫做流式多处理器，Streaming Multiprocessor）包括数十甚至上百个简单的计算核，整个GPU可以达到上千个核
    - 与CPU相比，每个核的结构简单了很多，通常不支持一些CPU中使用的较为复杂的调度机制
    - 在执行指令的时候，为了充分利用每一次读取指令带来的开销，GPU会以一组线程为单位同时执行相同的指令，即SIMT（单指令多线程）的方式。在CUDA GPU上，一组线程称为warp，一个warp有32个线程，每个线程执行相同的指令但访问不同的数据。
- GPU编程模型
    - CUDA,ROCM
    - CUDA中首先将一组线程（通常不超1024个）组成一个线程块（block），每个线程块中的线程又可以分成多个warp被调度到GPU核上执行，一个线程块可以在一个SM上运行。多个线程块又可以组成一个网格（grid）。
    - 线程块和网格分别通过一个3维整数类型描述其大小(blockDim和gridDim)，每个线程都可以通过threadIdx和blockIdx来确定其属于哪个线程块以及哪个线程。
- GPU实现一个简单的计算

### ASIC
- 低精度计算，如16bit、8bit
- 简化指令集
- 流水线计算模型来减少数据读取，如脉动阵列
    - 将多个ALU运算单元串起来，从而避免每次计算都读取寄存器
- 使用比向量比指令计算密度更高的并行方法

## 第5章-深度学习框架的编译与优化
- 深度学习的编译与优化就是将当前的深度学习计算任务通过一层或多层中间表达进行翻译和优化，最终转化成目标硬件上的可执行代码的过程

### 深度神经网络编译器
- 编译器（compiler）在计算机语言编译中往往指一种计算机程序，它会将某种编程语言写成的源代码（原始语言）转换成另一种编程语言（目标语言），在转换的过程中进行的程序优化就是编译优化过程。
- 深度神经网络编译器的提出主要是解决多种设备适配性和性能优化的问题。
- 深度神经网络编译就是将当前的深度学习计算任务通过一层或多层中间表达进行翻译和优化，最终转化成目标硬件上的可执行代码的过程。

- 前端
    - 深度神经网络编译器的前端一般共享深度学习框架的前端表达，如TensorFlow和PyTorch，即一般为基本Python的DSL（Domain Specific Language）。
- 后端
    - 指最终变化后的代码要执行的设备或神经网络加速器: CPU、GPU、FPGA、TPU等。
    - 不同的类型的计算设备往往采用完全不同的芯片架构，从而对应的编程模型和优化也完全不同。
- 中间表达（Intermediate Representation, IR）
    - 目前在神经网络编译器中较为常用的中间表达主要包括计算图（DAG）和算子表达式等。
        - 计算图的节点是算子，边表示张量，所有的节点和边构成一张有向无坏图，节点之间的依赖关系表示每个算子的执行顺序。
        - 在将算子继续向下转换到下层并生成设备代码时，算子表达式（Tensor Expression）作为另一类中间表达被广泛使用在不同的神经网络编译器中，如TVM、Ansor、Tensor Comprehension等。算子表达式的主要作用是描述算子的计算逻辑，从而可以被下层编译器进一步翻译和生成目标设备的可执行代码。
- 优化过程
    - 最核心部分
    - 定义在每一种中间表达上的函数，其输入是某一种中间表达，经过一系统优化和变化过程，输出一个新的被优化过后的中间表达
    - 可分为设备相关和设备无关的优化
    - 计算图优化、内存优化、内核优化和调度优化

### 计算图优化
- 算术表达式化简
    - 在计算图中的一些子图所对应的算术表达式，在数学上有等价的化简方法来简化表达式，这反应在计算图上就是将子图转化成一个更简单的子图（如更少的节点），从而降低计算量。
- 公共子表达式消除
    - 也是经典编译优化中常用的优化。其目的是通过找到程序中等价的计算表达式，然后通过复用结果的方式消除其它冗余表达式的计算。
- 常数传播
    - 也是经典编译优化中的常用优化，其主要方法是通过在编译期计算出也是常数表达式的值，用计算出的值来替换原来的表达式，从而节省运行时的开销。
- 矩阵乘自动融合
    - 如果把些矩阵乘算子融合成一个大的矩阵乘算子，可以更好的利用到GPU的算力，从而加速模型计算。
- 算子融合
    - 针对大量的小算子的融合都可以提高GPU的利用率，减少内核启动开销、减少访存开销等好处。
- 子图替换和随机子图替换
    - 编译器在计算图中识别出一个子图并替换成一个等价的新的算子或子图的过程就是子图替换优化

### 内存优化
- 一个深度学习计算任务中的内存占用主要包括输入数据、中间计算结果和模型参数，在训练场景中，由于反向求导计算需要使用到前向输出的中间结果，因此，前面计算出的算子需要一直保留到对应的反向计算结束后才能释放，对整个计算任务的内存占用挑战比较大。
- 计算图可以精确的描述出所有张量之前的依赖关系以及每个张量的生命周期，因此，根据计算图对张量进行合理的分配，可以尽可能的优化计算内存的占用。

- 基于拓扑序的最小内存分配
- 张量换入换出
- 张量重计算

### 内核优化
- 算子编译的核心思想是首先为通用算子找到一种能够描述算子与硬件无关的计算逻辑的表示，然后由编译器根据这种逻辑描述再结合具体的硬件生成相应的内核代码。

- 算子表达式
- 算子表示与调度逻辑的分离
- 自动调度搜索与代码生成

### 算子调度优化
- 任意算子的融合
- 编译时全局算子调度

## 第6章-分布式训练算法与系统
### 分布式深度学习计算简介
- 串行计算到并行计算的演进
- 并行计算加速定律
    - 阿姆达尔定律：存在加速的极限，为非可并行计算的占比之倒数
    - Gustafson定律： 允许计算问题的规模随着处理能力的增加而相应地增长，从而避免了加速比提升受限的问题
- 深度学习的并行化训练
    - 算子内并行：保持已有的算子的组织方式，探索将单个深度学习算子有效地映射到并行硬件设备上的执行
    - 算子间并行：注重发掘多个算子在多个设备上并行执行的策略，甚至解耦已有的单个算子为多个等效算子的组合，进一步发掘并行性
        - 数据并行：多个样本并行执行
        - 模型并行：多个算子并行执行
        - 组合并行：多种并行方案组合叠加

### 分布式训练算法分类
- 数据并行
    - 基于参数服务器（Parameter-Server）的实现
    - 基于All-Reduce的实现：目前的主流
    - 缺陷在于每个设备需要保留一份完整的模型副本，在模型参数量急剧增长的深度学习领域，能够轻松地超过单设备的存储容量，甚至一个算子也有可能超过单设备有限的存储，造成无法执行的境况
- 模型并行
    - 计算图中不同的算子(Operator) 被划分至不同设备上执行。 跨设备通过传递激活（Activation）的方式建立连接，协作执行完整的模型训练处理。每个设备分别利用反向传播计算中获得的梯度更新模型的本地部分。
    - 张量并行
        - 通过拆分算子，并把拆分出的多个算子分配到不同设备上并行执行
- 流水并行
    - 另一类特殊的模型并行
    - 依照模型的运算符的操作将模型的上下游算子分配为不同的流水阶段（Pipeline Stage），每个设备负责其中的一个阶段模型的存储和计算
    - GPipe
    - PipeDream

### 深度学习并行训练同步方式
- 同步并行
    - 在通信协调的过程中，所有的工作节点都必须等全部工作节点完成了本次通信之后才能继续下一轮本地计算
    - 其优点是本地计算和通信同步严格顺序化，能够容易地保证并行的执行逻辑于串行相同。但完成本地计算更早的工作节点需要等待其它工作节点处理，造成了计算硬件的浪费。
- 异步并行
    - 时间轴上并没有统一的时刻用于通信或者本地计算，而是工作节点各自分别随时处理自己收到的消息，并且随时发出所需的消息，以此完成节点间的协调。这样做的好处是没有全局同步障带来的相互等待开销。
- 半同步并行

### 分布式训练系统简介
- 模型的分布式训练依靠相应的分布式训练系统协助完成。这样的系统通常分为：
    - 分布式用户接口：表述采用何种模型的分布化策略
    - 单节点训练执行模块：产生本地执行的逻辑
    - 通信协调：实现多节点之间的通信协调
- TensorFlow 中的分布式支持
- PyTorch 中的分布式支持
    - 与TensorFlow相对的，PyTorch 的用户接口更倾向于暴露底层的通信原语用于搭建更为灵活的并行方式。
- 通用的数据并行系统Horovod
    - 针对多个训练框架提供分布训练功能

### 分布式训练的通信协调
- 机器内通信：共享内存、GPUDirect P2P over PCIe、GPUDirect P2P over NVLink
- 机器间通信：TCP/IP网络、 RDMA网络和GPUDirect RDMA网络
- 通信协调的硬件
    - NVLink (300GB/s) vs. PCIe 4.0 (32GB/s)。二者的链路带宽差距高达约10倍
    - 互联拓扑
    - 通信协议
        - GPUDirect P2P：GPU可以直接访问另一GPU的显存，无需CPU介入或系统内存中转，从而实现“零拷贝（zero-copy）
        - GPUDirect RDMA：GPU中的数据通过网络直接发送，无需系统内存中转，也实现了“零拷贝（zero-copy）
- 通信协调的软件
    - NVIDIA提出了针对其GPU等硬件产品的通信库 NCCL: NVIDIA Collective Communication Library
        - NCCL提供类似MPI的通信接口，包含集合式通信（collective communication）all-gather、 all-reduce、 broadcast、 reduce、reduce-scatter 以及点对点(point-to-point)通信send 和receive
    - 其它的厂商也发布了针对自身产品的高效通信库，例如AMD的RCCL以及intel的OneCCL

## 第7章-异构计算集群调度与资源管理系统
- 异构计算集群
    -  GPU（Graphics Processing Unit）加速器、InfiniBand 网卡
    - 集群的资源以多租户的形式被组织内的算法工程师使用
    - 集群管理系统（也称作平台）支撑模型的训练，提供作业，数据与模型的管理，并提供资源隔离
- 异构计算集群调度与资源管理系统在人工智能系统中类似传统 操作系统(Operating System) 作用，它对下抽象异构资源（例如，GPU，CPU等），对上层的深度学习作业进行调度和资源分配，在启动作业后也要提供相应的运行时进行资源隔离和环境隔离和作业进程的生命周期管理。

### 异构计算集群管理系统简介
- 异构计算集群管理系统是管理计算机集群内的多节点硬件（GPU，CPU，内存，磁盘等）、软件资源（框架，作业，镜像等）并为计算机程序（通常为深度学习训练作业程序）提供通用作业开发服务（提交，调试，监控，克隆等）的系统软件

- 多租环境运行的训练作业
    - 多租户（Multi-Tenancy）技术是一种软件架构技术，它实现如何于多用户多作业的环境下，共用使用系统或程序组件，并且仍可确保各用户间资源和数据保持隔离性。
    - 设计需求
        - 多作业（Job），多用户
        - 作业环境需求多样
        - 作业资源需求多样
        - 服务器软件环境单一
        - 服务器空闲资源多样
- 作业生命周期
    - 作业提交与排队
    - 作业资源分配与调度
    - 作业执行完成与释放
- 集群管理系统架构
    - 主要组件
        - 集群调度与资源管理模块
        - 镜像中心
        - 存储模块
        - 作业生命周期管理器
        - 集群监控与报警
        - 集成开发环境
        - 测试集群
    - 部署模式
        - 本地（On-Premises）
        - 公有云
        - 混合云：敏感数据放在本地数据中心，非敏感数据或弹性资源需求上公有云
        - 多云

### 作业，镜像与容器
- 深度学习作业
- 环境依赖：镜像（Image）
- 运行时资源隔离：容器
- 从操作系统视角看 GPU 技术栈
- 人工智能作业开发体验（Development Experience）

### 调度
- 调度问题优化目标
    - 调度（Scheduling）是分配资源以执行任务的动作。在深度学习平台中，资源可以是处理器、GPU、内存等，任务是用户提交的作业。 调度活动（Scheduling Activity）由称为调度器的进程执行。
- 单作业调度-群调度
    - 一种用于并行系统的调度算法，用于调度相关线程或进程,在不同处理器上同时启动并运行。
- 作业间调度-主导资源公平 DRF（Dominant Resource Fairness）调度
    - 在多资源环境中，资源分配应该由作业（用户或队列）的主导份额决定，这是作业已分配的任何资源（内存或 CPU）的最大份额。
- 组间作业调度-容量调度（Capacity Scheduling）
- 虚拟集群（Virtual Cluster）机制
- 抢占式调度（Preemptive Scheduling）
- 深度学习调度算法实验与模拟研究
    - 一般调度算法的研究基于历史作业日志，通过模拟器验证

### 面向深度学习的集群管理系统

### 存储
- 沿用大数据平台存储路线
    - 分布式文件系统
        - 对深度学习框架来说，原生对大数据存储系统接口支持并不充分，造成有些框架需要自己定义自定义数据读取器，相比大数据框架效率低，实现容易性能差
    - 云存储
    - 分布式内存文件系统：Alluxio
- 沿用高性能计算平台存储路线
    - 网络文件系统（Network File System）：简称 NFS
    - 利用高速网卡等异构硬件的 HPC 文件系统
- 面向深度学习的存储

### 开发与运维
- 平台功能模块与敏捷开发
    - 平台中涉及以下重要服务和功能模块
        - 内核：调度器，运行时，管理界面，权限管理，文件管理等。
        - 监控与报警：性能监控，异常监控，报警系统等
        - 工具链：IDE，作业提交与调试工具，API（例如：Restful）
        - 应用市场：模型市场，镜像市场
    - 不同的功能模块可以分配不同的工程师进行开发，通过 Scrum 敏捷开发，定期规划与执行冲刺（Sprint）计划，并通过每日的会议进行规划反思与推进
- 监控体系构建
    - 全局监控
    - 性能监控
    - 服务与硬件稳定性监控
        - 心跳机制（Heartbeat）：心跳是由硬件或软件生成的周期性信号，用户证明当前系统正常运行，同时可以通过心跳传递控制平面的信息，如果监测节点定期没有收到或者监测到心跳认为被监测节点或系统出现了故障，需要修复或者使用冗余机器替代。复杂的场景下还可以设计心跳机制的协议
    - 报警
        - 报警质量：如果警报经常是假的而不是真的，那么单位就会出现一种文化，即工作人员可能会延迟对警报的响应，尤其是当工作人员从事其他患者护理活动时，可能会错过更重要的关键警报。
- 测试
    - 单元测试
        - Python：PyUnit
    - 集成测试
        - 每次有一定的功能更新在开发分支需要合并到主分支之前，可以触发集成测试，将各个模块都进行打包，并在测试环境整体部署，运行一定的测试作业或者检测脚本，验证不会产生构建版本冲突，服务调用冲突等由于模块互操作产生的问题。
    - 压力测试
        - 对于平台性能，可以通过经典深度学习基准测试（例如，MLPerf等）中的负载进行性能与压力测试。
    - 回归测试
        - 一旦完成平台新功能开发或者在线热修复，需要进行回归测试保证符合之前系统假设。 回归测试是指修改了已有代码后，重新进行测试以确认修改没有引入新的错误或导致其他代码产生错误。
    - 缺陷注入，模糊测试与混沌工程
- 平台 DevOps
    - 平台可以通过 CI/CD 机制，持续集成（Continuous Integration 简称 CI），持续交付（Continuous Delivery 简称 CD）新的功能或者上线热修复
    - 平台本身的各个服务组件也可以打包为 Docker 镜像，通过 Kubernetes 等工具部署和管理相应服务
- 平台运维
    - 直接责任人（DRI）与候命（On-Call）机制
    - 事件管理
        - 人工处理
        - 基于规则处理
        - 基于机器学习处理
    - 智能运维（AIOps）
        - 目前对于过于复杂问题，非结构化运维数据，以及希望数据驱动解决的运维问题，也可以考虑智能运维（AIOps）技术进行智能运维，主动介入与预测，互补于常规运维方法。

## 第8章-深度学习推理系统
- 推理系统（Inference System）是用于部署人工智能模型，执行推理预测任务的人工智能系统
- 当推理系统将完成训练的模型进行部署和服务时，需要考虑设计和提供模型压缩，负载均衡，请求调度，加速优化，多副本和生命周期管理等支持。
- 相比深度学习框架等为训练而设计的系统，推理系统不仅关注低延迟，高吞吐，可靠性等设计目标，同时受到资源，服务等级协议（Service-Level Agreement，SLA），功耗等约束。

### 推理系统简介
- 对比推理与训练过程
    - 相比训练阶段，推理阶段则只需要执行前向传播（Forward Propagation）一个过程，将输入样本通过深度学习模型计算输出标签（Label）
    - 训练作业常常在异构集群管理系统中进行执行，通常执行，几个小时，天，或周，是类似传统的批处理作业（Batch Job）。而推理作业需要 7×24 小时运行，类似传统的在线服务（Online Service）
    - 深度学习模型推理相比训练的新特点与挑战主要有以下几点：
        - 模型被部署为长期运行的服务
        - 推理有更苛刻的资源约束
        - 推理不需要反向传播梯度下降
        - 部署的设备型号更加多样
- 推理系统的优化目标与约束
    - 目标
        - 低延迟
        - 高吞吐量
        - 高效率
        - 灵活性
        - 可靠性
        - 可扩展性
    - 约束
        - 服务等级协议（SLA）对延迟的约束
        - 资源约束
        - 准确度约束
    - 推理系统完成以下处理并涉及以下系统设计问题
        - 请求与响应处理
        - 请求调度
        - 后端框架执行
        - 模型版本管理
        - 健康汇报
        - 推理芯片与代码编译
        - 推理系统和训练系统间可以通过模型库与上线的协议建立起联系，一般训练系统有一套完整的 DevOps 流水线，也被称作 MLOps

### 模型推理的离线优化
- 通过程序理解推理优化动机
- 推理（Inference）延迟（Latency）
    - 延迟优化
        - 模型优化，降低访存开销
        - 降低一定的准确度，进而降低计算量，最终降低延迟
        - 自适应批尺寸（Batch Size）
        - 缓存结果
- 层（Layer）间与张量（Tensor）融合
    - 相对于内核启动开销和每个层的张量数据读写成本，内核（Kernel）计算通常非常快
- 目标后端自动调优
    - 深度学习模型中的层的计算逻辑可以转为矩阵运算，而矩阵计算又可以通过循环进行实现。
    - 对于循环的实现，由于在不同的推理 CPU 和硬件设备中，有不同的缓存和内存大小以及访存带宽，进而如何针对不同的设备进行循环的并行化和考虑数据的局部性降低访存开销，可以抽象为一个搜索空间巨大的优化问题。
    - TVM，NNFusion，Halide，Ansor
- 模型压缩
    - 在保证模型预测效果满足一定要求的前提下，尽可能地降低模型权重的大小，进而降低模型的推理计算量，内存开销和模型文件的空间占用，最终降低模型推理延迟
    - 常用的模型压缩技术有如下几种：
        - 参数裁剪（Parameter Pruning）和共享（Sharing）
            - 剪枝（Pruning）
            - 量化（Quantization）
            - 编码（Encoding）
        - 低秩分解（Low-Rank Factorization）
        - 知识精炼（Knowledge Distillation）
- 低精度推理

### 部署
- 可靠性（Reliability）和可扩展性（Scalability）
    - 通过底层的部署平台（例如，Kubernetes）的支持，用户可以通过配置方便地描述和自动部署多个推理服务的副本，并通过部署前端负载均衡服务达到负载均衡，进而达到高扩展性提升了吞吐量，同时更多的副本也使得推理服务有了更高的可靠性。
- 部署灵活性
    - 深度学习模型开放协议：通过 ONNX 等模型开放协议和工具，将不同框架的模型进行通过标准协议转换，优化和部署
    - 接口抽象：将模型文件封装并提供特定语言的调用接口。
    - 远程过程调用（Remote Procedure Call）：可以将不同的模型或数据处理模块封装为微服务，通过远程过程调用（RPC）进行推理流水线构建
    - 镜像和容器技术：通过镜像技术解决多版本与部署资源隔离问题
- 模型转换与开放协议
    - 模型中间表达标准（ONNX）：让框架，工具和运行时有一套通用的模型标准，使得优化和工具能够被复用。
    - 模型转换工具（MMdnn）：让模型可以打通不同框架已有工具链，实现更加方便的部署或迁移学习（Transfer Learning）。
- 移动端部署
- 推理系统简介
    - 服务端推理系统
        - GPU：TensorRT，Triton推理服务器
        - TensorFlow Serving（TFX）
        - TorchServe
        - ONNX Runtime
    - 边缘（Edge）端推理库
        - TensorFlow Lite
        - Triton 推理服务器

### 推理系统的运行期优化
- 推理系统的吞吐量
- 加速器模型并发执行
- 动态批尺寸
    - 通过提升批尺寸（Batch Size）可以提升吞吐量，对于较高请求数量和频率的场景，通过大的批次可以提升吞吐量。但是推理系统要注意，没有免费的午餐，随着吞吐量上升的还有延迟
- 多模型装箱（Bin Packing）
- 内存分配策略调优
- 深度学习模型内存分配算法实验与模拟研究

### 开发、训练与部署的全生命周期管理-MLOps
- MLOps 是一种用于人工智能（包含机器学习与深度学习）全生命周期的工程化方法，它借鉴 DevOps 思想将机器学习（例如，模型的训练与推理）开发（Dev 部分）与机器学习系统（深度学习框架，自动化机器学习系统）统一起来操作与维护（Ops 部分）
- MLOps 的过程希望标准化和自动化机器学习全生命周期的关键步骤。
- MLOps 提供了一套标准化的流程和工具，用于构建、部署、快速可靠地运行机器学习全流程和机器学习系统。
- MLOps工具链
    - 模型动物园（Model Zoo） 
    - 模型和工作流可视化
    - 模型转换
    - 程序，模型和搜索空间合法性（Validity）验证
    - 模型调试器
    - 模型与数据监控
    - 模型版本管理
- 线上发布与回滚策略
- MLOps 持续集成，持续交付（CI/CD）
- MLOps 工具与服务

### 推理专有芯片
- 推理芯片架构对比
    - CPU
    - GPU
    - ASIC
    - FPGA
- 神经网络推理芯片的动机和由来
- 数据中心推理芯片
- 边缘推理芯片
- 芯片模拟器