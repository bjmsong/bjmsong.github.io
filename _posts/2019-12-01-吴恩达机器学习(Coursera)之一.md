---
layout:     post
title:      吴恩达机器学习(Coursera)之一
subtitle:   
date:       2019-12-01
author:     bjmsong
header-img: img/machineLearning/machineLearning.png
catalog: true
tags:
    - 机器学习
---
> 这个版本的课程相较CS229而言，较为简单，一共分为18章。本文将介绍1~4章的内容，主要回顾了线性代数的基础知识，介绍了回归算法。

一、 引言(**Introduction**) 

1.2 机器学习是什么？ 

• Arthur Samuel (1959). Machine Learning: Field of study that gives computers the ability to learn without being explicitly programmed. 

• Tom Mitchell (1998) Well-posed Learning Problem: A computer program is said to *learn* from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E. 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/definition of ml.jpg) 
</li> 
</ul> 

1.3 监督学习 

1.4 无监督学习 



二、单变量线性回归(**Linear Regression with One Variable**) 

2.1 模型表示 

2.2 代价函数 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/costFunction2.jpg) 
</li> 
</ul> 

2.3 代价函数的直观理解I 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/costFunction.jpg) 
</li> 
</ul> 

2.5 梯度下降 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/gradient_descent.jpg) 
</li> 
</ul> 

2.6 梯度下降的直观理解 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/learning_rate.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/local_minimum.jpg) 
</li> 
</ul> 

2.7 梯度下降的线性回归 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/gd_linear.jpg) 
</li> 
</ul> 



三、线性代数回顾(**Linear Algebra Review**) 

3.1 矩阵和向量 

3.2 加法和标量乘法 

3.3 矩阵向量乘法 

3.4 矩阵乘法 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/matrix_mu.jpg) 
</li> 
</ul> 

3.5 矩阵乘法的性质 

$$AB \neq BA$$

$$A\times(B\times C) = (A \times B) \times C$$

$$A\cdot I = I \cdot A = A$$

其中，I是单位矩阵

3.6 逆、转置

$$A\cdot A^{-1} = A^{-1} \cdot A = I$$

奇异（singular）矩阵/退化（degenerate）矩阵没有逆

- 转置

  $$B=A^T,B_{ij}=A_{ji}$$



四、多变量线性回归(**Linear Regression with Multiple Variables**) 

4.1 多维特征 

$$h_{\theta}(x) = \theta^Tx$$

4.2 多变量梯度下降 

4.3 梯度下降法实践1-特征缩放 

<ul> 
<li markdown="1"> 
get every feature into approximately a [-1,1] range 
![]({{site.baseurl}}/img/CourseraML/feature_scaling.jpg) 
</li> 
</ul> 

- mean normalization

  $$\frac{x-\mu}{\sigma}$$

4.4 梯度下降法实践2-学习率 

4.5 特征和多项式回归 

4.6 Normal equation 

methods to solve for $$\theta$$ analytically（令损失函数的偏导等于0）

$$\theta = (X^TX)^{-1}X^Ty$$

计算量大，效率低

4.7 正规方程及不可逆性（选修） 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/不可逆.jpg) 
</li> 
</ul> 




## 参考资料
- https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes




