---
layout:     post
title:      深度模型压缩与加速
subtitle:   
date:       2022-09-02
author:     bjmsong
header-img: 
catalog: true
tags:
    - 模型优化与部署
---
## Parameter Quantization 量化
### 高通量化白皮书
https://zhuanlan.zhihu.com/p/462971462
https://blog.csdn.net/weixin_44935658/article/details/125335053
https://www.bilibili.com/video/BV1vf4y1K7km/?spm_id_from=333.880.my_history.page.click&vd_source=7798c62f92ce545f56fd00d4daf55e26

### https://www.bilibili.com/video/BV1gS4y1Y7KR/?spm_id_from=333.999.0.0&vd_source=7798c62f92ce545f56fd00d4daf55e26
- zz123211
    - https://space.bilibili.com/289239037/channel/series
- 神经网络加速基础知识
    - 指令执行过程：取指令，译码，访存，执行
        - 访存是瓶颈
    - 现代处理器
        - CPU
            - 译码器、控制器、内存控制器、运算器需要支持太多指令集，设计复杂，运算核心面积小
        - GPU
            - 无须支持过多指令集，译码器和控制器可以设计得相对简单
            - 要求运算器共享指令译码与控制，因此只需一套指令译码与控制器件，即可同时操作多个运算器
            - 更多计算核心，但是ALU必须运行相同的指令
        - ASIC
            - 移除浮点运算器，添加更多整数运算器
            - 不支持图形相关指令
            - 更大的缓存，更大的显存
    - 异构计算
        - 现代计算机，为了运行应用程序，首先需要建立一个操作系统或类似的中间组件
        - 对计算设备的任何请求，都将通过OS->CPU->OS->GPU的路径传输
        - 物理层面，CPU与GPU通过PCIE总线互联
        - 软件层面，GPU驱动使得操作系统知道如何操作GPU进行计算
        - CPU与GPU之间是异步的，CPU可以在等待GPU完成运算的时候去做其它的事情
        - GPU有指令流水线，可以缓存接收到的任务并逐个执行
        - 由于硬件特性以及设备流水线的存在，神经网络的延迟与吞吐并不是完全反比的关系
            - 延迟：推理一个数据需要的时间
            - 吞吐：每秒钟推理多少个数据
            - 看实际应用是需要低延迟还是高吞吐？
- 使用torch.profiler来分析神经网络性能
    - tensorboard
        - Trace
    - 分析瓶颈在哪里，然后再优化，不要盲目做量化/压缩等优化
    - mmdeploy
        - https://mmdeploy.readthedocs.io/zh_CN/latest/
        - https://github.com/open-mmlab/mmdeploy
        - PPQ: OpenPPL 之多平台量化部署工具
        https://zhuanlan.zhihu.com/p/478898529
- 神经网络量化硬件实现
    - 量化算子（quantize function）
    - 量化子图与全精度子图（quantized subgraph）
    - 有些算子不能量化：需要查文档
    - 对称量化，非对称量化，整数量化
        - 使用对称量化还是非对称量化，取决于float的分布，像relu的output是非对称的 
        - power-of-2 quantization
            - 直接移位(二进制右移一位相当于除以2), 省去浮点运算单元
            https://blog.csdn.net/FlushHip/article/details/82495034
    - Per-Tensor量化、Per-channel量化
        - Per-Tensor量化：同一个tensor的数共享scale，offset
        - Per-channel量化：同一个tensor，同一个channel共享scale，offset
            - 降低量化误差
    - int8运算比float32运算快的原因
        - 不是指令执行的速度快
        - 而是
            - 访存少
            - 向量化指令：一次性处理大量数据
                - TensorCore
                - 因为int8只需要8位，因此可以一次性处理更多的数据
                    - 在线显式汇编代码
                    https://godbolt.org/
    - 量化算子实现
        - 乘法
            - int8乘以int8：结果会溢出
            - 中间过程需要反量化
        - 加法
        - 激活函数
        - 被动量化
        - 矩阵乘法
        - 非线性运算
- 神经网络图优化与量化
    - 计算图
        - 算子
            - 算子是神经网络的最小调度单位，但它不是原子的：一个复杂的算子可以被更细粒度的算子所表示
            - https://github.com/onnx/onnx/blob/main/docs/Operators.md
    - 算子融合加速 Graph Fusion
        - 优点
            - 减少kernel启动次数
            - 减少访存
        - 常见计算图优化（可以递归）
            - 激活函数融合
            - 移除batchnorm与dropout
            - 常量折叠
                - 常量计算提前算好
            - 矩阵乘融合
            - Conv-Add融合
    - 联合定点 Union-Quantize
- 神经网络算子调度与图模式匹配
    - 图模式匹配是量化算法、算子融合、算子调度的基础
- 神经网络部署
    - program runtime 运行时
        - 程序语言提供的一个代码库，你的程序将调用运行时中的功能来访问系统资源。最典型的运行时例如java虚拟机、C++ runtime等
        - Runtime is a piece of code that implements portions of a programming language's execution model.In doing this, it allows the program to interact with the computing resources it need to work. Runtimes are often integral parts of the programming language and don't need to be installed separately.
    - Neural Network Runtime
        - TensorRT、ONNX Runtime、Tengine、OpenVINO、TFlite，PyTorch、TNN、MNN。。。
        - 硬件、软件厂商都在做自己的runtime
            - 硬件厂商不喜欢搞自己的图表示，采用onnx
            - 软件厂商都喜欢搞自己的图表示：TFLite、Torchscript、PNNX
        - onnx希望做成行业标准（神经网络表示），但是没有做很好
        - 截止2022年，神经网络的表示问题仍然没有得到完善解决
            - 算法工程师总是喜欢在现有研究基础上进行排列组合
            - 神经网络领域的快速发展，技术的快速迭代使得原有表示不再适用
            - 制定标准意味着利益，各大厂商（巨头林立）蠢蠢欲动（根本原因）
    - 神经网络部署
        - tips
            - 确保网络可以被ONNX表示，避免出现复杂条件及循环逻辑
            - 学会自定义算子（包括自定义算子的推理实现），以备不时之需
            - 避免使用各种小trick，额外加入的算子可能会破坏图优化
            - 神经网络能跑多快是Runtime决定的，神经网络加速应当根据runtime进行
            - 用一下ONNX Simplifier：会把onnx进行简化
            - 写一个固定的batchsize
        - onnxruntime
- TensorRT部署
    - GPGPU：General-purpose computing on graphics processing units, 通用图形处理器
        - CUDA: 通用并行计算架构
    - torch2trt：加速pytorch模型，在python脚本中运行
    - TensorRT QAT
    - TensorRT PTQ
    - 提升算子计算效率
    - 网络结构与图融合
    - Tensor对齐
    - Profiling is all you need
    - 自定义算子
- 《社区开放麦#34 | 突破 AI 推理性能瓶颈，神经网络量化的方法与实践》 （张志）
    - https://www.bilibili.com/video/BV128411n7yN/?spm_id_from=333.880.my_history.page.click&vd_source=7798c62f92ce545f56fd00d4daf55e26 

### https://oldpan.me/archives/how-to-quan-1

### 李宏毅：模型压缩系列讲解
https://www.bilibili.com/video/BV1LE411Z76J?spm_id_from=333.337.search-card.all.click&vd_source=7798c62f92ce545f56fd00d4daf55e26
http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20
    - 第七课
- 伪量化
    - 利用低精度来保存每一个网络参数，同时保存拉伸比例scale和零值对应的浮点数zero_point
    - 存储时使用了低精度进行量化，但推理时会还原为正常高精度
    - 伪量化可以实现模型压缩，但对模型加速没有多大效果
- weight clustering
    - 进阶：霍夫曼编码
        - Represent frequent clusters by less bits, represent rare clusters by more bits
- 定点化
    - 与伪量化不同的是，定点化在推理时，不需要还原为浮点数
    - 《Deep learning with limited numerical precision》
- 二值化
《Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1》
https://blog.csdn.net/elaine_bao/article/details/50950969

## Song Han
https://www.youtube.com/watch?v=EKZbdh6xia8&ab_channel=SongHan
https://www.youtube.com/@songhan4243
https://zhuanlan.zhihu.com/p/417034031

### https://www.bilibili.com/video/BV1384y187tL/?spm_id_from=333.337.search-card.all.click&vd_source=7798c62f92ce545f56fd00d4daf55e26
- 低比特量化
    - 基础
        - 优点
            - 保持精度
            - 加速计算
            - 节省内存
            - 节省芯片面积
    - 原理
        - 桥接了定点与浮点，建立了一种有效的数据映射关系，使得以较小的精度损失代价获得了较好的收益
        - 对称量化、非对称量化
        - 量化：浮点->定点
        - 反量化：定点->浮点
        - 求解S（缩放因子）、Z（零点）
    - Quantization Aware Training(QAT) 感知量化
        - 模型中插入伪量化节点fake quant来模拟量化引入的误差。端侧推理的时候折叠fake quant节点中的属性到tensor中，在端侧推理的过程中直接使用tensor中带有的量化属性参数
        - 让模型感知量化运算对模型精度带来的影响，通过finetune训练降低量化误差
    - Post-Training Quantization(PTQ) 训练后量化
        - Post-Training Quantization Dynamic
            - 将模型中特定算子的权重从FP32映射到INT8/16
        - Post-Training Quantization Static：更常使用
            - 使用少量无标签较准数据，采用KL散度等方法计算量化比例因子
        - 需要插入观察op来收集每一层的激活分布以及权重分布
            - https://blog.csdn.net/m0_38043555/article/details/118278641
    - 量化部署


### 
- https://github.com/microsoft/AI-System/tree/main/Textbook/%E7%AC%AC11%E7%AB%A0-%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E5%8A%A0%E9%80%9F
- https://github.com/Ewenwan/MVision/tree/master/CNN/Deep_Compression
- https://zhuanlan.zhihu.com/p/355598250
- pytorch
    - https://pytorch.org/docs/stable/quantization.html?highlight=quantization#dynamic-quantization
    - https://pytorch.org/tutorials/recipes/quantization.html
    + https://pytorch.org/blog/quantization-in-practice/
    - https://www.bilibili.com/video/BV17P411L7rw/?spm_id_from=333.337.search-card.all.click&vd_source=7798c62f92ce545f56fd00d4daf55e26
+ https://www.tensorflow.org/model_optimization/guide/quantization/training?hl=zh-cn
+ https://www.tensorflow.org/lite/performance/post_training_quantization?hl=zh-cn
+ https://chromium.googlesource.com/external/github.com/tensorflow/tensorflow/+/r0.10/tensorflow/g3doc/how_tos/quantization/index.md
- https://pytorch.org/docs/stable/quantization.html
- Apex 混合精度
https://zhuanlan.zhihu.com/p/79887894
- mqbench：工业级模型量化框架
https://www.bilibili.com/video/BV1G44y1g7i9/
- 参考资料
    - 消费级GPU运行1760亿参数大模型
    https://mp.weixin.qq.com/s/gyLzQhW0mZxKXtCUau3ScQ
    https://github.com/timdettmers/bitsandbytes
    - https://blog.csdn.net/WZZ18191171661/article/details/103332338/
    - https://jilei-hou.github.io/2022/06/18/%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96/
    - 社区分享 | TensorFlow 模型优化：模型量化
    https://mp.weixin.qq.com/s/9QeVVESP3_rBZ6n_D96lwg
    - https://zhuanlan.zhihu.com/p/505570612
    - https://www.zhihu.com/question/421743958

## Architecture Design
- Low rank approximation
    - ALBERT
- Depthwise Separable Convolution
    - Depthwise Convolution
    - Pointwise Convolution: 1*1 卷积
    - MobileNet
        - https://www.bilibili.com/video/BV1yE411p7L7/?spm_id_from=333.337.search-card.all.click&vd_source=7798c62f92ce545f56fd00d4daf55e26
    - SuffleNet、SqueezeNet...
    https://blog.csdn.net/briblue/article/details/84995553
    https://zhuanlan.zhihu.com/p/49465950
- 全局平均池化代替全连接层

## Network Pruning 模型剪枝
- 步骤
    - Evaluate the importance
        - importance of weight: L1,L2
        - importance of neuron: the number of times it wasn't zero on a given dataset
    - prune redundant weights or neurons
    - Fine-tune
    - 继续以上步骤
- 将权重矩阵中不重要的参数设置为0，结合稀疏矩阵来进行存储和计算
- 为了保证performance，需要一小步一小步地进行迭代剪枝
- 一般是prune neuron：神经元直接拿掉
    - prune weight存在的问题
        - 连接直接拿掉，不容易实现
        - weight设为0，容易实现，方便加速，但是模型没有真正压缩，并没有得到加速
- https://pytorch.org/tutorials/intermediate/pruning_tutorial.html
- https://github.com/he-y/Awesome-Pruning
- 用NNI、TinyNeuralNetwork 等框架对模型进行剪枝，阅读其源码了解常用的剪枝方法

## Knowledge Distillation 蒸馏
- student模型学习teacher模型的输出
- 大模型/ensemble model -> 小模型
- 《Distilling the Knowledge in a Neural Network》
- DistilBERT

    
## Dynamic Computation
- adjust the computation power it need
- possible solution
    - Train multiple classifiers
    - Classifiers at the intermedia layer

## Pytorch

## TensorFlow 模型优化工具包
https://www.tensorflow.org/model_optimization?hl=zh-cn
https://mp.weixin.qq.com/mp/appmsgalbum?action=getalbum&album_id=1642498502104547335&__biz=MzU1OTMyNDcxMQ==#wechat_redirect
### gemmlowp
- https://github.com/google/gemmlowp

## 参考资料
- https://zhuanlan.zhihu.com/p/138059904
- https://github.com/microsoft/AI-System/tree/main/Textbook/%E7%AC%AC11%E7%AB%A0-%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E4%B8%8E%E5%8A%A0%E9%80%9F
《A Survey of Model Compression and Acceleration for Deep Neural Networks》
https://awesomeopensource.com/projects/model-compression
https://github.com/cedrickchee/awesome-ml-model-compression
https://github.com/j-marple-dev/model_compression
https://github.com/SpursLipu/YOLOv3v4-ModelCompression-MultidatasetTraining-Multibackbone
https://github.com/memoiry/Awesome-model-compression-and-acceleration
https://github.com/chester256/Model-Compression-Papers
- MIT公开课 TinyML and Efficient Deep Learning
https://www.bilibili.com/video/BV1uK411m7f6/?is_story_h5=false&p=1&share_from=ugc&share_medium=android&share_plat=android&share_session_id=11c52362-cb9b-4c45-a9ac-2f4a3091cc1a&share_source=WEIXIN&share_tag=s_i&timestamp=1669178663&unique_k=4FcyhYZ
- 阿里妈妈搜索广告CTR模型的“瘦身”之路
https://mp.weixin.qq.com/s/fOA_u3TYeSwAeI6C9QW8Yw
- 《Training Deep Nets with Sublinear Memory Cost》
    - 用低于线性复杂度就可以训练更深模型的内存优化算法，直接把 backward 所需要的内存量从 O(N) 降低到 O(sqrt(N))
    https://github.com/Lyken17/pytorch-memonger
    https://zhuanlan.zhihu.com/p/62270652
- 《Stripes: Bit-serial deep neural network computing》
- 《Minerva: Enabling low- power, highly-accurate deep neural network accelerators》
- https://www.bilibili.com/video/BV1364y1d7Ks?is_story_h5=false&p=1&share_from=ugc&share_medium=android&share_plat=android&share_session_id=59556101-a815-4452-bc93-b8e4ee3797b3&share_source=WEIXIN&share_tag=s_i&timestamp=1661949215&unique_k=iw6RbGx
- http://eyeriss.mit.edu/2019_neurips_tutorial.pdf?ich_args2=526-06113205060278_e0b61138dc2d908aa0766d50302e6d8a_10001002_9c896324d5cbf0d49239518939a83798_92e0a859ed6aa28355c3deea32562911
- https://zhuanlan.zhihu.com/p/421154873
- https://www.zhihu.com/question/329372124/answer/2675692342


