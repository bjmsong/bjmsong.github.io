---
layout:     post
title:      深度学习推理引擎
subtitle:   
date:       2022-12-09
author:     bjmsong
header-img: 
catalog: true
tags:
    - 模型优化与部署
---
## 概况
- 主要功能
    - 产生IR
    - 图优化
        - 端侧AI框架加速优化方法
            - 编译优化、缓存优化、多线程、稀疏存储和计算、NEON指令应用、算子优化等
        - 系统优化: 指在特定系统平台上，通过Runtime层面性能优化，以提升AI模型的计算效率
            - Op-level的算子优化：FFT Conv2d (7x7, 9x9), Winograd Conv2d (3x3, 5x5) 等；
            - Layer-level的快速算法：Sparse-block net [1] 等；
            - Graph-level的图优化：BN fold、Constant fold、Op fusion和计算图等价变换等；
            - 优化工具与库（手工库、自动编译）：TensorRT (Nvidia), MNN (Alibaba), TVM (Tensor Virtual Machine), Tensor Comprehension (Facebook) 和OpenVINO (Intel) 等；
- 产品
    - TVM
    - 英伟达：TensorRT
    - 谷歌：TensorFlow XLA
    - Facebook
        - Tensor Comprehensions
        - AITemplate
            - https://ai.facebook.com/blog/gpu-inference-engine-nvidia-amd-open-source/
            - https://github.com/facebookincubator/AITemplate
            - https://mp.weixin.qq.com/s/POMDj0P4-IHVR1y63CKBkQ
    - 阿里：MNN
    https://toutiao.io/posts/8250zb/preview
    https://mp.weixin.qq.com/s/5I1ISpx8lQqvCS8tGd6EJw
    https://mp.weixin.qq.com/s/vc1xQdJJjJT-xoYSPMGshA
    《MNN: A Universal and Efficient Inference Engine》
    - OpenVINO（Intel CPU）
    https://www.zhihu.com/column/c_1131328261854191616
        - nGraph
    - 小米：MACE
    - 腾讯：NCNN，TNN
    - TNN
    https://www.cnblogs.com/LXP-Never/p/14840535.html
    - 字节：LightSeq
    https://mp.weixin.qq.com/s/5VeersQKxcT9AtFXS_CXnA
    - https://zhuanlan.zhihu.com/p/463654311
    - openppl
    https://www.zhihu.com/question/346965029
    - HugeCTR
    https://cloud.tencent.com/developer/article/2011186
    https://assets-global.website-files.com/61398f0b3344b9d4ec0973b9/6318c6c61dc053b7a51597f0_Merlin%20HugeCTR%20GPU-accelerated%20Recommender%20System%20(1)%20-%20Sara%20Sprinkles%20US.pdf
    https://github.com/NVIDIA-Merlin/HugeCTR
- DSL -> Deep Learning IR -> LLVM IR -> Target
- 不同推理引擎侧重点不同，且采用不同的抽象层次和优化策略
- https://blog.csdn.net/lixuejun2008/article/details/103897626
- https://zhuanlan.zhihu.com/p/534708101
- https://codeantenna.com/a/yyqRoA11ED


## NCNN
### 谈谈ncnn的设计理念和软件工程
- https://www.bilibili.com/video/BV1Zo4y1R7En/?spm_id_from=333.999.0.0&vd_source=7798c62f92ce545f56fd00d4daf55e26
- zen
    - usability
        - 易用
        - 易学习：授人以鱼不如授人以渔
        - 例子很多、社群很好
    - portability
        - C++03：一些嵌入式平台不支持C++11
        - Vulkan API
            - 一套代码支持不同平台：桌面/移动端，CPU/GPU
            - 比较难写
    - maintainability
        - minimal prerequisite knowledge
            - 代码不会用到复杂特性，方便阅读
        - 所有的优化代码都是可选的，可以去掉 
    - compatibility
        - High-level API稳定：4年没有调整过
- 数据结构：Mat
    - 一开始用opencv的mat，后来是自己写的
        - opencv太大了
    - channel，height，width，没有batch_size
        - 推理场景基本都是batch_size=1
            - 推理场景latency比thoughput更重要
        - 如果batch_size>1，套个for循环即可
        - 优点
            - 算子操作时不用考虑batch_size
    - Alignment：内存对齐，加快数据访问速度
        - Armv7：差异比较明显
        - OpenMP：内存不对齐的话不同线程不能对齐
    - Type-less
        - 用来量化
        - Type convention
- 文件格式：.param,.bin
    - human editable：.param
        - 模型结构
    - plain storage：.bin
        - 可以方便地把几个模型的bin文件concatenate到一起
- Dynamic
    - 数据shape可以任意
    - 数据dimension可以任意 
    - 计算图可以动态
- is-a（继承）
    - x86, arm, vulkan 都是继承自 cpu naive
    - 通常kernel和operator是分开的，好处是不同opeartor可以复用kernel（例如gemm kernel），坏处是用户想要看某一个operator的实现，需要找很多地方，NCNN把一个opeartor的实现放到了一个地方，同时可以减少开销
- CI
    - Github CodeCov 单元测试结果可视化
        - https://blog.csdn.net/weiwei9363/article/details/124352067
        - 提高单元测试覆盖率，有助于检查到corner case
    - Swiftshader：GPU测试
    - QEMU

### PNNX
- https://github.com/Tencent/ncnn/tree/master/tools/pnnx
- 2021 Q3面世
- PyTorch Neural Network Exchange
    https://www.bilibili.com/video/BV1Uv411u78D/?spm_id_from=333.999.0.0
    - Pytorch部署模式
    1. model -> TorchScript IR -> TorchScript -> LibTorch
        - Pros
            - 可以支持Pytorch模型里面所有的算子(op)：libtorch就是pytorch的底层实现
            - run fast on x86 CPU and nvidia gpu
        - Cons
            - Run slower than vendor optimized runtime：OpenVINO，TensorRT
            - Binary size is too large for mobile：libtorch库太大（十几M）
    2. model -> TorchScript IR -> ONNX -> ONNXRuntime
        - Pros
            - 支持各种训练框架：TensorFlow、MxNet
            - Run fast with vendor optimized backend
        - Cons
            - 部分Pytorch算子不支持，OP更新跟不上Pytorch框架的更新速度
            - Lower操作之后，产生的计算图过于庞大
                - 每个算子粒度都很细，推理效率很低，需要进行图优化
                - 计算图复杂，hard to distinguish the mapping with the source
                - 有大量的gule operators(Gather,Unsqueeze,etc)，第三方推理引擎不支持
        - 优化方案
            - 简化计算图：ONNX-Simplifier
            - 图优化：第三方推理引擎都有，把大量的小op捏合成一个大op
                - 原理：pattern matching(模板匹配)
                - 不好做
                    - Poor infrastructure for graph rewrites in ONNX，文件是protobuf
                    - Pytorch or ONNX version update, 计算图都会改变，原先的模板匹配失效了
                    - 高层OP的不同参数，可能导致不同的计算图，增加了匹配的复杂性
    3. model -> TorchScript IR -> ONNX -> 第三方推理引擎（TensorRT/OpenVINO/NCNN/...）：当前主流部署方式
        - model -> TorchScript -> 第三方推理引擎： 这个方式目前还比较少，第三方推理引擎对TorchScript支持还比较少，因为ONNX时间比较久，用户多，资料多
        - Pros: fastest inference speed
        - Cons: 部分算子不支持
            - model -> ONNX: 部分算子不支持
            - ONNX -> 第三方推理引擎： 又有部分算子不支持 
    - Lower
        - Simulate unsupported operator with one or more supported operators
        - PyTorch model -> TorchScript IR: 有lower的过程
        - TorchScript IR -> ONNX: 有lower的过程
    - What are the characteristics of a good PyTorch model exchange?
        - Researchers prefer high-level IR for model exchange
        - Vendor library/ runtime prefer high-level IR for aggressive optimization
        - Human reading and editing friendly representatio
        - Closely integrated into the PyTorch ecosystem
    - PNNX
        - Pytorch model -> TorchScript IR -> TorchScript -> PNNX -> ncnn
        - 是一种开放标准，直接利用pytorch API进行算子定义，未建立新的算子库
            - 可以通过PNNX文件转回到Pytorch代码
        - 用户友好的模型表示（沿袭ncnn的格式），并未采用类MLIR的语言表达（tensorflow生态，编译困难，api不稳定）
            - 模型的图是文本文件，可以直接修改
        - Expression operator：完整的算术表达式，阅读方便，减少访存
        - Module opeartor：自动根据Module来合并出一个大算子
        - 支持Pytorch自定义OP（ONNX不支持），OP实现采用cu或cpp代码实现
        - 相对ONNX，更加优异的支持QAT（量化感知训练）过程
        - Shape propagation
            - 根据input shape，自动推导后面层的shape，可以进行常量折叠优化
            - 支持动态shape和静态shape
        - 计算图优化
            - 基于模板匹配
        - 不支持control OP，但不排除未来会增加相关的动态图OP
        - 支持PNNX转回Python脚本，进行模型结构中任意段的输入输出测试
        - 目前的工作重点在与ncnn的打通，是否与OpenVINO、TensorRT打通，需要考虑双方进一步的合作意愿。
        - 目前还不支持模型里面带分支、判断

### ncnn 使用vulkan的通用神经网络推理
https://www.bilibili.com/video/BV1fQ4y1Z7Yx/?spm_id_from=333.999.0.0&vd_source=7798c62f92ce545f56fd00d4daf55e26
- 使用vulkan写GPU代码，可以在所有GPU平台都能用

### ncnn赋能多媒体AI应用
https://www.bilibili.com/video/BV1P84y1F7Wz/?spm_id_from=333.999.0.0&vd_source=7798c62f92ce545f56fd00d4daf55e26
- 应用：物体检测、人脸解构、超分辨率、补帧
- 优化技巧
    - 整张图做计算，显存消耗太大：分块做，降低显存占用
    - 多CPU+多GPU并行加速
    - 在手机端，一次性提交大量GPU任务，影响到GPU显式、UI卡顿：拆成小的GPU计算任务
    - 模型大，GPU模型加载要很久：相同参数的算子复用之前编译的结果
    - 内存池复用：节省内存
    - 动态尺寸输入：输入图任何尺寸都可以，不需要padding
    - 动态任务推理：根据之前的结果，决定要不要进行部分模型的计算
    - 算子融合加速
    - 充分利用机器的大小核
    - 优化内存布局：内存连续访问，减少访问延迟
    - FP16,BF16使用16位表示浮点数：节省50%内存，寄存器里可以存放多1一倍的数据
    - 模型量化：float->int
### 
https://blog.csdn.net/weixin_42137700/article/details/107137353
https://zhuanlan.zhihu.com/p/267272974
https://github.com/Tencent/ncnn
https://github.com/zchrissirhcz/awesome-ncnn
https://www.zhihu.com/question/441269200
https://zhuanlan.zhihu.com/p/454835595


## 参考
- https://zhuanlan.zhihu.com/p/87392811
- https://juejin.cn/post/7055907600071655455
- https://towardsdatascience.com/the-deep-learning-inference-acceleration-blog-series-part-1-introduction-668e39b8b14b
- 公众号：深度学习推理工具链
- https://github.com/HolmesShuan/CNN-Inference-Engine-Quick-View
- https://zhuanlan.zhihu.com/p/519433974
https://zhuanlan.zhihu.com/p/344442534
https://bbs.huaweicloud.com/blogs/332726
https://blog.csdn.net/oakchina/article/details/123848097
https://blog.csdn.net/weixin_42370067/article/details/106135411
https://www.bilibili.com/video/BV14r4y1J7AV?spm_id_from=333.337.search-card.all.click&vd_source=7798c62f92ce545f56fd00d4daf55e26
- 滴滴自研IFX引擎
https://www.bilibili.com/video/BV1MT4y1M7YK?spm_id_from=333.337.search-card.all.click&vd_source=7798c62f92ce545f56fd00d4daf55e26
    - 完整的AI加速、部署解决方案
    - 适用于云/端/IOT/边缘计算等场景
    - 提供汇编级优化，图优化，int8量化，模型瘦身，模型性能分析报告，模型加密等功能
- 业务场景
    - 算法部署要求高
        - 吞吐大，并发高，时延低
        - 应用场景广泛
    - DL框架多样性
     - 异构设备丰富


