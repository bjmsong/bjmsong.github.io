---
layout:     post
title:      知识图谱之二
subtitle:   知识表示学习
date:       2019-10-22
author:     bjmsong
header-img: img/Graph/kg.jpg
catalog: true
tags:
    - 知识图谱
---

> 本文将介绍知识表示学习的定义及主要方法

### 研究意义
知识表示是知识获取与应用的基础,因此知识表示学习问题,是贯穿知识库的构建与应用全过程的关键问题。人们通常以网络的形式组织知识库中的知识,网络中每个节点代表实体(人名、地名、机构名、概念等),而每条连边则代表实体间的关系。然而,**基于网络形式的知识表示**面临诸多挑战性难题,主要包括如下两个方面:

- 计算效率问题：基于网络的知识表示形式中,每个实体均用不同的节点表示。当利用知识库计算实体间的语义或推理关系时,往往需要人们设计专门的图算法来实现,存在可移植性差的问题。更重要的,基于图的算法计算复杂度高,可扩展性差,当知识库规模达到一定规模时,就很难较好地满足实时计算的需求。
- 数据稀疏问题：与其他类型的大规模数据类似,大规模知识库也遵守长尾分布,在长尾部分的实体和关系上,面临严重的数据稀疏问题。例如,对于长尾部分的罕见实体,由于只有极少的知识或路径涉及它们,对这些实体的语义
或推理关系的计算往往准确率极低。

### 定义
以深度学习为代表的表示学习，旨在将研究对象的语义信息表示为稠密低维实值向量。在该低维向量空间中，两个对象距离越近则说明其语义相似度越高。
**知识表示学习**,则是面向知识库中的实体和关系进行表示学习。知识表示学习实现了对实体和关系的**分布式表示**,它具有以下主要优点:
- 显著提升计算效率：知识库的三元组表示实际就是基于独热表示的。如前所分析的,在这种表示方式下,需要设计专门的图算法计算实体间的语义和推理关系,计算复杂度高,可扩展性差。而表示学习得到的分布式表示,则能够
高效地实现语义相似度计算等操作,显著提升计算效率。
- 有效缓解数据稀疏：由于表示学习将对象投影到统一的低维空间中,使每个对象均对应一个稠密向量,从而有效缓解数据稀疏问题,这主要体现在两个方面。一方面,每个对象的向量均为稠密有值的,因此可以度量任意对象之间
的语义相似程度。另一方面,将大量对象投影到统一空间的过程,能够将高频对象的语义信息用于帮助低频对象的语义表示,提高低频对象的语义表示的精确性。
- 实现异质信息融合：不同来源的异质信息需要融合为整体,才能得到有效应用。例如,人们构造了大量知识库,这些知识库的构建规范和信息来源均有不同,例如著名的世界知识库有 DBPedia、YAGO、Freebase 等。大量实体和关系在不同知识库中的名称不同。如何实现多知识库的有机融合,对知识库应用具有重要意义。通过设计合理的表示学习模型,将不同来源的对象投影到同一个语义空间中,就能够建立统一的表示空间,实现多知识库的信息融合。此外,当我们在信息检索或自然语言处理中应用知识库时,往往需要计算查询词、句子、文档和知识库实体之间的复杂语义关联。由于这些对象的异质性,在往常是棘手问题。而知识表示学习亦能为此提供统一表示空间,轻而易举实现异质对象之间的语义关联计算。

>分布式表示（distributed representation）: 孤立地看向量中的每一维，都没有明确对应的含义；而综合各维形成一个向量，则能够表示对象的语义信息，这种方案也是受到人脑的工作机制启发而来。

### 典型应用
- 相似度计算
- 知识图谱补全（link prediction,knowledge graph completion）:利用知识表示学习模型，可以预测两个实体的关系
- 其他应用，如关系抽取、自动问答等


### 主要方法
- 翻译模型：灵感来自word2vec中词汇关系的**平移不变性**,典型的方法包括基于向量的三角形法则和范数原理的TransE模型,通过超平面转化或线性变换处理多元关系的TransH、TransR和TransD模型,通过增加一个稀疏度参数向量解决异构多元关系的TranSparse模型等。
- 组合模型：采用的是向量的线性组合和点积原理,典型特征是将实体建模为列向量、关系建模为矩阵,然后通过头实体向量与关系矩阵的线性组合,再与尾实体进行点积来计算打分函数。经典成员包括采用普通矩阵的RESCAL、采用低秩矩阵的LFM、采用对角矩阵的DistMult 和采用循环矩阵的HolE。
- 神经网络模型：采用神经网络拟合三元组,典型模型包括采用单层线性或双线性网络的SME、采用单层非线性网络的SLM、NTN和MLP,以及采用多层网络结构的NAM 。

### TransE
Bordes等人受word2vec的思想启发，提出了TransE模型，将知识库中的关系看作是实体间的某种平移向量。对于每个三元组(h,r,t),TransE用关系r的向量$$I_r$$作为头实体向量$$I_h$$和尾实体向量$$I_t$$之间的平移。即对于每个三元组(h,r,t),TransE希望:

$$I_h+I_r \approx I_t$$

TransE定义了如下损失函数:

$$f_r(h,t)=|I_h+I_r-I_t|_{L1/L2}$$

在实际学习过程中，为了增强知识表示的区分能力，TransE采用最大间隔方法，定义了如下优化目标函数：

$$\displaystyle \sum_{(h,r,t) \in S}\sum_{(h',r',t') \in S'}{max(0,f_r(h,t)+\gamma-f_{r'}(h',t'))} $$

S是合法的三元组集合，S‘是错误的三元组集合，$$\gamma$$ 为合法三元组得分与错误三元组得分之间的间隔距离。

错误三元组并非随机产生，为了选取有代表性的错误三元组，TransE将S中每个三元组的头实体、关系和尾实体其中之一随机替换成其他实体或关系来得到S’。

完整的算法如图：
<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/KG/TransE.png) 
</li> 
</ul> 

TransE具有参数较少、计算复杂度低、可以直接建立实体和关系之间的复杂语义联系等优点，已经成为知识表示学习的代表模型。


### 主要挑战

#### 1. 复杂关系建模

按照知识库中关系两端连接实体的数目，可以将关系划分为1-1，1-N，N-1，N-N四种类型。例如N-1类型关系指的是，该类型关系中的一个尾实体会平均对应多个头实体，我们将1-N，N-1，N-N称为复杂关系。

以TransE为代表的知识表示学习模型在处理复杂关系时，性能及准确率显著降低。

<ul> 
<li markdown="1"> 
例如，有2个二元组，分别是（美国，总统，奥巴马）和（美国，总统，布什），这里的关系“总统”是典型的1-N的复杂关系，如果用TransE从这2个三元组学习知识，将会使得奥巴马和布什的向量变得相同。
![]({{site.baseurl}}/img/KG/复杂关系示例.png) 
</li> 
</ul> 

为了实现表示学习对复杂关系的建模，人们尝试对TransE进行扩展，例如：TransH、TransR、TransD、TranSparse、TransA、TransG和KG2E。

#### 2. 多源信息融合

TransE仅利用知识图谱的三元组结构信息进行表示学习，尚有大量与知识有关的其他信息没有得到有效利用，例如：

- 知识库中的其他信息，如实体和关系的描述信息、类别信息等
- 知识库外的海量信息，如互联网文本蕴含了大量与知识库实体和关系有关的信息。

这方面的代表性工作有：

- DKRL模型
- 文本与知识库融合的知识表示学习

#### 3. 关系路径建模

在知识图谱中，多步的关系路径也能够反映实体之间的语义关系。这方面的算法有Path-Constraint Random Walk，Path Ranking Algorithms、Path-based TransE等。


### 参考资料
- 《知识图谱发展报告》(2018) 中国中文信息学会
- 《知识表示学习研究进展》 刘知远等
- https://github.com/thunlp/KRLPapers
- https://github.com/thunlp/openKE