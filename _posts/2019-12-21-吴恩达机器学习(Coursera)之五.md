---
layout:     post
title:      吴恩达机器学习(Coursera)之五
subtitle:   
date:       2019-12-21
author:     bjmsong
header-img: img/machineLearning/machineLearning.png
catalog: true
tags:
    - 机器学习
---
> 本文是课程的第16~18章，将介绍推荐系统、大规模机器学习、OCR。

## 十六、推荐系统(**Recommender Systems**) 

吴恩达老师的推荐系统概念跟我之前了解的不大一致。基于内容的推荐看成是一个回归问题，之前我理解是基于特征做匹配。而协同过滤更像是ALS算法。

16.1 问题形式化 

16.2 基于内容的推荐系统 

<ul> 
<li markdown="1"> 
把推荐问题看成是一个回归问题。这种方法存在的问题是：电影本质的特征（如爱情占多少比重，动作占多少比重），很难得到。
![]({{site.baseurl}}/img/CourseraML/回归预测推荐系统.png) 
</li> 
</ul> 

16.3 协同过滤 

<ul> 
<li markdown="1"> 
假设用户已经给了他们对电影的偏好(\theta)
![]({{site.baseurl}}/img/CourseraML/协同过滤的优化目标.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
通过随机初始化特征，通过迭代，可以自行学习所要使用的特征，但是这种方法不是很高效！
![]({{site.baseurl}}/img/CourseraML/协同过滤迭代.png) 
</li> 
</ul> 

16.4 高效的协同过滤算法 

<ul> 
<li markdown="1"> 
整合两个优化目标，同时优化用户特征和产品（电影）特征
![]({{site.baseurl}}/img/CourseraML/协同过滤优化目标合并.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/协同过滤算法.png) 
</li> 
</ul> 

16.5 向量化：低秩矩阵分解 

协同过滤的向量化实现

16.6 推荐工作上的细节：均值归一化 

<ul> 
<li markdown="1"> 
对于没有给任何电影打过分数的新用户（冷启动问题），协同过滤算法倾向于给出全零的评分。
![]({{site.baseurl}}/img/CourseraML/协同过滤冷启动.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
均值归一化可以解决这个问题
![]({{site.baseurl}}/img/CourseraML/均值归一化.png) 
</li> 
</ul> 

​	

## 十七、大规模机器学习(**Large Scale Machine Learning**) 

17.1 大型数据集的学习 

<ul> 
<li markdown="1"> 
在采用大数据训练模型之前，可以先用一个小规模的数据集，得到在训练集和验证集上的误差，如果误差如左图所示，那么需要增大训练集数量，但如果误差如右图所示，则表明，增大训练集数量不能增加模型效果，此时更应该考虑增加模型复杂度，或者增加特征数量。
![]({{site.baseurl}}/img/CourseraML/是否采用大数据集训练.png) 
</li> 
</ul> 

17.2 随机梯度下降法 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/梯度下降.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/sgd.png) 
</li> 
</ul> 

17.3 mini-batch梯度下降 

Batch gradient descent: Use all     examples in each iteration

Stochastic gradient descent: Use 1 example in each iteration

Mini-batch gradient descent: Use    examples in each iteration

17.4 随机梯度下降收敛 

<ul> 
<li markdown="1"> 
为了更高效地检查梯度下降过程是否收敛，随机梯度下降可以不在全量样本上计算误差的变化
![]({{site.baseurl}}/img/CourseraML/梯度下降收敛.png) 
</li> 
</ul> 

学习率可以设置为逐渐衰减（E.g. $$\alpha = \frac{const1}{iteration Number+const2}$$）

17.5 在线学习 

当拥有连续充沛的数据流，并且想做模型的实时更新，可以使用在线学习。

<ul> 
<li markdown="1"> 
跟随机梯度下降法的区别：随机梯度下降法的训练样本固定的，而在线学习的训练样本是不固定，实时更新的
![]({{site.baseurl}}/img/CourseraML/在线学习.png) 
</li> 
</ul> 

17.6 map-reduce

 <ul> 
<li markdown="1"> 
可以将计算分发到多台不同的服务器，也可以分到同一台服务器不同的核。开源实现：Hadoop
![]({{site.baseurl}}/img/CourseraML/mapreduce.png) 
</li> 
</ul> 



## 十八、应用实例：图片文字识别(**Application Example: Photo OCR**) 

18.1 问题描述和流程图

18.2 滑动窗口 

18.3 获取大量数据和人工数据 

18.4 上限分析：哪部分管道的接下去做 



## 十九、总结与致谢

 <ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/总结.png) 
</li> 
</ul> 

 <ul> 
<li markdown="1"> 
Thank you Ng. It's my honor to be your student!
![]({{site.baseurl}}/img/CourseraML/thankyou.png) 
</li> 
</ul> 

