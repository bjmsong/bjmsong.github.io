---
layout:     post
title:      分布式机器学习
subtitle:   
date:       2022-09-23
author:     bjmsong
header-img: 
catalog: true
tags:
    - 并行计算
---
## https://github.com/wangshusen/DeepLearning
- Chapter 9 Parallel Computing

### 9.1 Basics and MapReduce
- 并行梯度下降：数据分布在不同处理器上，分别计算梯度
- 通信方式
    - 共享内存(share memory): 没有办法做到大规模并行
    - 消息传递(message passing)：节点之间通信
- 架构
    - client-server：worker node上存储数据、做计算
    - peer-to-peer：没有server，所有的节点都做计算，每个节点都有几个邻居，邻居之间可以通信 
#### MapReduce
- 架构：client-server
- 通信：message passing
- 适合做大数据处理，但是来做机器学习不高效 
- 步骤
    - 数据平分到所有节点：数据并行
    - broadcast：server把参数广播到所有的worker节点上
    - map：所有的worker计算梯度
    - reduce：worker把计算结果传回给server，server把结果sum起来 
    - 更新参数
    - 迭代以上步骤
- 时间开销
    - computation
    - Communication
        - Communication complexity：how many words are transmitted between server and workers
        - Latency(网络延迟)：
        - Communication time： Communication complexity/bandwith + Latency
    - 同步
        - 所有的worker都必须等最慢的worker

### 9.2 Parameter server and decentralized network
#### Parameter server
- 应用广泛
- client-server
- message passing
- 异步
    - 每个节点计算完成后，就跟server通信，不用等所有节点都算完，就更新server端参数
    - 比同步快很多
    - 不同worker之间效率要差不多，不然收敛会有问题 
    - 《hogwild a lock-free approach to parallelizing stochastic gradient descent》
- Ray： 开源，支持parameter server
    - 《A distributed framework for emerging AI Applicaitons》
    + 可以很方便地将python代码多线程、多机器运行
    - https://www.zhihu.com/question/576114739
    + https://www.youtube.com/watch?v=q_aTbb7XeL4&ab_channel=JackofSome 
    + https://www.youtube.com/watch?v=LmROEotKhJA&ab_channel=Databricks
    + https://www.infoq.cn/article/ualtzk5owdb1crvhg7c1
    + https://github.com/ray-project/ray
    + https://zhuanlan.zhihu.com/p/460600694
- 数据并行

#### Decentralized network
- peer-to-peer：没有server，所有节点都是worker
- message passing
- 数据并行
- 同步 or 异步
- 目前不太流行

### 9.3 TensorFlow's mirrored strategy and ring all-reduce
- 六个并行计算策略
    - MirroredStrategy：单机多GPU
        - 同步随机梯度下降
        - 原理：Ring All-Reduce
            - All-Reduce：Every node gets a copy if the result of reduce
                - 实现方法
                    - reduce+broadcast
                    - all-to-all commnuication: 通信量太大
                    - ring all-reduce
                        - 环形网络
                        - 每个节点传递梯度给下一个节点，传两圈所有节点上就有了所有的梯度
                        - 优化：每个节点的梯度分成几个部分，算完一部分就通信（并发通信），同时计算下一部分
    - TPUStrategy
    - MultiWorkerMirroredStrategy
    - CentralStorageStrategy
    - ParameterServerStrategy
    - OneDeviceStrategy

### 9.4 联邦学习
- https://www.zhihu.com/question/315844487/answer/920036089
- 《A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection》

## UCB cs294 19fall - AI-Systems Distributed Training
- https://ucbrise.github.io/cs294-ai-sys-fa19/assets/lectures/lec06/06_distributed_training.pdf

## adasum
- 在all-reduce时，近似模拟单卡训练的梯度权重更新的一种算法
- https://bbs.huaweicloud.com/blogs/208668

## 科普：分布式深度学习系统 
- https://zhuanlan.zhihu.com/p/29032307

## 分布式深度学习框架的前世今生，从 MapReduce 到 Pathways
https://zhuanlan.zhihu.com/p/533852982?utm_id=0
- 上古时代的分布式机器学习平台：MPI,Hadoop,Spark
- 参数服务器
- Pathways


## Paper
- Large Scale Distributed Deep Networks
    - Google的第一代深度学习系统Distbelief
- Scaling Distributed Machine Learning with System and Algorithm Co-design
- Distributed Training of Deep Learning Models: A Taxonomic Perspective
- A Survey on Distributed Machine Learning
- TensorFlow:Large-Scale Machine Learning on Heterogeneous Distributed Systems
- LARGE BATCH TRAINING OF CONVOLUTIONAL NETWORKS
- Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes
- Accurate, large minibatch sgd: Training imagenet in 1 hour
    - https://blog.csdn.net/Jing_xian/article/details/79982209
    - https://www.zhihu.com/question/60874090
- Imagenet training in minutes
- https://zhuanlan.zhihu.com/p/29032307
- https://www.zhihu.com/question/473002888?utm_campaign=Sharon&utm_medium=social&utm_oi=30249563717632&utm_psn=1577219211981664256&utm_source=wechat_session&utm_content=group2_supplementQuestions
- https://www.zhihu.com/question/360728623/answer/939929658?utm_campaign=shareopn&utm_medium=social&utm_oi=30249563717632&utm_psn=1577219055790055424&utm_source=wechat_session
- Deep Learning with COTS HPC

## 源码：ring-AllReduce
https://github.com/baidu-research/baidu-allreduce
https://github.com/baidu-research/tensorflow-allreduce
https://zhuanlan.zhihu.com/p/435438871?utm_campaign=shareopn&utm_medium=social&utm_oi=30249563717632&utm_psn=1581400190145159168&utm_source=wechat_session
https://zhuanlan.zhihu.com/p/69797852?utm_campaign=shareopn&utm_medium=social&utm_oi=30249563717632&utm_psn=1581400245883404288&utm_source=wechat_session
https://zhuanlan.zhihu.com/p/79030485?utm_campaign=shareopn&utm_medium=social&utm_oi=30249563717632&utm_psn=1581400341668712448&utm_source=wechat_session

## 《Dive into Big Model Training》
https://github.com/qhliu26/BM-Training
https://zhuanlan.zhihu.com/p/506052424?utm_id=0


## Pathways
- 李沐读paper

## ZeRo
https://zhuanlan.zhihu.com/p/394064174?utm_id=0


## DeepSpeed
https://github.com/microsoft/DeepSpeed
https://www.youtube.com/watch?v=pDGI668pNg0&ab_channel=MarkSaroufim
https://www.youtube.com/watch?v=zqsOEzKZX2Y&ab_channel=MicrosoftResearch
https://www.cnblogs.com/rossiXYZ/p/15815013.html
http://aheader.org/2021/10/02/deepspeed-source-code-study/
https://morioh.com/p/1a44ffc5e457
https://deepspeed.readthedocs.io/en/latest/_modules/deepspeed.html
https://blog.csdn.net/u010751000/article/details/123516433
https://zhuanlan.zhihu.com/p/414773915

## Jax
https://zhuanlan.zhihu.com/p/468223891
https://www.zhihu.com/question/306496943
https://zhuanlan.zhihu.com/p/536377657

## Consistent Hashing 一致性哈希
https://www.youtube.com/watch?v=lm6Zeo3tqK4&list=PLLuMmzMTgVK7XfFadhkPuF_ztvhxbriDr&index=2&ab_channel=HuaHua
- 通过hash function把任务hash到不同server上
- server数量会变化：增加server、server挂掉
    - 重新hash，会导致key需要移动到不同server上
- 一致性哈希可以保证只有少部分key需要移动
    - ring-based

## 参考资料
- https://mp.weixin.qq.com/s?__biz=MjM5NjQ5MTI5OA==&mid=2651749161&idx=1&sn=2a34bc1e36fbf259ac63ffe7c94f528b&chksm=bd12a2648a652b72a9027a572a0038d3b8a6448949c53e7aefab617a12811da84efdc88bcd29&mpshare=1&scene=1&srcid=1025pjqMWAMqES7L6Vsb6QTE#rd
- https://hunch.net/?p=151364
- https://zhuanlan.zhihu.com/p/40115973
- https://www.zhihu.com/column/c_127078572
- https://www.zhihu.com/column/c_1434596730881269762?utm_source=wechat_session&utm_medium=social&utm_oi=30249563717632
- 《分布式机器学习》（刘铁岩）
- 《How to scale distributed deep learning?》
- https://www.zhihu.com/question/24070061/answer/1325421472
- https://zhuanlan.zhihu.com/p/450689346
- https://zhuanlan.zhihu.com/p/466002243
- http://www.cnblogs.com/LeftNotEasy/archive/2010/11/27/1889598.html
