---
layout:     post
title:      推荐系统三十六式之
subtitle:   数据采集
date:       2020-04-20
author:     bjmsong
header-img: img/Recommendation System/th.jpg
catalog: true
tags:
    - 推荐系统
---
>可以结合这篇博客一起看：[推荐系统之数据源](https://bjmsong.github.io/2020/01/03/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%BA%90/)

- 数据采集，按照用途分类
    - 报表统计：统计一些核心的产品指标，例如次日留存，七日留存等，一方面是为了监控产品的健康状况，另一方面是为了对外秀肌肉
    - 数据分析：不但需要知道产品是否健康，还需要知道为什么健康、为什么不健康，做对了什么事、做错了什么事，要从数据中去找到根本的原因
        - **数据分析工作，最后要产出的是比较简明清晰直观的结论，这是数据分析师综合自己的智慧加工出来的，是有人产出的**
        - 它主要用于指导产品设计、指导商业推广、指导开发方式。走到这一步的数据采集，已经是实打实的数据驱动产品了
    - 机器学习： 样本是多多益善，数据采集的维度，也就是字段数多多益善，但另一方面，数据是否适合分析，数据是否易于可视化地操作并不是核心的内容
- 下面专门介绍为了满足推荐系统，需要怎么收集日志、采集数据


### 数据模型
- **所谓数据模型，其实就是把数据归类**
- 如果看山是山，一个数据来源一个数据来源地去对待的话，那将效率非常低下，因此需要首先把要收集的日志数据归入几个模型。不同的数据应用，数据模型略有不同。
<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/Recommendation System/36/数据模型.png) 
</li> 
</ul> 


### 数据在哪
- Event 类别的数据从后端各个业务服务器产生的日志来，Item 和 User 类型数据，从业务数据库来，还有一类特殊的数据就是 Relation 类别，也从业务数据库来

#### 埋点
- 按技术手段分类
    - SDK埋点： 最经典古老的埋点方法，就是在开发自己的App或者网站时，嵌入第三方统计的 SDK 
        - App如友盟等，网站如 Google Analytics 等
        - SDK 在要收集的数据发生点被调用，将数据发送到第三方统计，第三方统计得到数据后再进一步分析展示
        - 这种数据收集方式对推荐系统的意义不大，因为得不到原始的数据而只是得到统计结果，我们可以将其做一些改动，或者自己仿造做一些开发内部数据采集 SDK，从而能够收集到鲜活的数据
        - 缺点：复杂度高，一旦埋点有错，需要更新客户端版本
    - 可视化埋点：在SDK埋点基础上做了进一步工作，埋点工作通过可视化配置的方式完成，一般是在App端或者网站端嵌入可视化埋点套件的SDK，然后再管理端接收前端传回的应用控件树，通过点选和配置，指令前端收集那些事件数据。业界有开源方案实现可参考，如`Mixpanel`
        - 缺点：收集数据不能收集到非界面数据，例如收集了点击事件，也仅仅能收集一个点击事件，却不能把更详细的数据一并返回
    - 无埋点：所谓无埋点不是不埋点收集数据，而是尽可能多自动收集所有数据，但是使用方按照自己的需求去使用部分数据
- 按收集数据的位置分类：前端埋点，后端埋点
    - 举个例子，要收集用户的点击事件，前端埋点就是在用户点击时，除了响应他的点击请求，还同时发送一条数据给数据采集方。后端埋点就不一样了，由于用户的点击需要和后端交互，后端收到这个点击请求时就会在服务端打印一条业务日志，所以数据采集就采集这条业务日志即可
- 埋点是一项非常复杂繁琐的事情，需要前端工程师或者客户端工程师细心处理，不在本文讨论范围内。但是幸好，国内如**神策数据**等公司，将这些工作已经做得很傻瓜化了，大大减轻了埋点数据采集的困扰
- **对于推荐系统来说，所需要的数据基本上都可以从后端收集，采集成本较低，但是有两个要求：要求所有的事件都需要和后端交互，要求所有业务响应都要有日志记录。这样才能做到在后端收集日志**
- 后端收集业务日志好处很多，比如下面几种
    - 实时性。由于业务响应是实时的，所以日志打印也是实时的，因此可以做到实时收集
    - 可及时更新。由于日志记录都发生在后端，所以需要更新时可以及时更新，而不用重新发布客户端版本
    - 开发简单。不需要单独维护一套 SDK。

### 元素有哪些
- 后端收集事件数据需要业务服务器打印日志。需要打印哪些信息才算是一条完整的时间数据呢？大致需要包含下面的几类元素
    - 用户 ID，唯一标识用户身份
    - 物品 ID，唯一标识物品。这个粒度在某些场景中需要注意，例如电商，物品的 ID 就不是真正要去区别物和物之间的不同，而是指同一类试题，例如一本书《岛上书店》，库存有很多本，并不需要区别库存很多本之间的不同，而是区别《岛上书店》和《白夜行》之间的不同
    - 事件名称，每一个行为一个名字
    - 事件发生时间，时间非常重要
- 以上是基本的内容，下面再来说说加分项
    - 事件发生时的设备信息和地理位置信息等等
    - 从什么事件而来
    - 从什么页面而来
    - 事件发生时用户的相关属性
    - 事件发生时物品的相关属性
- **把日志记录想象成一个 Live 快照，内容越丰富就越能还原当时的场景**

### 如何收集
<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/Recommendation System/36/数据采集架构.png) 
</li> 
</ul> 

- 数据源
    - **非常稳定的网络服务器日志**，`nginx`或者`Apache`产生的日志
        - 因为有一类埋点，在PC互联网时代，有一种事件数据收集方式是，放一个一像素的图片在某个要采集数据的位置。这个图片被点击时，向服务端发送一个不做什么事情的请求，只是为了在服务端的网络服务器那里产生一条系统日志。 这类日志用`logstash`收集
    - **业务服务器**： 处理具体场景的具体业务，甚至推荐系统本身也是一个业务服务器
        - 有各自不同的日志记录方式，例如 Java 是 Log4j，Python 是 Logging 等等，还有 RPC 服务。这些业务服务器通常会分布在多台机器上，产生的日志需要用 Flume 汇总
- `Kafka`是一个分布式消息队列，按照 Topic 组织队列，订阅消费模式，可以横向水平扩展，非常适合作为日志清洗计算层和日志收集之间的缓冲区。所以一般日志收集后，不论是 Logstash 还是 Flume，都会发送到 Kafka 中指定的 Topic 中
- 在 Kafka 后端一般是一个流计算框架，上面有不同的计算任务去消费 Kafka 的数据 Topic，流计算框架实时地处理完采集到的数据，会送往分布式的文件系统中永久存储，一般是 HDFS
- 日志的时间属性非常重要。因为在 HDFS 中存储日志时，为了后续抽取方便快速，一般要把日志按照日期分区。当然，在存储时，按照前面介绍的数据模型分不同的库表存储也能够方便在后续构建推荐模型时准备数据

### 质量检验
- 关注数据质量，大致需要关注下面几个内容
    - 是否完整？事件数据至少要有用户 ID、物品 ID、事件名称三元素才算完整，才有意义
    - 是否一致？一致是一个广泛的概念。数据就是事实，同一个事实的不同方面会表现成不同数据，这些数据需要互相佐证，逻辑自洽
    - 是否正确？该记录的数据一定是取自对应的数据源，这个标准不能满足则应该属于 Bug 级别，记录了错误的数据
    - 是否及时？虽然一些客户端埋点数据，为了降低网络消耗，会积攒一定时间打包上传数据，但是数据的及时性直接关系到数据质量。由于推荐系统所需的数据通常会都来自后端埋点，所以及时性还可以保证


### 参考资料
http://www.woshipm.com/pmd/1667903.html