---
layout:     post
title:      跟着李沐大佬读paper
subtitle:   
date:       2022-09-16
author:     bjmsong
header-img: 
catalog: true
tags:
    - 
---
- b站看李沐大神的视频：auto ml现在包已经很成熟了，调参侠的作用越来越小了。跟着大神精度论文，先看摘要和结论，图表，再决定要不要读细节。 娓娓道来论文背后的故事，大佬特别谦虚，会站在几年前的视角，会说这个自己也看不懂，我们先圈起来，后面再看。听大佬一席话，胜读N年小红书。一定要多读论文，多总结，形成自己的观点。观点没有绝对的正确，关键是要跟别人不一样，不然你对这个世界的贡献就没那么多了！从这个角度讲，黎哥有些话是对的，要知其然更要知其所以然。 
- 好的文章应该是结构清晰，图表详实，深入浅出，让读者有收获，没有多余的内容
- 沐神的论文讲解深入浅出，让我有了一个更全局的视角，破除了迷雾，对整个系统更加清晰，其实要解决的问题主要就是这些，没有太高深的。主要是平常工作视野太局限了

## AlexNet
深度学习奠基作，后面被大量跟进研究。但李沐认为写得一般，只讲了做了什么，没有讲跟其它工作的区别，有什么意义，很多地方over engineer，很多观点现在看来是错的。深度学习领域日新月异，尽管这么牛的文章，很多观点现在看来是错误的：比如relu比sigmod、tanh快，比如不需要这么多层全连接，比如alexnet减少一层效果就会差很多等等。
- 主要工作
    - 有些地方作者没有highlight，但之后被认为非常重要。比如倒数第二层抽取的embedding（信息压缩，特征抽取，变成机器能理解的信息，并且保留原始信息），比如原始图片没有做预处理(end2end，计算机视觉领域过去30年的特征抽取工作不重要了，简单有效的东西是能够持久的)。
    - 模型架构：5层卷积+3层全连接，ReLU，正则化。在工程上花了很大精力，为了在两块GPU上训练模型，做了模型并行。
    - 防止过拟合：数据增强、dropout（当时认为是模型ensemble，现在认为等价于L2）
    - 模型训练：SGD（当时还有LBFGS之类的，这个之后SGD成为主流），momentum，参数初始化，learning_rate衰减（现在的做法是学习率先设置为一个比较小的值，然后线性上升，然后在逐渐衰减）
    - 不同层的神经网络在学什么：可解释性现在还不行

## ResNet，解决深度神经网络训练难的问题（深度越深，训练误差和测试误差反而都变大了），在2015年做出了152层的网络，并赢得了当你的ImageNet，也达到了人类的水平。
- 显式地构造了一个identity mapping，学习残差（这个残差跟gbdt里面的残差概念一致，大佬说这是20年前很火的，我用的东西太老了。。。）

## 斯坦福2022AI指数报告
- 没有突破性工作，数据更大，模型更大，硬件成本更低，训练效率更高
    - 论文
        - 每年发表30W+论文，99%以上论文没什么意义，可以看成是写论文的人学习AI的练习题
        - 中美的合作数量全球第一
    - 图像：分类，生成，deep fake，姿势识别，语义分割，人脸识别，视觉推理
    - 视频：
    - 自然语言处理：语言理解，文本摘要，语言推理，情绪识别，机器翻译
    - 语音：识别
    - 推荐系统：业务强相关，开源的数据集不足以反应业务真实情况
    - 强化学习：玩游戏、下国际象棋
    - 硬件：在经典任务上达到一定精度所需要的模型训练时间/成本，训练用到的GPU数量（最多 4000多张卡，光GPU就4000多万美金。。。）
    - 机器人：机械臂价格
- AI伦理：公平，安全，偏见
- AI的投资还是越来越火！最火的还是美国和中国。领域有数据管理处理、云、医药、金融科技、自动驾驶等。
- 每年的计算机毕业生数量一直在增长，北美比7年前增加了3倍（3W vs 1W），21%的博士是ai方向的（太卷了）
- 立法

## 图神经网络 《A Gentle Introduction to Graph Neural Networks》
- 技术博客，发表在distill，没有提供目录
    - distill文章特点，图很重要，文字基本在解释图 
- GNN十几年前提出，近几年发展很大，在很多领域有应用：药物发现、物理模拟、虚假新闻检测、交通预测、推荐系统等  
- 重点是如何通过GNN学到顶点、边、整个图的向量表达
- 首先是将对象表示成图（图片、文本、社交网络等等）
- 图上面的问题主要有
    - graph-level
    - node-level
    - edge-level
- 将图跟神经网络兼容
    - node、edge、global-context：embedding表示
    - connectivity：邻接列表
- GNN：输入图，输出图，图结构保持，但是属性发生变化
    - 最简单的做法：graph、node、edge有各自对应的mlp
    - 信息传 递：先汇聚(pooling,把邻居的向量加入进来)，再变换(mlp)
- 调参很玄学
- 其它技术
    - 其它图结构：节点之间有多条边，子图
    - 图采样：数据量太大训练不动 
- 李沐大神的评价：图的优化很难（图太稀疏、动态），GNN对超参数很敏感。目前研究很多，但是工业界的应用很少。

## Transformer

## Bert
- nlp领域第一次有了通用的预训练模型
- 在大量没有label的数据上学习到的模型应用到其它任务：迁移学习
- “芝麻街”系列：ELMo 、BERT
- 两种预训练模型的使用方式
https://blog.csdn.net/sinat_25394043/article/details/104308576
    - feature-based：ELMo
    - fine-tuning：BERT，GPT
- 预训练
    - 没有label的数据上进行训练：非常大的数据集 
    - wordpiece分词：把出现频率不高的词，用出现频率较高的子序列替换（找词根 ）
    - masked language model：随机替换，做"完形填空"
    - next sentence prediction ：NSP
- fine-tuning
    - 用预训练得到的参数初始化，然后用有label的数据微调

## 参数服务器：大佬自己的工作

## GAN

## GPT系利

## Pathways

## 参考资料
https://github.com/mli/paper-reading


