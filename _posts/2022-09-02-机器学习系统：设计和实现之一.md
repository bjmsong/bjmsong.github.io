---
layout:     post
title:      机器学习系统：设计和实现
subtitle:   之一
date:       2022-09-02
author:     bjmsong
header-img: 
catalog: true
tags:
    - ML System
---
## 2. 导论
### 机器学习应用
- 学习模式
    - 监督学习：已知输入输出对应关系情况下的学习，比如：给定输入图像和它对应的内容标签，学习图像分类（Classification）。
    - 无监督学习：只有输入数据但不知道输出标签情况下的学习，比如：给定一堆猫和狗的图像，自主学会猫和狗的分类，这种无监督分类也称为聚类（Clustering）。
    - 强化学习：给定一个学习环境和任务目标，算法自主地去不断尝试、改进自己、以实现任务目标 ，比如： AlphaGo围棋就是用强化学习实现的，给定的环境是围棋的规则、而目标则是胜利得分。
- 应用领域
    - 计算机视觉
    - 自然语言处理
    - 智能决策
- 机器学习系统位于机器学习应用和通用处理器/智能加速器之间

### 设计目标
- 支持多种神经网络
- 支持自动微分
- 支持数据管理和处理
- 支持模型的训练和部署
- 高效使用硬件加速器
- 分布式计算

- 机器学习系统：TensorFlow，PyTorch、PaddlePaddle、MindSpore

### 基本组成
- 编程接口
    - 不仅需要高层次简易编程（Python，Julia和Java），同时也需要支持低层次高性能编程（利用C和C++函数调用操作系统和硬件加速器）
- 计算图
    - 利用不同编程接口实现的机器学习程序需要共享一个运行后端。实现这一后端的关键技术是：应用无关的计算图。计算图包含计算节点，节点之间的边表达计算依赖。计算图可以被同步和异步执行。
- 编译器前端
    - 给定一个计算图，机器学习框架会对计算图做一系列优化。和硬件无关的优化由编译器前端实现。编译器前端实现包括：中间表达，自动微分，类型推导和静态分析等等。
- 编译器后端和运行时
    - 机器学习框架利用编译器后端对计算图可以进一步针对硬件的特性（例如说，L2/L3大小，指令流水线长度）进行性能优化。最终优化后的计算图通过运行时执行在通用处理器（CPU）或者是硬件加速器之上。运行时需要实现算子选择和内存分配等技术
- 硬件加速器
    - 现代硬件加速器提供了丰富的编程接口。
- 数据处理
    - 机器学习系统拥有专门的数据处理框架来实现数据读取，存储和预处理的功能由数据处理模块（例如，TensorFlow的tf.data和PyTorch的DataLoader）。这一框架需要针对机器学习应用实现易用性，保序性和高效性等设计目标。
- 模型部署
    - 为了确保模型可以在内存有限的硬件上执行，我们会使用模型转换，量化，蒸馏等模型压缩技术。同时，我们也需要实现针对推理硬件平台（例如，英伟达Jetson）的模型算子优化。最后，为了保证模型的安全（不被黑客窃取），实践者还会对模型进行混淆设计。
- 分布式训练

## 3. 编程接口
- 平衡框架性能和易用性
    - 为了达到最优的性能，开发者需要利用硬件亲和的编程语言如：C和C++来进行开发
        - C和C++的使用使得机器学习框架可以高效调用硬件的底层API，从而最大限度发挥硬件性能
        - 现代操作系统（如Linux和Windows）提供丰富的基于C和C++的编程接口（如文件系统，网络编程，多线程管理等），通过直接调用操作系统API，可以降低框架运行的开销
    - 从易用性的角度分析，机器学习框架的使用者往往具有丰富的行业背景（如数据科学家，生物学家，化学家，物理学家等）。他们常用的编程语言是高层次脚本语言：Python，Matlab，R和Julia
- 核心设计目标
    - 要具有易用编程接口来支持用户用高层次语言如Python来实现机器学习算法
    - 也要具备以C和C++为核心的低层次编程接口，使得框架开发者可以用C和C++实现大量高性能组件，从而在硬件上高效执行。

### 机器学习系统编程模型的演进
- 第一代：高层次编程语言来编写机器学习程序：Lua（Torch），Python（Theano）
- 第二代：以C和C++作为核心编程接口：Caffe
    - 训练深度神经网络需要消耗大量的算力，而这些算力无法被以Lua和Python所主导开发的Torch和Theano所满足
    - 计算加速卡（如英伟达GPU）的通用编程接口（例如CUDA C）日趋成熟，而构建于CPU多核技术之上的多线程库（POSIX Threads）也被广大开发者所接受。因此，许多的机器学习用户希望基于C和C++来开发高性能的深度学习应用
- 第三代：Python作为面向用户的主要前端语言，而利用C和C++实现高性能后端：TensorFlow，PyTorch，MXNet，CNTK
    - 大量基于Python的前端API确保了TensorFlow可以被大量的数据科学家和机器学习科学家接受，同时帮助TensorFlow能够快速融入Python为主导的大数据生态（大量的大数据开发库如Numpy，Pandas，SciPy， Matplotlib和PySpark）
    - Python具有出色的和C语言的互操作性，这种互操作性已经在多个Python库中得到验证
    - Keras和TensorLayer等高层次机器学习开发库提供了更高层次的Python API从而可以快速导入已有的模型， 这些高层次API进一步屏蔽了底层框架的实现细节，因此Keras和TensorLayer可以运行在不同的机器学习框架之上。
- 第四代：JAX，MindSpore
    - MindSpore在继承了TensorFlow，PyTorch的Python和C/C++的混合接口的基础上，进一步拓展了机器学习编程模型从而可以高效支持多种AI后端芯片（如华为Ascend，英伟达GPU和ARM芯片），实现了机器学习应用在海量异构设备上的快速部署
    - MindSpore进一步完善了机器学习框架的分布式编程模型的能力，从而让单节点的MindSpore程序可以无缝地运行在海量节点上

### 机器学习工作流
- 数据处理：读取、预处理、特征变换
- 模型结构
- 损失函数和优化算法
- 训练过程：给定一个数据集、模型、损失函数和优化器，用户需要训练API来定义一个循环（Loop）从而将数据集中的数据按照小批量（mini-batch）的方式读取出来，反复计算梯度来更新模型。这个反复的过程称为训练
- 测试和调试

### 定义深度神经网络
- 构建深度神经网络结构始终遵循最基本的规则：
1. 承载计算的节点；
2. 可变化的节点权重（节点权重可训练）；
3. 允许数据流动的节点连接
- 机器学习编程库中神经网络是以层为核心，它提供了各类神经网络层基本组件；将神经网络层组件按照网络结构进行堆叠、连接就能构造出神经网络模型。
#### 以层为核心定义神经网络
- 常用层：全连接、卷积、池化、RNN、BatchNorm、dropout...

#### 神经网络层的实现原理
- 机器学习编程库大都提供了更高级用户友好的API，它将神经网络层抽象成一个基类，所有的神经网络层实现都继承基类调用低级API
    - 不需要用户手动管理训练变量
- 神经网络层需要的功能：该层的训练参数（变量，包括初始化方法和训练状态）以及计算过程
- 神经网络模型需要的功能：对神经网络层管理和神经网络层参数的管理
    - MindSpore的Cell、PyTorch的Module
    - 将Layer(负责单个神经网络层的参数构建和前向计算)和Model(负责对神经网络层进行连接组合和神经网络层参数管理)抽象成一个方法，该方法既能表示单层神经网络层也能表示包含多个神经网络层堆叠的模型
- 神经网络抽象方法(以MindSpore的Cell为例)
    - 构造器(__init__): 初始化神经网络层和神经网络层参数的存储
        - self._params = OrderedDict()
            - OrderedDict: 初始化神经网络层和神经网络层参数的存储；它的输出是一个有序的，相比与Dict更适合深度学习这种模型堆叠的模式。
        - self._cells = OrderedDict()
        - self.training = False
    - 设置属性(__setattr__): 参数和神经网络层的管理
        - isinstance(value, parameter)
        - isinstance(value, Cell)
    - 执行计算(__call__): 实现神经网络层时在这里定义计算过程
        - 动态图模式：转发计算并返回结果
        - 静态图模式：编译并运行cell
    - 训练参数(parameters_and_names): 为了给优化器传所有训练参数
        - 遍历神经网络层，返回所有神经网络层参数
    - 神经网络层(cells_and_names)
        - 返回所有神经网络层迭代器
    - 自定义方法


#### 自定义神经网络层
- 假设已经有了神经网络模型抽象方法Cell，构建Conv2D将继承Cell，并重构__init__和__call__方法，在__init__里初始化训练参数和输入参数，在__call__里调用低级API实现计算逻辑
```python
# 接口定义：
卷积层的接口：convolution(input, filters, stride, padding)
变量：Variable(value, trainable=True)
高斯分布初始化方法：random_normal(shape)
神经网络模型抽象方法：Cell

# 定义卷积层
class Conv2D(Cell):
    def __init__(self, in_channels, out_channels, ksize, stride, padding):
        # 卷积核大小为 ksize x ksize x inchannels x out_channels
        filters_shape = (out_channels, in_channels, ksize, ksize)
        self.stride = stride
        self.padding = padding
        self.filters = Variable(random_normal(filters_shape))

    def __call__(self, inputs):
        outputs = convolution(inputs, self.filters, self.stride, self.padding)

# 在初始化Conv2D时，__setattr__会判断属性，属于Cell把神经网络层Conv2D记录到self._cells，filters属于parameter把参数记录到self._params
conv = Conv2D(in_channel=10, out_channel=20, filter_size=3, stride=2, padding=0)
# 查看神经网络层参数使用conv.parameters_and_names；查看神经网络层列表使用conv.cells_and_names；执行操作使用conv(input)
output = conv(input)
```

#### 自定义神经网络模型
- 神经网络模型也可以采用SubClass的方法自定义神经网络模型
- 构建时需要在__init__里将要使用的神经网络组件实例化，在__call__里定义神经网络的计算逻辑
```python
# 使用Cell子类构建的神经网络层接口定义：
# 构建卷积神经网络的组件接口定义：
全连接层接口：Dense(in_channel, out_channel)
卷积层的接口：Conv2D(in_channel, out_channel, filter_size, stride, padding)
最大池化接口：MaxPool2D(pool_size, stride, padding)
张量平铺：Flatten()

# 使用SubClass方式构建卷积模型
class CNN(Cell):
    def __init__(self):
        self.conv1 = Conv2D(in_channel=3, out_channel=16, filter_size=3, stride=1, padding=0)
        self.maxpool1 = MaxPool2D(pool_size=3, stride=1, padding=0)
        self.conv2 = Conv2D(in_channel=16, out_channel=32, filter_size=3, stride=1, padding=0)
        self.maxpool2 = MaxPool2D(pool_size=3, stride=1, padding=0)
        self.flatten = Flatten()
        self.dense1 = Dense(in_channels=768, out_channel=128)
        self.dense2 = Dense(in_channels=128, out_channel=64)
        self.dense3 = Dense(in_channels=64, out_channel=10)

    def __call__(self, inputs):
        z = self.conv1(inputs)
        z = self.maxpool1(z)
        z = self.conv2(z)
        z = self.maxpool2(z)
        z = self.flatten(z)
        z = self.dense1(z)
        z = self.dense2(z)
        z = self.dense3(z)
        return z
net = CNN()
```


### C/C++编程接口
- 很多时候，用户需要添加自定义的算子来帮助实现新的模型、优化器
、数据处理函数等。这些自定义算子需要通过C和C++实现，从而获得最优性能。但是为了帮助这些算子被用户使用，他们也需要暴露为Python函数，从而方便用户整合入已有的Python为核心编写的工作流和模型

#### 在Python中调用C/C++函数的原理
- 现代机器学习框架（包括TensorFlow，PyTorch和MindSpore）主要依赖**Pybind11**来将底层的大量C和C++函数自动生成对应的Python函数，这一过程一般被称为Python绑定（Binding）
- 在Pybind11出现以前，将C和C++函数进行Python绑定的手段主要包括：
    - Python的C-API。这种方式要求在一个C++程序中包含Python.h，并使用Python的C-API对Python语言进行操作。使用这套API需要对Python的底层实现有一定了解，比如如何管理引用计数等，具有较高的使用门槛。
        - https://blog.csdn.net/fitzzhang/article/details/79212411
        - https://www.bilibili.com/video/BV1GS4y1E7Az/?spm_id_from=333.999.0.0&vd_source=7798c62f92ce545f56fd00d4daf55e26
            - https://github.com/ttttLarva/cpython-extension-example
    - 简单包装界面产生器（Simplified Wrapper and Interface Generator，SWIG)。SWIG可以将C和C++代码暴露给Python。SWIG是TensorFlow早期使用的方式。这种方式需要用户编写一个复杂的SWIG接口声明文件，并使用SWIG自动生成使用Python C-API的C代码。自动生成的代码可读性很低，因此具有很大代码维护开销。
    - Python的ctypes模块，提供了C语言中的类型，以及直接调用动态链接库的能力。缺点是依赖于C的原生的类型，对自定义类型支持不好。
    - Cython是结合了Python和C语言的一种语言，可以简单的认为就是给Python加上了静态类型后的语法，使用者可以维持大部分的Python语法。Cython编写的函数会被自动转译为C和C++代码，因此在Cython中可以插入对于C/C++函数的调用。
    - Boost::Python是一个C++库。它可以将C++函数暴露为Python函数。其原理和Python C-API类似，但是使用方法更简单。然而，由于引入了Boost库，因此有沉重的第三方依赖。
- 相对于上述的提供Python绑定的手段，Pybind11提供了类似于Boost::Python的简洁性和易用性，但是其通过专注支持C++ 11，并且去除Boost依赖，因此成为了轻量级的Python库，从而特别适合在一个复杂的C++项目中暴露大量的Python函数。

#### 添加C++编写的自定义算子
- 算子是构建神经网络的基础，在前面也称为低级API；通过算子的封装可以实现各类神经网络层，当开发神经网络层遇到内置算子无法满足时，可以通过自定义算子来实现
- 以MindSpore为例，实现一个GPU算子需要如下步骤：
    - Primitive注册：算子原语是构建网络模型的基础单元，用户可以直接或者间接调用算子原语搭建一个神经网络模型。
    - GPU Kernel实现：GPU Kernel用于调用GPU实现加速计算。 
    - GPU Kernel注册：算子注册用于将GPU Kernel及必要信息注册给框架，由框架完成对GPU Kernel的调用。
1. 注册算子原语
- 算子原语通常包括算子名、算子输入、算子属性（初始化时需要填的参数，如卷积的stride、padding）、输入数据合法性校验、输出数据类型推导和维度推导
- 假设需要编写加法算子，主要内容如下：
    - 算子名：TensorAdd
    - 算子属性：构造函数__init__中初始化属性，因加法没有属性，因此__init__不需要额外输入
    - 算子输入输出及合法性校验：infer_shape方法中约束两个输入维度必须相同，输出的维度和输入维度相同。infer_dtype方法中约束两个输入数据必须是float32类型，输出的数据类型和输入数据类型相同
    - 算子输出
```python
# mindspore/ops/operations/math_ops.py
class TensorAdd(PrimitiveWithInfer):
    """
    Adds two input tensors element-wise.
    """
    @prim_attr_register
    def __init__(self):
        self.init_prim_io_names(inputs=['x1', 'x2'], outputs=['y'])

    def infer_shape(self, x1_shape, x2_shape):
        validator.check_integer('input dims', len(x1_shape), len(x2_shape), Rel.EQ, self.name)
        for i in range(len(x1_shape)):
            validator.check_integer('input_shape', x1_shape[i], x2_shape[i], Rel.EQ, self.name)
        return x1_shape

    def infer_dtype(self, x1_dtype, x2_type):
        validator.check_tensor_type_same({'x1_dtype': x1_dtype}, [mstype.float32], self.name)
        validator.check_tensor_type_same({'x2_dtype': x2_dtype}, [mstype.float32], self.name)
        return x1_dtype
```
- 在mindspore/ops/operations/math_ops.py文件内注册加法算子原语后，需要在mindspore/ops/operations/__init__中导出，方便python导入模块时候调用。

```python
# mindspore/ops/operations/__init__.py
from .math_ops import (Abs, ACos, ..., TensorAdd)
__all__ = [
  'ReverseSequence',
  'CropAndResize',
  ...,
  'TensorAdd'
]
```

2. GPU算子开发
- 继承GPUKernel，实现加法使用类模板定义TensorAddGpuKernel，需要实现以下方法：
    - Init(): 用于完成GPU Kernel的初始化，通常包括记录算子输入/输出维度，完成Launch前的准备工作；因此在此记录Tensor元素个数。
    - GetInputSizeList():向框架反馈输入Tensor需要占用的显存字节数；返回了输入Tensor需要占用的字节数，TensorAdd有两个Input，每个Input占用字节数为element_numsizeof(T)。
    - GetOutputSizeList():向框架反馈输出Tensor需要占用的显存字节数；返回了输出Tensor需要占用的字节数，TensorAdd有一个output，占用element_numsizeof(T)字节。
    - GetWorkspaceSizeList():向框架反馈Workspace字节数，Workspace是用于计算过程中存放临时数据的空间；由于TensorAdd不需要Workspace，因此GetWorkspaceSizeList()返回空的std::vector<size_t>。
    - Launch(): 通常调用CUDA kernel(CUDA kernel是基于Nvidia GPU的并行计算架构开发的核函数)，或者cuDNN接口等方式，完成算子在GPU上加速；Launch()接收input、output在显存的地址，接着调用TensorAdd完成加速。

```C
// mindspore/ccsrc/backend/kernel_compiler/gpu/math/tensor_add_v2_gpu_kernel.h

template <typename T>
class TensorAddGpuKernel : public GpuKernel {
 public:
  TensorAddGpuKernel() : element_num_(1) {}
  ~TensorAddGpuKernel() override = default;

  bool Init(const CNodePtr &kernel_node) override {
    auto shape = AnfAlgo::GetPrevNodeOutputInferShape(kernel_node, 0);
    for (size_t i = 0; i < shape.size(); i++) {
      element_num_ *= shape[i];
    }
    InitSizeLists();
    return true;
  }

  const std::vector<size_t> &GetInputSizeList() const override { return input_size_list_; }
  const std::vector<size_t> &GetOutputSizeList() const override { return output_size_list_; }
  const std::vector<size_t> &GetWorkspaceSizeList() const override { return workspace_size_list_; }

  bool Launch(const std::vector<AddressPtr> &inputs, const std::vector<AddressPtr> &,
              const std::vector<AddressPtr> &outputs, void *stream_ptr) override {
    T *x1 = GetDeviceAddress<T>(inputs, 0);
    T *x2 = GetDeviceAddress<T>(inputs, 1);
    T *y = GetDeviceAddress<T>(outputs, 0);

    TensorAdd(element_num_, x1, x2, y, reinterpret_cast<cudaStream_t>(stream_ptr));
    return true;
  }

 protected:
  void InitSizeLists() override {
    input_size_list_.push_back(element_num_ * sizeof(T));
    input_size_list_.push_back(element_num_ * sizeof(T));
    output_size_list_.push_back(element_num_ * sizeof(T));
  }

 private:
  size_t element_num_;
  std::vector<size_t> input_size_list_;
  std::vector<size_t> output_size_list_;
  std::vector<size_t> workspace_size_list_;
};
```
- 这一步会调用CUDA

3. GPU算子注册
- 算子信息包含
    - Primive；
    - Input dtype, output dtype；
    - GPU Kernel class； 
    - CUDA内置数据类型。
- 框架会根据Primive和Input dtype, output dtype，调用以CUDA内置数据类型实例化GPU Kernel class模板类。




## 参考资料
- https://openmlsys.github.io/
- https://zhuanlan.zhihu.com/p/484878657