---
layout:     post
title:      图表示学习（Graph Embedding）之二
subtitle:   经典算法介绍
date:       2019-10-28
author:     bjmsong
header-img: img/Graph/kg.jpg
catalog: true
tags:
    - 图算法
---
>本文将介绍图表示学习的几种经典算法，包括DeepWalk、LINE、GNN等。

<ul> 
<li markdown="1"> 
这幅图展示了图表示学习这个领域算法的发展脉络
![]({{site.baseurl}}/img/Graph/network embedding/study.jpg) 
</li> 
</ul> 

- 大体上可以将图表示方法分为三大类
  - 基于因子分解的方法
  - 基于随机游走的方法
  - 基于深度学习的方法。

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/Graph/network embedding/图表示学习方法分类.jpg) 
</li> 
</ul> 


本文介绍其中的几种算法。

### DeepWalk
<ul> 
<li markdown="1"> 
首先来介绍2014年提出来的DeepWalk，其思想源于自然语言处理领域的word2vec。这两件事情确实是有很多相似的地方，在图中经过一定距离的随机游走，所经过的节点是满足幂律（或者说长尾）分布的，如左图所示。同样，在自然语言处理中，词的分布也满足幂律分布，如右图所示。
![]({{site.baseurl}}/img/Graph/network embedding/law distribution.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
word2vec通过语料构建语言模型，得到词的向量化表示。
![]({{site.baseurl}}/img/Graph/network embedding/w2v.jpg) 
</li> 
</ul> 


<ul> 
<li markdown="1"> 
我们知道，wordvec训练需要语料，那么DeepWalk的语料从何而来呢？答案是：随机游走
DeepWalk分为两个阶段，第一个阶段，在图上进行固定距离的随机游走，通过这种方式采样，得到节点序列。
![]({{site.baseurl}}/img/Graph/network embedding/deepwalk.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
第二个阶段，借鉴word2vec里的SkipGram算法，计算每一个节点上下文的似然概率，极大化似然概率，最后得到节点的向量化表示。
![]({{site.baseurl}}/img/Graph/network embedding/Representation Mapping.jpg) 
</li> 
</ul> 



### LINE

第二个要介绍的算法是LINE，全名叫做Large-scale Information Network Embedding。顾名思义，这个算法可以应用在大规模的数据场景中。


#### LINE的优点


<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/Graph/network embedding/LINE优点.jpg) 
</li> 
</ul> 


#### LINE的原理

<ul> 
<li markdown="1"> 
对于由边（u，v）连接的每一对顶点，边上的权重wuv表示u和v之间的相似度，如果在u和v之间没有观察到边，则它们的一阶相似度为0。一阶邻近通常意味着现实世界网络中两个节点的相似性。例如，在社交网络中相互交友的人往往有着相似的兴趣;在万维网上相互链接的页面倾向于谈论类似的话题
![]({{site.baseurl}}/img/Graph/network embedding/一阶相似度.jpg) 
</li> 
</ul> 


<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/Graph/network embedding/二阶相似度.jpg) 
</li> 
</ul> 


<ul> 
<li markdown="1"> 
LINE设计了同时保留一阶相似度和二阶相似度的目标函数
![]({{site.baseurl}}/img/Graph/network embedding/如何保留一阶相似度.jpg) 
</li> 
</ul> 



### GNN（图神经网络）
<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/Graph/network embedding/whygnn1.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/Graph/network embedding/whygnn2.jpg) 
</li> 
</ul> 


<ul> 
<li markdown="1"> 
最近几年，图神经网络是一个比较热门的研究领域。下面这张图是近些年GNN论文发表数量的情况，我们可以看到从2016年开始，GNN论文发表数量呈指数上升趋势。
![]({{site.baseurl}}/img/Graph/network embedding/gnn paper.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/Graph/network embedding/gnn1.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/Graph/network embedding/gnn2.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/Graph/network embedding/gnn3.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/Graph/network embedding/gnn4.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/Graph/network embedding/gnn5.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/Graph/network embedding/gnn6.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/Graph/network embedding/gnn7.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/Graph/network embedding/gnn8.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/Graph/network embedding/gnn9.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/Graph/network embedding/gnn10.jpg) 
</li> 
</ul> 


### 参考资料
- Tutorial “Graph representation learning” by William L. Hamilton and me has been accepted by AAAI’19

   https://jian-tang.com/

- CNCC报告 唐杰 
  https://www.aminer.cn/cncc19-classicml

- 《DeepWalk: Online Learning of Social Representations》

- LINE 
  《LINE: Large-scale Information Network Embedding》
  https://www.jianshu.com/p/8bb4cd0df840

- GNN
《The graph neural network model》
https://blog.csdn.net/u011748542/article/details/86289511
https://towardsdatascience.com/graph-neural-networks-20d0f8da7df6
https://zhuanlan.zhihu.com/p/75307407

- 《deep learning for learning graph representation》

- https://github.com/talorwu/Graph-Neural-Network-Review




