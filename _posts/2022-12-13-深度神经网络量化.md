---
layout:     post
title:      深度神经网络量化
subtitle:   
date:       2022-12-13
author:     bjmsong
header-img: img/ai.jpg
catalog: true
tags:
    - 模型推理
---

## 什么是模型量化

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/1.png) 
</li> 
</ul> 



## 模型量化的优点

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/2.png) 
</li> 
</ul> 



## 量化技术落地的三大挑战
<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/3.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/4.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/5.png) 
</li> 
</ul> 



## 量化原理

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/6.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/7.png) 
</li> 
</ul> 

浮点与定点数据的转化公式如下：

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/8.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/9.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/10.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/11.png) 
</li> 
</ul> 



## 矩阵运算的量化

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/13.png) 
</li> 
</ul> 



## `Quantize`，`Dequantize`，`Requantize`算子

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/14.png) 
</li> 
</ul> 

假设模型已经量化好，即模型算子已经转换为`int`类型算子（如下图的`Conv2D`）。在模型推理阶段，需要在计算图中插入`Quantize`，`Dequantize`，`Requantize`3种算子。

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/15.png) 
</li> 
</ul> 

3种算子的计算过程如下，其中的`scale`，`offset`是在推理前已经计算好的。

<ul> 
<li markdown="1">
float -> int
![]({{site.baseurl}}/img/compress/quantization/16.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
int -> float
![]({{site.baseurl}}/img/compress/quantization/17.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
int32 -> int8
![]({{site.baseurl}}/img/compress/quantization/18.png) 
</li> 
</ul> 



## 动态训练后量化(Post-Training Quantization Dynamic, PTQ Dynamic)

**不需要重新进行训练或者是标签数据，仅将模型中特定算子的权重从FP32类型映射成INT8/16类型。**因此这是一种轻量化的量化方法。在大多数情况下PTQ Dynamic使用8bit量化时可以接近浮点模型的精度。

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/19.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/20.png) 
</li> 
</ul> 




## 量化感知训练(Quantization Aware Training, QAT)

**让模型感知量化运算对模型精度带来的影响，通过finetune训练降低量化误差。**

QAT算法流程：

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/21.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/22.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/23.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/24.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/25.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/26.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/27.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/28.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/29.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/30.png) 
</li> 
</ul> 




## 静态训练后量化(Post-Training Quantization Static, PTQ Static)

使用少量无标签较准数据，采用KL散度等方法计算量化比例因子。

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/31.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/32.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/33.png) 
</li> 
</ul> 



## 量化方法比较

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/compress/quantization/34.png) 
</li> 
</ul> 

