---
layout:     post
title:      深度模型加速
subtitle:   
date:       2022-07-25
author:     bjmsong
header-img: 
catalog: true
tags:
    - 深度模型加速
---
>模型压缩和加速是两个不同的话题，有时候压缩并不一定能带来加速的效果，有时候又是相辅相成的。压缩重点在于减少网络参数量，加速则侧重在降低计算复杂度、提升并行能力等。

## 背景
- AI模型越来越复杂，训练和预测需要大量算力，大量时间
- 很多场景下算力、存储、电量等有限，比如移动端AI落地
- 目标：模型尺寸小、计算复杂度低、电池耗电量低、下发更新部署灵活


## 算法层压缩加速


## 框架层加速
- 移动端AI框架: 谷歌的tf-lite，腾讯的NCNN，阿里的MNN，百度的PaddleLite, 小米的MACE
https://easyai.tech/blog/10-mobil-deeplearning-frame/
https://www.cnblogs.com/LXP-Never/p/14840535.html
- 端侧AI框架加速优化方法
    - 编译优化、缓存优化、多线程、稀疏存储和计算、NEON指令应用、算子优化等
- 系统优化: 指在特定系统平台上，通过Runtime层面性能优化，以提升AI模型的计算效率
    - Op-level的算子优化：FFT Conv2d (7x7, 9x9), Winograd Conv2d (3x3, 5x5) 等；
    - Layer-level的快速算法：Sparse-block net [1] 等；
    - Graph-level的图优化：BN fold、Constant fold、Op fusion和计算图等价变换等；
    - 优化工具与库（手工库、自动编译）：TensorRT (Nvidia), MNN (Alibaba), TVM (Tensor Virtual Machine), Tensor Comprehension (Facebook) 和OpenVINO (Intel) 等；


### ONNX
作为开放神经网络交换标准，对不同框架和不同的移动端、边缘设备 Runtime 进行了标准化，从而降低了模型在不同框架和不同 Runtime 之间转换的成本，现在主流的框架和设备都支持ONNX
https://github.com/onnx/onnx


## 硬件层加速,异构计算
- 有GPU、FPGA、ASIC等多种方案，各种TPU、NPU就是ASIC这种方案，通过专门为深度学习进行芯片定制，大大加速模型运行速度


## Tensorflow加速技巧: 《Hands on ml2》Chapter 19
- 业界实践
https://zhuanlan.zhihu.com/p/430383324
- 默认使用全部CPU
https://stackoverflow.com/questions/42845261/does-tensorflow-job-use-multiple-cores-by-default
https://stackoverflow.com/questions/38836269/does-tensorflow-view-all-cpus-of-one-machine-as-one-device
- 使用GPU
- 模型并行
    - 比较难
    - https://www.tensorflow.org/tutorials/distribute/keras?hl=zh-cn
    - https://medium.com/analytics-vidhya/speeding-up-inference-using-parallel-model-runs-d76dcf393567   
- 分布式训练
https://zhuanlan.zhihu.com/p/469541810
https://zhuanlan.zhihu.com/p/56991108
https://www.tensorflow.org/guide/distributed_training?hl=zh-cn
https://github.com/yahoo/TensorFlowOnSpark
https://towardsdatascience.com/distribute-your-pytorch-model-in-less-than-20-lines-of-code-61a786e6e7b0
https://blog.csdn.net/weixin_39589455/article/details/120759372

## 上线tensorflow
- 方法二：
    - spark处理数据，生成文件（可以拉到本地，也可以存在云服务器端）
    - CPU/GPU 单机/多机训练模型（venus组件），生成模型文件
    - CPU/GPU 单机/多机预测
    - 预测结果上传到集群
- 方法一：参考train_deep_model_with_data_cluster.py
    - 流程
        - pyspark 读取训练数据，collect到driver端
        - train model
        - pyspark 读取预测数据，collect到driver端
        - predict
        - 结果输出到hive表
    - 缺点
        - 适合于数据量较少，单机可以运行的情况
        - spark部署在CPU集群上，无法使用gpu进行深度学习模型训练
        - 数据和模型耦合，想要调试模型，必须重跑数据，浪费时间。适合于调试完之后最终版本


## 其它
- 读取训练文件
    https://tech.meituan.com/2018/06/07/searchads-dnn.html
    https://www.cnblogs.com/wj-1314/p/11211333.html
    https://www.tensorflow.org/tutorials/load_data/csv?hl=zh-cn
    https://www.tensorflow.org/tutorials/load_data/tfrecord?hl=zh-cn
    - spark输出训练文件（TensorFlow Record格式） 
        https://zhuanlan.zhihu.com/p/352025069
        https://cloud.tencent.com/developer/news/639997  
        - 归一化这步local来完成 
    - 数据预读：用多进程的方式，将HDFS上预处理好的数据拉取到本地磁盘（使用joblib库+shell将HDFS数据用多进程的方式拉取到本地，基本可以打满节点带宽2.4GB/s，所以，拉取数据也可以在10分钟内完成）
    - 程序通过TensorFlow提供的TFrecordReader的方式读取本地磁盘上的数据，这部分的性能提升是最为明显的。原有的程序处理数据的性能大概是1000条/秒，而通过TFrecordReader读取数据并且处理，性能大概是18000条/秒，性能大概提升了18倍
- tensorflow serving
    - 存在哪些问题 




## 参考资料
- https://towardsdatascience.com/how-to-reduce-the-training-time-of-your-neural-network-from-hours-to-minutes-fe7533a3eec5
- https://towardsdatascience.com/the-deep-learning-inference-acceleration-blog-series-part-1-introduction-668e39b8b14b
- https://blog.csdn.net/nature553863/article/details/81083955
- https://github.com/scutan90/DeepLearning-500-questions/blob/master/ch17_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/%E7%AC%AC%E5%8D%81%E4%B8%83%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2.md
- https://mp.weixin.qq.com/s/VdmukMxAFTJYQmqXP7rrcg
- https://blog.csdn.net/zeusee/article/details/89601634
- https://www.cnblogs.com/LXP-Never/p/14833772.html