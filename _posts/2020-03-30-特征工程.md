---
layout:     post
title:      特征工程
subtitle:   
date:       2020-03-30
author:     bjmsong
header-img: img/machineLearning/machineLearning.png
catalog: true
tags:
    - 机器学习

---

> 特征没做好，参数调到老
>
> 特征工程：简而言之，就是用最好的方法来表示数据的艺术/科学。
> 好的特征工程是专业知识，直觉和基础的数学能力的优雅组合。“最好的”是什么意思？大体上，提供给算法的数据的方式，应该以最有效的方式表示潜在信息的相关结构/属性。当你进行特征工程时，你是在把你的数据属性转化为数据特征。

### 构建特征

- **重要的事情说三遍：基于业务！基于业务！基于业务！不要拍脑袋**
- 可以尝试先用规则去解决这个问题，在这个过程中发现这个项目目标实际影响的因素，作为之后建模参考的变量
- 变量不是越多越好，以下几类变量要警惕加入模型：
  - 缺失值太多的
  - 数据源准确性不佳的
  - 取数过程工作量极大的
  - 与其它变量有较大相关性的



### 做探索性数据分析（EDA）

- **建模前，先分析数据，如果觉得人也没法从这个数据中得到想要的结果，不能指望机器能做到**

- **探索性数据分析（EDA）目的是最大化对数据的直觉，完成这个事情的方法只能是结合统计学的图形以各种形式展现出来。**

- 通过EDA可以实现
  ❏	得到数据的直观表现 
  ❏	发现潜在的结构 
  ❏	提取重要的变量 
  ❏	处理异常值 
  ❏	检验统计假设 
  ❏	建立初步模型 
  ❏	决定最优因子的设置

- 探索性数据分析常用的一些常见问题

  1. 数据的典型值是多少(均值，中位数等)？

  2. 变量的缺失性怎么样？

  3. 一组数据的良好分布拟合是什么？

  4. 一个因子是否有影响？

  5. 最重要的因素是什么？

  6. 我们可以将时间相关数据中的信号与噪声分离吗？

  7. 我们可以从多变量数据中提取任何结构吗？

  8. 数据是否有离群值？ 如t-sne可以实现对高维数据集降维可视化



### 数据清洗

- **数据（变量，目标值）的正确性无比关键，脏数据训练的结果就是garbage in，garbage out**

- 缺失值处理
	- “是否缺失”作为一个变量
	- **根据业务经验填充，重要变量尽量填充，避免做决策树模型时因为该值null，分到不合理的节点**
	- 模型预测缺失值
		- sknn：最近k个邻（3,5），距离平方的倒数为权，从缺失最少的开始，填充数据可以用于后续计算
		- svd：寻找与数据X同大小填补矩阵Y，最小化Φ=‖X-eTμ-Y‖，SVD迭代求解
		- EM：选取初始簇中心，E步（贝叶斯最大后验簇分配），M步（重新分配簇）
- 异常值处理
	- 基于统计的异常点检测算法
		- 3sigma：与平均值超过3倍标准差，数据需满足正态分布
			- 箱线图：大于QU+1.5IQR,小于QL−1.5IQR.QU:上四分位数,QL:下四分位数,IQR:QU-QL.
	- 基于距离的异常点检测算法
		- 主要通过距离方法来检测异常点，将数据集中与大多数点之间距离大于某个阈值的点视为异常点，主要使用的距离度量方法有绝对距离 ( 曼哈顿距离 ) 、欧氏距离和马氏距离等方法。
	- 基于密度的异常点检测算法
		- 考察当前点周围密度，可以发现局部异常点，例如LOF算法
	- 异常检测算法（孤立森林，多元高斯分布）
	- 异常值的判断和处理
	  - 类别型异常：稀有事件（<1%）
	  - 连续型：3个标准差之外
	  - 需要结合业务逻辑：是真异常，还是另类？是否代表一类有特殊需求的用户？
	  - 删除异常值有利于模型的稳定
	  - 异常值也可以是一个新的业务方向，比如新用户类型、恶意欺诈用户



#### 特征筛选

- 意义
   - 少而精的自变量有助于提高模型的稳定性
   - 有效自变量可以提高模型预测能力
   - 提高运算速度和效率

 - 基础筛选

   - 去除常量
   - 去除缺失率高的变量
   - 取值过泛的变量，比如邮编

 - 结合业务经验的筛选

   - 让业务专家来讨论变量的重要性，但需要用数据再验证
    统计方法筛选
   
 -   RF/xgboost等树模型
   
 - L1正则
   
 - hashing：适用于离散变量
   
 - 不推荐以下方法
   
     - PCA不适合用来防止过拟合，适用于数据降维（为了减少存储开销等），可视化
       - 求自变量和因变量的相关性：也许自变量和因变量有非线性的关系，仅根据相关性不能作为筛选依据 
       - sklearn.RFECV：计算量大
    
 - 自变量之间
   
      - 相关性检验，检验出相关系数>0.6的共线性多余自变量
      - 这种方法不能排除非线性相关
      
 - 自变量与因变量  
      - 相关性过低的（对于连续型）
      - 卡方检验不显著（对于离散型）
      - 信息价值IV（information value）过低
      - 基尼系数（Gini Score）过低
      - 这些检验可能表示自变量不重要，但不能排除交互作用
      
      
      
       

### 特征衍生

- 特征自动生成：`featuretools`

- 基于原有特征构造
  - 单变量：x^2、e^x、偏度较大的特征：log、Box－Cox变换
  - 多变量：线性组合、非线性组合、x/y、x*y
  - 核函数
- XGBOOST叶节点
- 连续值离散化：df.qcut,或者按照业务经验分段
- 连续值排序：df.rank
- 按时间偏移：对于部分时间序列变量存在“时滞”效应
- K-means对样本聚类，以所属类别的哑编码为新特征
- Auto-encoder
  - GBDT算法的特性，在面对高维稀疏矩阵的时候效果并不好。因为为了控制过拟合，GBDT通常会限制叶子节点的样本数目，稀疏矩阵经过少量分支就已经到达了减枝条件，也就无法体现深度挖掘组合关系的特点。
  - 采用autoencoder在没有人工干预的情况下自动将稀疏特征编码
  - 通过一个深度学习网络结构，用特征经过一系列网络结构预测自己。这样的模型设定通过设定最优化目标是是否可以通过自己预测自己，保证在降低数据噪音、降低数据维度的同时，最大限度的保留原有数据的信息量。
- 定量特征分桶
  ■	之后不需要再做one-hot-encoder
  ■	其同一区间的数字能够代表相同的特征
  ■	减少过拟合
- 利用特征哈希，把大量的稀疏特征映射到固定维度的空间，对准确率有影响，但提高模型效率



### 其它

- [IV & WOE](https://blog.csdn.net/kevin7658/article/details/50780391)	
- id类特征 
  - 编码：Hash成64位：前16位空置，中间12位为Slot_id,最后36位为FeatureID（即原ID的二进制表示）
  - embedding处理吗
- 标准化：可以让梯度下降更快收敛
  - 树模型不需要
- [卡方分箱](https://www.cnblogs.com/wqbin/p/10547167.html)
- 抽样
  - 训练集样本至少1000，保证模型的稳定性
  - 验证集可以是训练集的1/10~1/2
  - 训练集必须是自变量数的10倍以上
  - 目标事件必须是自变量数的6~8倍以上
  - 自变量数一般在8~20个，过多、过少都不一定好
- 类别变量的处理
  https://mp.weixin.qq.com/s?__biz=Mzg5ODAzMTkyMg==&mid=2247486758&idx=1&sn=1a5b2558511fc52621300ca45f27356c&chksm=c069857bf71e0c6d866b6b8cbef4fed0225f1e4acc3208fd44fbef737a9901704f4b3a89306e&mpshare=1&scene=1&srcid=&sharer_sharetime=1566085108359&sharer_shareid=602b1ccf63ca4ea52755ecd058f6d407&key=8a4c04f4ab18b288a2b1e2103adeeb4e7952007b30884c7484c137968fd745d35a8a4b501869275fe676f69010d14f457a372ef4786f5df8b08b9c956eb27d54b931cd34464b8ba75546b8af6ec2e9e7&ascene=1&uin=MjM1OTMwMzkwMA%3D%3D&devicetype=Windows+10&version=62060833&lang=en&pass_ticket=IUUUfbK16TvO1gSfqeJNL4tVQmZmZcGoL1QKs%2BBeaVEhl7p4xYuuhsscZVM1JWZn





### 参考资料

- https://github.com/Pysamlam/Tips-of-Feature-engineering
- https://mp.weixin.qq.com/s?__biz=MzIzOTA0NDEwNA==&mid=2649610460&idx=1&sn=1f16f445ff6c959911e5c7d0ab825b4c&chksm=f1296bbbc65ee2ad9606913e587e3c627cafd22357ea66e3c1d087b5534c1cd61e23e0133228&mpshare=1&scene=1&srcid=0827fLP8yH0iHgW9CzOJt6Cj&sharer_sharetime=1566867087749&sharer_shareid=49581f7bdbef8664715f595bc62d7044&key=2ff8c7df758d8641e1492532e38fc67e8f006f64dfc765376813b20466a420c21b69006c4dfea5dc23101f793d8d9687b7d8e0bcb8d592ed75dd6920ac3bb0d5970e0f8e7ece45744fbfa30ad273d314&ascene=1&uin=MjM1OTMwMzkwMA%3D%3D&devicetype=Windows+10&version=62060833&lang=en&pass_ticket=UB%2B7CmMbMLz0ys%2FpT6qpodMcirRwudt1ebiYBTxE83SAgLTUWsBNrU4bY6vfNIay
- https://mp.weixin.qq.com/s?__biz=MzI1MzY0MzE4Mg==&mid=2247485232&idx=1&sn=e83004347d6b5c993af3cfa64f533a0c&chksm=e9d017f1dea79ee7f2e399884b90f3e55c33c1c98747fb55f9a4f5257bcce22374465a344f64&scene=0&xtrack=1&key=c86a338f58bd007cdffd48c99e9e2298a13ebf736243b5d96f69a9361632ffa5a9388e60759653767b14c83729bcbdc54fb6f30806f1c212ff1fb14b99f43248d0ece5001c9cbc78052f7ba5cda89774&ascene=1&uin=MjM1OTMwMzkwMA%3D%3D&devicetype=Windows+7&version=62060833&lang=zh_CN&pass_ticket=fZx3R6hAbiq5JXQFJM5ZrEYTbNGY4PKgyOa4uD91VEjM4%2FQUCMtw94n3V1bJeCDB
- https://segmentfault.com/a/1190000014799038
- https://blog.csdn.net/jingyi130705008/article/details/82670011?utm_source=blogxgwz9
- https://www.cnblogs.com/magle/articles/6110195.html