---
layout:     post
title:      加速你的python代码(一)
subtitle:   
date:       2019-10-28
author:     bjmsong
header-img: img/python.jpg
catalog: true
tags:
    - python
---
>

python是处理数据的利器，然而在面对一些比较大的数据集时，python有时会感觉运行很慢，这大大影响了工作的效率。那么有哪些方法可以提升python的效率呢，下面就来介绍几种方法。

### 向量化
当我们要进行向量间的运算，比如矩阵乘法时。常规的做法是需要用for循环去遍历向量里面的每一个值
```
import numpy as np
import time
# Number of features
n = 1000
# Number of training examples
m = 10000
# Initialize X and W
X = np.random.rand(n,m)
W = np.random.rand(n,1)

# Non Vectorized code
Z1 = np.zeros((1,m))
t2 = time.time()
for i in range(X.shape[1]):
    for j in range(X.shape[0]):
        Z[0][i] += W[j]*X[j][i]
print("Time taken for non vectorized code is : ",(time.time()-t2)*1000,"ms")
```
python的向量化可以帮助我们高效地实现这个过程。
```
# Vectorized code
t1=time.time()
Z = np.dot(W.T,X)
print("Time taken for vectorized code is : ",(time.time()-t1)*1000,"ms")
```

### 拒绝for循环
#### 一个典型的for循环
```
import seaborn as sns
import numpy as np
import pandas as pd
import time

data = sns.load_dataset('iris')

def compute_class(petal_length):
    if petal_length <= 2:
        return 1
    elif 2 < petal_length < 5:
        return 2
    else:
        return 3

# for loop
start = time.time()

class_list = list()
for i in range(len(data)):
    petal_length = data.iloc[i]['petal_length']
    class_num = compute_class(petal_length)
    class_list.append(class_num)

end = time.time()
print("For-loop run time = {}".format(end - start))
```

#### iterrows方法
for循环是非常耗时的，因此需要想办法抛弃掉for循环。可以采用.iterrows()方法，该方法实现了一个生成器函数，它将在每次迭代中生成一行数据，缺点是iterrows()不能保留行之间的dtype。
```
start = time.time()
class_list = list()
for index, data_row in data.iterrows():
    petal_length = data_row['petal_length']
    class_num = compute_class(petal_length)
    class_list.append(class_num)

end = time.time()
print("Iterrows run time = {}".format(end - start))
```

#### apply是更优方案
apply本身并不快，但与DataFrame结合使用时，它具有很大的优势。这取决于apply表达式的内容。如果它可以在Cython中执行，那么apply要快得多。
```
start = time.time()
class_list = data.apply(lambda row: compute_class(row['petal_length']), axis=1)
end = time.time()
print("apply() run time = {}".format(end - start))
```

#### 使用内置的函数来替代循环的语句
```
start = time.time()
class_list = pd.cut(x=data.petal_length, bins=[0, 2, 5, 100], include_lowest=True, labels=[1, 2, 3]).astype(int)
end = time.time()
print("cut() run time = {}".format(end - start))
```

#### pandas向量化
需要改写计算的函数，将series作为输入
```
def compute_class(series):
    data['class'] = 3
    data.loc[series <= 2, 'class'] = 1
    data.loc[(series < 5) & (series > 2), 'class'] = 2

start = time.time()
compute_class(data["petal_length"])
end = time.time()
print("pandas vectorization  run time = {}".format(end - start))
```

#### numpy向量化
Numpy array会更快
```
start = time.time()
compute_class(data["petal_length"].values)
end = time.time()
print("numpy vectorization  run time = {}".format(end - start))
```


### 比apply再快100倍

```
np.random.seed(0)
N = 10**5
A_list = np.random.randint(1, 100, N)
B_list = np.random.randint(1, 100, N)
df = pd.DataFrame({'A': A_list, 'B': B_list})

%timeit list(map(divide, df['A'], df['B']))                                   # 43.9 ms
%timeit np.vectorize(divide)(df['A'], df['B'])                                # 48.1 ms
%timeit [divide(a, b) for a, b in zip(df['A'], df['B'])]                      # 49.4 ms
%timeit [divide(a, b) for a, b in df[['A', 'B']].itertuples(index=False)]     # 112 ms
%timeit df.apply(lambda row: divide(*row), axis=1, raw=True)                  # 760 ms
%timeit df.apply(lambda row: divide(row['A'], row['B']), axis=1)              # 4.83 s
%timeit [divide(row['A'], row['B']) for _, row in df[['A', 'B']].iterrows()]  # 11.6 s
```

- 基于tuple的方法（前四个）要远快于基于Series的方法（后三个）
- apply方法设置raw=True，可以大大改善性能


np.vectorize：可以比上面案例中的for循环提升百倍以上性能
```
start = time.time()
np.vectorize(compute_class)(data["petal_length"])
end = time.time()
print("np.vectorize  run time = {}".format(end - start))
```

### numba：就像坐上了火箭
numba是一个用于编译Python数组和数值计算函数的编译器，这个编译器能够大幅提高直接使用Python编写的函数的运算速度。

numba使用LLVM编译器架构将纯Python代码生成优化过的机器码，通过一些添加简单的注解，将面向数组和使用大量数学的python代码优化到与c，c++和Fortran类似的性能，而无需改变Python的解释器。
```
from numba import njit

# 在jit装饰器装饰的函数中，不可以有第三方的package
@njit
def divide(a, b):
    res = np.empty(a.shape)
    for i in range(len(a)):
        if b[i] != 0:
            res[i] = a[i] / b[i]
        else:
            res[i] = 0
    return res

%timeit divide(df['A'].values, df['B'].values)  # 717 µs
```


### 多线程还是多进程
#### CPU密集 vs IO密集
**CPU密集**：即计算密集型，指的是系统的硬盘、内存性能相对CPU要好很多，此时，系统运作大部分的状况是CPU Loading 100%，CPU要读/写I/O(硬盘/内存)，I/O在很短的时间就可以完成，而CPU还有许多运算要处理，CPU Loading很高。

计算密集型任务的特点是要进行大量的计算，消耗CPU资源，比如计算圆周率、对视频进行高清解码等等，全靠CPU的运算能力。这种计算密集型任务虽然也可以用多任务完成，但是任务越多，花在任务切换的时间就越多，CPU执行任务的效率就越低，所以，要最高效地利用CPU，计算密集型任务同时进行的数量应当等于CPU的核心数。

计算密集型任务由于主要消耗CPU资源，因此，代码运行效率至关重要。Python这样的脚本语言运行效率很低，完全不适合计算密集型任务。对于计算密集型任务，最好用C语言编写。

**IO密集**：系统的CPU性能相对硬盘、内存要好很多，此时，系统运作，大部分的状况是CPU在等I/O (硬盘/内存) 的读/写操作，此时CPU Loading并不高。

涉及到网络、磁盘IO的任务都是IO密集型任务，这类任务的特点是CPU消耗很少，任务的大部分时间都在等待IO操作完成
（因为IO的速度远远低于CPU和内存的速度）。对于IO密集型任务，任务越多，CPU效率越高，但也有一个限度。常见的大部分任务都是IO密集型任务，比如Web应用。
IO密集型任务执行期间，99%的时间都花在IO上，花在CPU上的时间很少，因此，用运行速度极快的C语言替换用Python这样运行速度极低的脚本语言，完全无法提升运行效率。
对于IO密集型任务，最合适的语言就是开发效率最高（代码量最少）的语言，脚本语言是首选，C语言最差。

**总之，计算密集型程序适合C语言多线程，I/O密集型适合脚本语言开发的多线程。**


#### 多进程
一个程序的执行实例就是一个进程。每一个进程提供执行程序所需的所有资源。（进程本质上是资源的集合）

一个进程有一个虚拟的地址空间、可执行的代码、操作系统的接口、安全的上下文（记录启动该进程的用户和权限等等）、唯一的进程ID、环境变量、优先级类、最小和最大的工作空间（内存空间），还要有至少一个线程。

每一个进程启动时都会最先产生一个线程，即主线程。然后主线程会再创建其他的子线程。

进程（processes）的数量取决于任务是cpu密集还是io密集，如果是cpu密集，cpu已经loading已经很高，进程数量应该小于等于cpu core的数量

```
from multiprocessing import Pool
p_pool = Pool(processes=4)
rb_hfre_result = []
for ticker in ticker_list:
	rb_hfre_result.append(p_pool.apply_async(run_unit, args=(ticker, db, freq, cfg_filepath)))
p_pool.close()
p_pool.join()
result = [i.get() for i in rb_hfre_result]
```

#### 多线程
线程是操作系统能够进行运算调度的最小单位。它被包含在进程之中，是进程中的实际运作单位。一条线程指的是进程中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务。
一个线程是一个execution context（执行上下文），即一个cpu执行时所需要的一串指令。

- 线程的工作方式

假设你正在读一本书，没有读完，你想休息一下，但是你想在回来时恢复到当时读的具体进度。有一个方法就是记下页数、行数与字数这三个数值，这些数值就是execution context。
如果你的室友在你休息的时候，使用相同的方法读这本书。你和她只需要这三个数字记下来就可以在交替的时间共同阅读这本书了。
线程的工作方式与此类似。CPU会给你一个在同一时间能够做多个运算的幻觉，实际上它在每个运算上只花了极少的时间，本质上CPU同一时刻只干了一件事。
它能这样做就是因为它有每个运算的execution context。就像你能够和你朋友共享同一本书一样，多任务也能共享同一块CPU。


```
import time, threading
# 假定这是你的银行存款:
balance = 0
lock = threading.Lock()

def change_it(n):
    # 先存后取，结果应该为0:
    global balance
    balance = balance + n
    balance = balance - n

def run_thread(n):
    for i in range(100000):
        # 先要获取锁:
        lock.acquire()
        try:
            # 放心地改吧:
            change_it(n)
        finally:
            # 改完了一定要释放锁:
            lock.release()

t1 = threading.Thread(target=run_thread, args=(5,))
t2 = threading.Thread(target=run_thread, args=(8,))
t1.start()
t2.start()
t1.join()
t2.join()
print(balance)
```


```
import time
import threadpool  
def sayhello(str):
    print "Hello ",str
    time.sleep(2)

name_list =['xiaozi','aa','bb','cc']
start_time = time.time()
pool = threadpool.ThreadPool(10) 
requests = threadpool.makeRequests(sayhello, name_list) 
[pool.putRequest(req) for req in requests] 
pool.wait() 
print '%d second'% (time.time()-start_time)
```

#### 多线程 vs 多进程
多线程和多进程最大的不同在于：

- 多进程中，同一个变量，各自有一份拷贝存在于每个进程中，互不影响
- 多线程中，所有变量都由所有线程共享，所以，任何一个变量都可以被任何一个线程修改。因此，线程之间共享数据最大的危险在于多个线程同时改一个变量，把内容给改乱了。所以需要加锁

- 进程和线程的区别
1. 同一个进程中的线程共享同一内存空间，但是进程之间是独立的。
2. 同一个进程中的所有线程的数据是共享的（进程通讯），进程之间的数据是独立的。
3. 对主线程的修改可能会影响其他线程的行为，但是父进程的修改（除了删除以外）不会影响其他子进程。
4. 线程是一个上下文的执行指令，而进程则是与运算相关的一簇资源。
5. 同一个进程的线程之间可以直接通信，但是进程之间的交流需要借助中间代理来实现。
6. 创建新的线程很容易，但是创建新的进程需要对父进程做一次复制。
7. 一个线程可以操作同一进程的其他线程，但是进程只能操作其子进程。
8. 线程启动速度快，进程启动速度慢（但是两者运行速度没有可比性）。


### 参考资料
- https://stackoverflow.com/questions/52673285/performance-of-pandas-apply-vs-np-vectorize-to-create-new-column-from-existing-c
- https://towardsdatascience.com/how-to-make-your-pandas-loop-71-803-times-faster-805030df4f06
- https://juejin.im/post/5c62bb92e51d45407f057b45


### 且待下回分解
- [Cython](https://towardsdatascience.com/use-cython-to-get-more-than-30x-speedup-on-your-python-code-f6cb337919b6)
- [CuPy](https://towardsdatascience.com/heres-how-to-use-cupy-to-make-numpy-700x-faster-4b920dda1f56) 
- pandas DataFrame数据选取
    - loc：通过index值获取行数据
    - iloc：通过index位置获取行数据
    - ix：结合了前两种的数据获取方式
    - at：类似loc，数据选取更快
    - iat：类似iloc，数据选取更快
- **《python-parallel-programmning-cookbook》**：concurrent.futures。。。。
- https://www.ibm.com/developerworks/cn/linux/l-cn-python-optim/index.html
- https://www.cnblogs.com/xybaby/p/6510941.html
- 。。。