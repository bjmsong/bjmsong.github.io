---
layout:     post
title:      面试题
subtitle:   
date:       2020-04-03
author:     bjmsong
header-img: img/cs/interview.jpg
catalog: true
tags:
    - 面试
---


### Q1：l2正则化可以改善特征共线性的问题 

线性回归：
$$
y = \theta_1x_1+\theta_2x_2+...\theta_nx_n=X\theta+\epsilon
$$
解析解为：
$$
\theta = (X^TX)^{-1}X^Ty
$$
如果存在特征共线性的问题，如：
$$
x_2 = kx_1+b
$$
那么，矩阵`X`将不满秩，导致参数的方程的数量少于参数变量的数量，这样参数就有无穷多个解，很难得到最优的参数解。

如果我们在损失函数上加上L2正则化项
$$
L = ||X\theta-y||^2+||\theta||^2
$$
最小化L，得到解析解
$$
\theta = (X^TX+CI)^{-1}X^Ty
$$
解决了不可逆的问题



共线性检验：方差检验、Pearson系数



### Q2：给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url?
- https://blog.csdn.net/tiankong_/article/details/77234726
- （1）首先我们最常想到的方法是读取文件a，建立哈希表（为什么要建立hash表？因为方便后面的查找），然后再读取文件b，遍历文件b中每个url，对于每个遍历，我们都执行查找hash表的操作，若hash表中搜索到了，则说明两文件共有，存入一个集合。
- （2）但上述方法有一个明显问题，加载一个文件的数据需要50亿*64bytes = 320G远远大于4G内存，何况我们还需要分配哈希表数据结构所使用的空间，所以不可能一次性把文件中所有数据构建一个整体的hash表。
- （3）针对上述问题，我们分治算法的思想。
    - step1：遍历文件a，对每个url求取hash(url)%1000，然后根据所取得的值将url分别存储到1000个小文件(记为a0,a1,...,a999，每个小文件约300M)，为什么是1000？主要根据内存大小和要分治的文件大小来计算，我们就大致可以把320G大小分为1000份，每份大约300M（当然，到底能不能分布尽量均匀，得看hash函数的设计）
    - step2：遍历文件b，采取和a相同的方式将url分别存储到1000个小文件(记为b0,b1,...,b999)（为什么要这样做? **文件a的hash映射和文件b的hash映射函数要保持一致，这样的话相同的url就会保存在对应的小文件中**，比如，如果a中有一个url记录data1被hash到了a99文件中，那么如果b中也有相同url，则一定被hash到了b99中）
    所以现在问题转换成了：找出1000对小文件中每一对相同的url（不对应的小文件不可能有相同的url）
    - step3：因为每个hash大约300M，所以我们再可以采用（1）中的想法

- 换一题：找最相似url？
- 对每一个url进行哈希：4字节（范围是0~2^32-1）
- 用局部哈希敏感算法来构建候选对：即对每一个url的哈希值进行多次哈希处理（行条化策略），相似项会比不相似项更有可能哈希到同一个桶中，将至少有一次哈希到同一个桶中的文档看成是候选对
- 比较候选对之间的url相似度
- 设n为a,b文件中的url数量，若暴力查询，时间复杂度为O（n^2）,以上方法的时间复杂度为O（n）
- 1个字符占1个字节，一串url占64字节，每个文件有10^9行


### Q3. 大文件中返回频数最高的100个词
- https://blog.csdn.net/tiankong_/article/details/77240283?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.channel_param&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.channel_param
- Q:有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M，要求返回频数最高的100个词
- 解答：
    - 通过哈希映射将文件分成很多个小文件，具体操作如下：读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件(记为f0,f1,...,f4999)中，这样每个文件大概是200k左右（每个相同的词一定会被映射到了同一文件中）
        - 如果存在某个词数量极多，所在文件超出了1M，可以再次哈希
    - 对于每个文件fi，都用hash_map做词和出现频率的统计，取出频率大的前100个词（topK问题，建立一个100个节点的最小堆），把这100个词和出现频率再单独存入一个文件
    - 根据上述处理，我们得到了5000个文件，归并（两两文件依次比较）这些文件取出top100




