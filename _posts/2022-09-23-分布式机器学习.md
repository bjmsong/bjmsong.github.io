---
layout:     post
title:      分布式机器学习
subtitle:   
date:       2022-09-23
author:     bjmsong
header-img: 
catalog: true
tags:
    - 并行计算
---
## https://github.com/wangshusen/DeepLearning
- Chapter 9 Parallel Computing

### 9.1 Basics and MapReduce
- 并行梯度下降：数据分布在不同处理器上，分别计算梯度
- 通信方式
    - 共享内存(share memory): 没有办法做到大规模并行
    - 消息传递(message passing)：节点之间通信
- 架构
    - client-server：worker node上存储数据、做计算
    - peer-to-peer：没有server，所有的节点都做计算，每个节点都有几个邻居，邻居之间可以通信 
#### MapReduce
- 架构：client-server
- 通信：message passing
- 适合做大数据处理，但是来做机器学习不高效 
- 步骤
    - 数据平分到所有节点：数据并行
    - broadcast：server把参数广播到所有的worker节点上
    - map：所有的worker计算梯度
    - reduce：worker把计算结果传回给server，server把结果sum起来 
    - 更新参数
    - 迭代以上步骤
- 时间开销
    - computation
    - Communication
        - Communication complexity：how many words are transmitted between server and workers
        - Latency(网络延迟)：
        - Communication time： Communication complexity/bandwith + Latency
    - 同步
        - 所有的worker都必须等最慢的worker

### 9.2 Parameter server and decentralized network
#### Parameter server
- 应用广泛
- client-server
- message passing
- 异步
    - 比同步快很多
    - 不同worker之间效率要差不多，不然收敛会有问题 
    - 《hogwild a lock-free approach to parallelizing stochastic gradient descent》
- Ray： 开源，支持parameter server
    - 《A distributed framework for emerging AI Applicaitons》
    + 可以很方便地将python代码多线程、多机器运行
    + https://www.youtube.com/watch?v=q_aTbb7XeL4&ab_channel=JackofSome 
    + https://www.youtube.com/watch?v=LmROEotKhJA&ab_channel=Databricks
    + https://www.infoq.cn/article/ualtzk5owdb1crvhg7c1
    + https://github.com/ray-project/ray
    + https://zhuanlan.zhihu.com/p/460600694
- 数据并行

#### Decentralized network
- peer-to-peer：没有server，所有节点都是worker
- message passing
- 数据并行
- 同步 or 异步
- 目前不太流行

### 9.3 TensorFlow's mirrored strategy and ring all-reduce
- 六个并行计算策略
    - MirroredStrategy：单机多GPU
        - 同步随机梯度下降
        - 原理：Ring All-Reduce
            - All-Reduce：Every node gets a copy if the result of reduce
                - 实现方法
                    - reduce+broadcast
                    - all-to-all commnuication: 通信量太大
                    - ring all-reduce
                        - 环形网络
                        - 每个节点传递梯度给下一个节点，传两圈所有节点上就有了所有的梯度
                        - 优化：每个节点的梯度分成几个部分，算完一部分就通信（并发通信），同时计算下一部分
    - TPUStrategy
    - MultiWorkerMirroredStrategy
    - CentralStorageStrategy
    - ParameterServerStrategy
    - OneDeviceStrategy

### 9.4 联邦学习
- https://www.zhihu.com/question/315844487/answer/920036089
- 《A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection》


## 分布式深度学习框架的前世今生，从 MapReduce 到 Pathways
https://zhuanlan.zhihu.com/p/533852982?utm_id=0
- 上古时代的分布式机器学习平台：MPI,Hadoop,Spark
- 参数服务器
- Pathways


## Paper
- Scaling Distributed Machine Learning with System and Algorithm Co-design
- LARGE BATCH TRAINING OF CONVOLUTIONAL NETWORKS
- Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes
- Accurate, large minibatch sgd: Training imagenet in 1 hour
    - https://www.zhihu.com/question/60874090/answer/181413785
    - https://blog.csdn.net/Jing_xian/article/details/79982209
- Imagenet training in minutes
- https://zhuanlan.zhihu.com/p/29032307
- https://www.zhihu.com/question/473002888?utm_campaign=Sharon&utm_medium=social&utm_oi=30249563717632&utm_psn=1577219211981664256&utm_source=wechat_session&utm_content=group2_supplementQuestions
- https://www.zhihu.com/question/360728623/answer/939929658?utm_campaign=shareopn&utm_medium=social&utm_oi=30249563717632&utm_psn=1577219055790055424&utm_source=wechat_session

## 《Dive into Big Model Training》
https://github.com/qhliu26/BM-Training
https://zhuanlan.zhihu.com/p/506052424?utm_id=0


## Pathways
- 李沐读paper

## ZeRo
https://zhuanlan.zhihu.com/p/394064174?utm_id=0



## ICML'22大模型技术Tutorial
https://zhuanlan.zhihu.com/p/562741952?utm_campaign=shareopn&utm_medium=social&utm_oi=30949685329920&utm_psn=1556101177766563840&utm_source=wechat_session&s_r=0
https://github.com/alpa-projects/alpa



## DeepSpeed
https://blog.csdn.net/u010751000/article/details/123516433
https://github.com/microsoft/DeepSpeed
https://zhuanlan.zhihu.com/p/414773915

## Jax
https://zhuanlan.zhihu.com/p/468223891
https://www.zhihu.com/question/306496943
https://zhuanlan.zhihu.com/p/536377657


## 参考资料
- 《分布式机器学习》（刘铁岩）
- https://www.zhihu.com/question/24070061/answer/1325421472
- https://zhuanlan.zhihu.com/p/450689346
- https://zhuanlan.zhihu.com/p/466002243
- http://www.cnblogs.com/LeftNotEasy/archive/2010/11/27/1889598.html
