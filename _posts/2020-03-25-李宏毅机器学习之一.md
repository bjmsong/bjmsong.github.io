---
layout:     post
title:      李宏毅机器学习之一
subtitle:   
date:       2020-03-25
author:     bjmsong
header-img: img/machineLearning/machineLearning.png
catalog: true
tags:
    - 机器学习
---

> 入门最佳课程



<ul> 
<li markdown="1"> 
Structured Learning:除分类、回归以外的机器学习任务，如语音识别、机器翻译
![]({{site.baseurl}}/img/machineLearning/机器学习概貌.png) 
</li> 
</ul> 

### Bias & Variance

<ul> 
<li markdown="1"> 
机器学习的误差来源于两个方面：bias，variance。每次采样一批样本做预测，bias是预测的期望值和真实值的差距，variance是单个预测结果和预测的期望值的差距。
![]({{site.baseurl}}/img/machineLearning/bias_variance.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/bias.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/bias_variance2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
诊断误差的来源，才能找到提升模型的方法    
![]({{site.baseurl}}/img/machineLearning/large_bias.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/large_variance.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/cross_validation.png) 
</li> 
</ul> 



### 梯度下降

<ul> 
<li markdown="1"> 
Loss Function要选凸函数：只有全局最优，没有局部最优
![]({{site.baseurl}}/img/machineLearning/梯度下降.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/梯度下降公式.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
做梯度下降要画loss随参数更新的变化趋势图
![]({{site.baseurl}}/img/machineLearning/learning_rate.png) 
</li> 
</ul> 



#### Tips

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/adaptive_learning_rate.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/adagrad.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/adagrad2.png) 
</li> 
</ul> 

- 目前比较好的方法是：`Adam`

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/随机梯度下降.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
通过feature scaling，loss下降更有效率
![]({{site.baseurl}}/img/machineLearning/特征缩放.png) 
</li> 
</ul> 



#### 数学推导

<ul> 
<li markdown="1"> 
初始化点(a,b)，在(a,b)为圆心的一个小圆范围内寻找点，使得loss funcion减小。在(a,b)附近做一阶泰勒展开
![]({{site.baseurl}}/img/machineLearning/梯度下降数学原理.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
loss funcion可以表示成两个向量点积的形式（如图），要使得值最小，两个向量应该相反，且值最大（达到圆的边界）
![]({{site.baseurl}}/img/machineLearning/梯度下降数学原理2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
要保证这个推导成立，泰勒展开要成立，那么圆的范围应该尽量小，也即learning rate尽量小。因此如果learning rate比较大，每一步迭代后，loss不一定下降。
![]({{site.baseurl}}/img/machineLearning/梯度下降数学原理3.png) 
</li> 
</ul> 

- 牛顿法：二阶泰勒展开

<ul> 
<li markdown="1"> 
对于多维变量，鞍点其实不是个问题
![]({{site.baseurl}}/img/machineLearning/梯度下降法的局限.png) 
</li> 
</ul> 



### 分类

<ul> 
<li markdown="1"> 
Probabilistic Generative Model(生成式模型):假设每个类别的样本都服从某一种分布（比如高斯分布），通过最大似然求出分布的参数，然后求出样本属于某个类别的概率。
![]({{site.baseurl}}/img/machineLearning/generative_modle.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
降低模型的复杂度，假设不同类别的分布的协方差矩阵是一样的
![]({{site.baseurl}}/img/machineLearning/极大似然求分布参数.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
假设不同特征来自于独立的分布，就可以得到Naive Bayes分类器。但是这显然是一个比较强的假设，如果数据不满足这个假设，NB的效果会比较差。
![]({{site.baseurl}}/img/machineLearning/naive_bayes.png) 
</li> 
</ul> 



####  Generative Model -> LR !!!

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/推导LR.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
中间省略若干步推导，得到Logistic Regression！因此，其实不需要绕一圈去估计分布的参数，直接求解参数w和b即可
![]({{site.baseurl}}/img/machineLearning/推导LR2.png) 
</li> 
</ul> 



### 逻辑回归

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/逻辑回归步骤一.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
最大化似然函数，可以求得最优的w和b。Loss funcion一般是越小越好，因此在前面加负号，再加log，将乘积变成求和。    
![]({{site.baseurl}}/img/machineLearning/逻辑回归步骤二.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
为了统一表达式，将式子转化成以下的形式。这个式子也可以理解为两个伯努利分布的交叉熵，当两个分布越接近，交叉熵越小。   
![]({{site.baseurl}}/img/machineLearning/逻辑回归步骤二2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
逻辑回归和线性回归，梯度下降的式子是完全一样的！
![]({{site.baseurl}}/img/machineLearning/lr对比.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
如果逻辑回归采用平方误差呢
![]({{site.baseurl}}/img/machineLearning/逻辑回归平方误差.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
均方误差的损失函数较为平坦，在离目标值很远的地方，梯度也很小，这样做梯度下降就很困难。交叉熵损失函数则不同，在离目标值很远的地方，梯度很大
![]({{site.baseurl}}/img/machineLearning/交叉熵vs平方误差.png) 
</li> 
</ul> 



### Deep Learning

