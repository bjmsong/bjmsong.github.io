---
layout:     post
title:      Pytorch入门之一
subtitle:   Tensor及autograd介绍
date:       2019-09-16
author:     bjmsong
header-img: img/dl/pytorch.jpg
catalog: true
tags:
    - 深度学习
---
>基于python的深度学习包，可以支持深度学习算法，GPU加速。

### Tensor
类似NumPy的ndarrays，所不同的是Tensor可以利用GPU加速。
#### 创建Tensor
构建Tensor的方法很多，可以指定Tensor大小
```
x = torch.empty(5, 3)
x = torch.rand(5, 3)
x = torch.zeros(5, 3, dtype=torch.long)
```
也可以从已有数据中构建

```
# list
x = torch.tensor([5.5, 3])
# Tensor
x = x.new_ones(5, 3, dtype=torch.double)
x = torch.randn_like(x, dtype=torch.float)
```

#### 操作Tensor
同一种操作有多种写法，例如加法：
```
y = torch.rand(5, 3)
print(x + y)
print(torch.add(x, y))
# 指定加法结果
result = torch.empty(5, 3)
torch.add(x, y, out=result)
# 原地加
y.add_(x)
```
切片操作和numpy一样
```
print(x[:, 1])
```
改变Tensor的维度
```
x = torch.randn(4, 4)
y = x.view(16)
z = x.view(-1, 8)  # the size -1 is inferred from other dimensions
print(x.size(), y.size(), z.size())
```
Tensor和ndarray的交互非常方便
```
# to Tensor
a = numpy.array([1, 2, 3])
t = torch.from_numpy(a)
# to ndarray
a = torch.ones(5)
b = a.numpy()
```

[Tensor操作API手册](https://pytorch.org/docs/stable/torch.html)

#### CUDA
CUDA（Compute Unified Device Architecture），是显卡厂商NVIDIA推出的运算平台。CUDA™是一种由NVIDIA推出的通用并行计算架构，该架构使GPU能够解决复杂的计算问题。
使用.to方法可以将Tensors移动到任意平台进行运算
```
# let us run this cell only if CUDA is available
# We will use ``torch.device`` objects to move tensors in and out of GPU
if torch.cuda.is_available():
    device = torch.device("cuda")          # a CUDA device object
    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU
    x = x.to(device)                       # or just use strings ``.to("cuda")``
    z = x + y
    print(z)
    print(z.to("cpu", torch.double))       # ``.to`` can also change dtype together!
```

### autograd 
autograd是Pytorch的核心功能，只需要用户定义前向传播过程，Pytorch会自动定义反向传播过程。Tensor和Function一起构建了包含整个计算过程的无环图。
```
x = torch.ones(2, 2, requires_grad=True)  # Pytorch会自动跟踪在这个tensor上的所有操作
print(x.requires_grad) # 查看Tensor的可导属性
y = x + 2         # y也是可导的，因为构建y的叶子变量x是可导的
print(y.grad_fn)  # grad_fn属性就包含了创建这个Tensor的Function信息
z = y * y * 3
out = z.mean()
out.backward() # 触发自动求导的过程
print(x.grad)  # d(out)/dx
x.requires_grad_(False) # 改变Tensor的可导状态

```
如果不想Tensor跟踪操作历史，可以使用.detach()方法，也可以把代码包在with torch.no_grad():中。
```
print(x.requires_grad)
print((x ** 2).requires_grad)

with torch.no_grad():
    print((x ** 2).requires_grad)
```
#### backwrad函数详解
**默认只能是【标量】对【标量】，或者【标量】对向【量/矩阵】求导。**实际场景中一般是损失函数对参数求导，损失函数是标量，而参数一般是向量（单层神经网络）或者矩阵（多层神经网络）。
```
#创建一个多元函数，即Y=XW+b=Y=x1*w1+x2*w2*x3*w3+b，x不可求导，W,b设置可求导
X=torch.tensor([1.5,2.5,3.5],requires_grad=False)
W=torch.tensor([0.2,0.4,0.6],requires_grad=True)
b=torch.tensor(0.1,requires_grad=True)
Y=torch.add(torch.dot(X,W),b)
 
 
#判断每个tensor是否是可以求导的
print(X.requires_grad)
print(W.requires_grad)
print(b.requires_grad)
print(Y.requires_grad)
 
 
#求导，通过backward函数来实现
Y.backward()  
 
#查看导数，也即所谓的梯度
print(W.grad)
print(b.grad)
```


##### 参数详解
- gradient
gradient参数的维度与求导的函数(y)保持一样的形状，每一个元素表示当前这个元素所对应的权重
```
x = torch.tensor([[1.,2.,3.],[4.,5.,6.]],requires_grad=True)
y = torch.add(torch.pow(x,2),x)
gradient=torch.tensor([[1.0,1.0,1.0],[1.0,1.0,1.0]])
y.backward(gradient)
print(x.grad)
```
- retain_graph
在构建函数关系的时候，特别是多个复合函数的时候，会有一个运算图。一个计算图在进行反向求导之后，为了节省内存，这个计算图就销毁了。如果你想再次求导，就会报错。我们可以通过设置retain_graph=True来保留计算图。但是这样会吃内存！，尤其是，你在大量迭代进行参数更新的时候，很快就会内存不足，所以这个参数在绝大部分情况下是不要去使用的。


### 参考资料
- https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py
- https://blog.csdn.net/qq_27825451/article/details/89393332

