---
layout:     post
title:      DLRM
subtitle:   
date:       2022-09-06
author:     bjmsong
header-img: 
catalog: true
tags:
    - 深度模型加速
---
## 官方文档
- https://arxiv.org/abs/1906.00091
- https://ai.facebook.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/
- https://www.youtube.com/watch?v=DFrCEvPgEcQ&ab_channel=OpenComputeProject

## 解读
- 参考
    - https://zhuanlan.zhihu.com/p/82839874
    - https://www.infoq.cn/article/isbHku4p*C4KZ49JIkLy
- 模型中规中矩
    - 离散特征：embedding
    - 连续特征：通过mlp处理成跟离散特征一样的长度
    - 特征交叉：embedding两两做点积，做完之后在跟之前dense features对应的embedding concat起来
- 并行训练
    - Embedding部分采用模型并行
        - 绝大部分参数在Embedding部分
        - 在一个device或者说计算节点上，仅有一部分Embedding层参数，每个device进行并行mini-batch梯度更新时，仅更新自己节点上的部分Embedding层参数
    - MLP和interactions部分采用数据并行
        - 参数少，计算量大
        - 每个device上已经有了全部模型参数，每个device上利用部分数据计算gradient，再利用allreduce的方法汇总所有梯度进行参数更新
    - 自研：PyTorch/caffe2不支持模型并行+数据并行
- Data通过三种渠道收集
    - Random
    - synthetic
    - public data sets
- Experience
    - 训练平台：CPU+GPU

## 源码
- https://github.com/facebookresearch/dlrm

## System Architecture Implication
- https://arxiv.org/abs/1906.03109
### 背景
- 在Facebook，个性化推荐在整个AI Inference处理中占了79%（占Training的大约占50%）。其中三类主要的模型(RMC1, RMC2, and RMC3)占了65%
- 已经有很多的工作，致力于optimizing the performance, en- ergy efficiency, and memory consumption of DNNs
    - efficient DNN architectures：GRU, ResNet, Mobilenets
    - reduced precision datatypes：压缩，量化
    - heavily parallelized training/inference：分布式
    - hardware accelerators
- 但是这些优化大部分针对于CNN/RNN, 无法应用到推荐系统模型上
    - 因为模型差异很大,对运算能力(FLOPS)/内存读取的需求都不一样
- 公开的推荐系统基准和生产环境的推荐系统有所不同
    - 生产环境的推荐系统需要高吞吐、低延迟地处理用户请求
    - 
- 

## 参考资料
