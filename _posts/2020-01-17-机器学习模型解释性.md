---
layout:     post
title:      机器学习模型解释性
subtitle:   
date:       2020-01-17
author:     bjmsong
header-img: img/machineLearning/machineLearning.png
catalog: true
tags:
    - 机器学习

---



## 模型可解释性

- 定义：理解（interpret）表示用可被认知（understandable）的说法去解释（explain）或呈现（present）。在机器学习的场景中，可解释性（interpretability）就表示模型能够使用人类可认知的说法进行解释和呈现
- 意义
    - 建模阶段，辅助开发人员理解模型，进行模型的对比选择，必要时优化调整模型，启发特征工程思路，指导后续数据搜集
    - 投入运行阶段，向业务方解释模型的内部机制，对模型结果进行解释。比如基金推荐模型，需要解释：为何为这个用户推荐某支基金。



## 如何洞悉模型

- 内在可解释性：就是利用机器学习模型，该模型本质上是可解释的

    - 线性模型：特征权重
    - 基于树的模型：如xgboost特征重要性-- importance_type
        - weight（默认）：the number of times a feature is used to split the data across all trees.
        - gain：is the average gain of splits which use the feature. 
        - cover：is the average coverage of splits which use the feature where coverage is defined as the number of samples affected by the split. 使用该特作为分割影响的平均样本数。——可以理解为被分到该节点的样本的二阶导数之和，而特征度量的标准就是平均的coverage值。
        - total_gain: the total gain across all splits the feature is used in
        - total_cover: the total coverage across all splits the feature is used in

- [SHAP ](https://github.com/slundberg/shap) 
  
    - SHAP（SHapley Additive exPlanation）有助于细分预测以显示每个特征的影响。它基于Shapley values，这是一种用于博弈论的技术，用于确定协作游戏中每个玩家促成其成功的贡献有多少。一个特征的shapley value是该特征在所有的特征序列中的平均边际贡献。
    - 优点
      - 解决了多重共线性问题
      - 不仅考虑单个变量的影响，而且考虑变量组的影响，变量之间可能存在协同效应
    - 缺点：计算效率低
    
- [LIME](https://github.com/marcotcr/lime)
  
    - 局部可解释不可知模型（LIME）是一种算法，它提供了一种新颖的技术，以可解释和可信任的方式解释任何预测模型的结果。它的工作原理是围绕想要解释的预测在本地训练可解释的模型
    
    - 流程
    
      - 训练模型，模型（记作 ff）可以是LR、NN、Wide and deep、C4.5 Decision tree、Random forest、GBDT等任意模型。
      
      - 训练结束后我们需要解析模型，先选择一个待解析的样本，样本通过模型计算可以得到一个prediction（包含预测的label以及预测为1的probability），这时我们在这个样本的附近选择新的样本并用模型计算出多个prediction，这样样本组合新的样本集。
      
      - 然后使用新的可解析的特征和prediction作为label来训练新的简单模型（例如LR），然后使用简单模型的权重作为这些特征的重要性作为输出。
      
    - 通俗来说：
    
      就是选择一个样本以及样本附近的点，然后训练一个简单模型来拟合，虽然简单模型不能在完整数据集上有效，但至少在这个点附近都是有效的，这个简单模型的特征是人类可解析的，而训练出的权重也可以表示特征重要性。
      
    - LIME旨在提供局部可解释性，因此对于特定决策或结果最为准确。
    
- Permutation Importance (排列重要性)
  
    - 思想很简单：随机重排或打乱样本中的特定一列数据，其余列保持不变。如果模型的预测准确率显著下降，那就认为这个特征很重要。与之对应，如果重排和打乱这一列特征对模型准确率没有影响的话，那就认为这列对应的特征没有什么作用
    
    ```
    import eli5
    from eli5.sklearn import PermutationImportance
    from sklearn.svm import SVC
    
    # ... load data
    
    svc = SVC().fit(X_train, y_train)
    perm = PermutationImportance(svc).fit(X_test, y_test)
    eli5.show_weights(perm)
    ```
    
- [InterpretML](https://github.com/interpretml/interpret)

- Partial Dependency Plots（部分依赖图）

    - PDP显示特征对机器学习模型的预测结果的边际效应，可以展示一个特征是如何影响预测的。

    - PDP分析步骤如下：
    
      - 训练一个模型（假设F1 … F4是我们的特征，Y是目标变量，假设F1是最重要的特征）。我们有兴趣探索Y和F1的直接关系。
    
      - 用F1（A）代替F1列，并为所有的观察找到新的预测值。采取预测的平均值。（称之为基准值）
    
      - 对F1（B）… F1（E）重复步骤3，即针对特征F1的所有不同值。
    
      - PDP的X轴具有不同的F1值，而Y轴是随该基准值F1值的平均预测而变化。
    
    - 如果使用线性回归或者逻辑回归，那么partial dependence可以被类比为这两类模型中的“系数”。并且partial dependence在复杂模型中的作用比在简单模型中更大，抓出更复杂的特性。
    
    - PDP特别适合用来回答类似这样的问题：
    
      - 在所有的收入水平的特征中，年龄和学历是如何影响收入的？或者说，在不同的国家相同年龄的人群收入水平有多少相似呢？
    
      - 预测推荐基金时，投资偏好的不同会带来多大的影响？还是有其他更重要的影响因素？
    
    



## 深度学习模型的可解释性







## 参考资料

- [腾讯技术工程-机器学习模型可解释性的详尽介绍](https://www.jiqizhixin.com/articles/2019-10-30-9)
- [Hands-on Machine Learning Model Interpretation](https://towardsdatascience.com/explainable-artificial-intelligence-part-3-hands-on-machine-learning-model-interpretation-e8ebe5afc608)
- [A Gudie for Making Black Box Models Explainable](https://christophm.github.io/interpretable-ml-book/)
- [Interpretable Machine Learning](https://www.leiphone.com/news/201907/zp33Hak0P49yNbKY.html)
- https://mathpretty.com/11205.html
- D Gunning, explainable artificial intelligence (xai)
- https://towardsdatascience.com/an-overview-of-model-explainability-in-modern-machine-learning-fc0f22c8c29a
- https://www.kaggle.com/learn/machine-learning-explainability
