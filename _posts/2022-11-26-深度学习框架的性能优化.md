---
layout:     post
title:      深度学习框架的性能优化
subtitle:   
date:       2022-11-26
author:     bjmsong
header-img: 
catalog: true
tags:
    - 深度学习框架
---
## Pytorch
### 分布式
+ https://pytorch.org/tutorials/distributed/home.html
1. Distributed Data-Parallel Training(DDP)
+ 相较于DataParallel（DP，单进程多线程，受限于全局锁GIL），DDP使用每个GPU启动一个进程，效率更高，代码稍微复杂一些
+ 迭代步骤
    * 将模型/优化器复制到各个GPU，将数据平分到各块GPU，每块GPU独立执行前向计算和反向传播，同时使用Ring All-Reduce算法，一边计算梯度，一边把梯度传递给另一块GPU(保证GPU始终在工作)，最终每块GPU得到相同的梯度，用优化器更新参数，然后进行下一轮迭代。
+ 适用于单机多GPU，多机多GPU
+ torchrun: fault-tolerant distributed training
    * 训练时定期保留快照，如果出现failure，可以从快照恢复，不需要从头开始跑
    * https://pytorch.org/docs/stable/elastic/run.html
+ Common Troubleshooting
    * Check that your nodes can communicate with each other
    * Set a debug flag for verbose logs
        - export NCLL_DEBUG=INFO
    * Explicitly pass network interface name 
+ 《PyTorch Distributed: Experiences on Accelerating Data Parallel Training》
+ 网上教程
    * https://zhuanlan.zhihu.com/p/98535650
    https://github.com/tczhangzhi/pytorch-distributed.git
    https://www.bilibili.com/video/BV1bd4y1X7hb/?spm_id_from=333.337.search-card.all.click&vd_source=7798c62f92ce545f56fd00d4daf55e26
    * https://zhuanlan.zhihu.com/p/113694038
    * https://blog.csdn.net/qq_34914551/article/details/110576421
    * https://www.bilibili.com/video/BV1yt4y1e7sZ/?spm_id_from=333.788.recommend_more_video.2&vd_source=7798c62f92ce545f56fd00d4daf55e26
    * https://zhuanlan.zhihu.com/p/361314953
2. Fully Sharded Data Parallel (FSDP)
- shards model parameters, optimizer states and gradients across DDP ranks
- makes the training of some very large models feasible
3. RPC-Based Distributed Training(RPC)
4. Custom Extensions
    * Collective Communication(c10d)

### 其它方法
https://efficientdl.com/faster-deep-learning-in-pytorch-a-guide/

### 多进程
+ https://pytorch.org/docs/stable/multiprocessing.html
- torch.multiprocessing扩展了python的multiprocessing模块


## Horovod
https://cloud.tencent.com/developer/article/1117910
https://eng.uber.com/horovod/
https://zhuanlan.zhihu.com/p/40578792


## TensorFlow
### 《Hands on ml2》Chapter 19
https://www.tensorflow.org/guide/distributed_training?hl=zh-cn

### 官方文档
- code
    - learn/MachineLearning/tf_distribute.py
- 业界实践
https://zhuanlan.zhihu.com/p/430383324
- 默认使用全部CPU
https://stackoverflow.com/questions/42845261/does-tensorflow-job-use-multiple-cores-by-default
https://stackoverflow.com/questions/38836269/does-tensorflow-view-all-cpus-of-one-machine-as-one-device
- 数据并行：更容易实现，tensorflow目前只支持这个
https://www.pythonf.cn/read/100405
https://cloud.tencent.com/developer/article/1453361
https://tf.wiki/zh_hans/appendix/distributed.html
- 模型并行
    - 比较难
    - https://www.tensorflow.org/tutorials/distribute/keras?hl=zh-cn
    - https://medium.com/analytics-vidhya/speeding-up-inference-using-parallel-model-runs-d76dcf393567   
- 分布式训练
https://zhuanlan.zhihu.com/p/469541810
https://zhuanlan.zhihu.com/p/56991108
https://www.tensorflow.org/guide/distributed_training?hl=zh-cn
https://github.com/yahoo/TensorFlowOnSpark
https://towardsdatascience.com/distribute-your-pytorch-model-in-less-than-20-lines-of-code-61a786e6e7b0
https://blog.csdn.net/weixin_39589455/article/details/120759372


