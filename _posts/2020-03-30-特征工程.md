---
layout:     post
title:      特征工程
subtitle:   
date:       2020-03-30
author:     bjmsong
header-img: img/machineLearning/machineLearning.png
catalog: true
tags:
    - 机器学习

---

> 特征没做好，参数调到老
>
> 特征工程：简而言之，就是用最好的方法来表示数据的艺术/科学。“最好的”是什么意思？大体上，提供给算法的数据的方式，应该以最有效的方式表示潜在信息的相关结构/属性。当你进行特征工程时，你是在把你的数据属性转化为数据特征。
> 好的特征工程是专业知识，直觉和基础的数学能力的优雅组合。

### 特征工程的目的
旨在去除原始数据中的杂质和冗余，设计更高效的特征以刻画求解的问题与预测模型之间的关系

### 构建特征

- **重要的事情说三遍：基于业务！基于业务！基于业务！不要拍脑袋**
- 可以尝试先用规则去解决这个问题，在这个过程中发现这个项目目标实际影响的因素，作为之后建模参考的变量
- 变量不是越多越好，以下几类变量要警惕加入模型：
  - 缺失值太多的
  - 数据源准确性不佳的
  - 取数过程工作量极大的
  - 与其它变量有较大相关性的
- 好的特征具有如下的特点：
  - 有区分性（Informative）
  - 特征之间相互独立（Independent）
  - 简单易于理解（Simple）


### 做探索性数据分析（EDA）

- **建模前，先分析数据，如果觉得人也没法从这个数据中得到想要的结果，不能指望机器能做到**

- **探索性数据分析（EDA）目的是最大化对数据的直觉，完成这个事情的方法只能是结合统计学的图形以各种形式展现出来。**

- 通过EDA可以实现:

  ❏	得到数据的直观表现 
  ❏	发现潜在的结构 
  ❏	提取重要的变量 
  ❏	处理异常值 
  ❏	检验统计假设 
  ❏	建立初步模型 
  ❏	决定最优因子的设置

- 探索性数据分析常用的一些常见问题

  1. 数据的典型值是多少(均值，中位数等)？

  2. 变量的缺失性怎么样？

  3. 一组数据的良好分布拟合是什么？

  4. 一个因子是否有影响？

  5. 最重要的因素是什么？

  6. 我们可以将时间相关数据中的信号与噪声分离吗？

  7. 我们可以从多变量数据中提取任何结构吗？

  8. 数据是否有离群值？ 如t-sne可以实现对高维数据集降维可视化



### 数据清洗

- **数据（变量，目标值）的正确性无比关键，脏数据训练的结果就是garbage in，garbage out**

- 缺失值处理
	- **根据业务经验填充，重要变量尽量填充，避免做决策树模型时因为该值null，分到不合理的节点**
	  - “用户风险偏好”可以填充所有用户的中位数
	  - 基金的数据可以勇基金所属公司下属其它基金的数据填充
	- 将“是否缺失”作为一个新的变量
	- 如果预测样本里面缺失值不可避免，则训练集可以保留缺失值，甚至随机掩盖一些数据，造缺失值
	- 模型预测缺失值
  	- sknn：最近k个邻（3,5），距离平方的倒数为权，从缺失最少的开始，填充数据可以用于后续计算
  	- svd：寻找与数据X同大小填补矩阵Y，最小化Φ=‖X-eTμ-Y‖，SVD迭代求解
  	- EM：选取初始簇中心，E步（贝叶斯最大后验簇分配），M步（重新分配簇）
  - https://blog.csdn.net/jingyi130705008/article/details/82670011?utm_source=blogxgwz9
  - http://blog.sina.com.cn/s/blog_670445240102v08m.html
- 异常值处理
	- 基于统计的异常点检测算法
		- 3sigma：与平均值超过3倍标准差，数据需满足正态分布
			- 箱线图：大于QU+1.5IQR,小于QL−1.5IQR.QU:上四分位数,QL:下四分位数,IQR:QU-QL.
	- 基于距离的异常点检测算法
		- 主要通过距离方法来检测异常点，将数据集中与大多数点之间距离大于某个阈值的点视为异常点，主要使用的距离度量方法有绝对距离 ( 曼哈顿距离 ) 、欧氏距离和马氏距离等方法。
	- 基于密度的异常点检测算法
		- 考察当前点周围密度，可以发现局部异常点，例如LOF算法
	- 异常检测算法（孤立森林，多元高斯分布）
	- 异常值的判断和处理
	  - 类别型异常：稀有事件（<1%）
	  - 连续型：3个标准差之外
	  - 需要结合业务逻辑：是真异常，还是另类？是否代表一类有特殊需求的用户？
	  - 删除异常值有利于模型的稳定
	  - 异常值也可以是一个新的业务方向，比如新用户类型、恶意欺诈用户


### 连续特征的处理
#### 归一化
- 对于数值型特征，不同特征的量级差别很大，通常需要对特征进行标准化，也就是将数据按比例缩放，使之落入一个小的特定区间。标准化可以帮助我们减少训练过程中的bias，同时加速训练过程。常用的标准化的方法有min-max，z-score等
- https://ai-pool.com/a/s/normalization-in-deep-learning
- https://tech.meituan.com/recommend_dnn.html
- deeplearning.ai Normalizing Inputs (C2W1L09)
- batch normalization
deeplearning.ai Normalizing Activations in a Network (C2W3L04)
《hands on ml》
https://towardsdatascience.com/batch-normalization-in-practice-an-example-with-keras-and-tensorflow-2-0-b1ec28bde96f
- 树模型不需要

#### 离散化
- **考虑到LR模型的特性，先对连续特征先进行等频离散化，即基于特征值的频率等频分成N个桶，并做0-1标识，最后和离散型特征，统一进行One-hot编码，喂给LR模型**。
- **在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型**，这样做的优势有以下几点：
0. 离散特征的增加和减少都很容易，易于模型的快速迭代；
1. 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；
2. 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；
3. 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；
4. 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；
5. 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；
6. 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。

- 李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。

- https://www.zhihu.com/question/31989952/answer/54184582


### id类特征的处理
https://www.jianshu.com/p/5aea25e80520
https://www.deeplearn.me/3842.html
https://www.cnblogs.com/gczr/p/14350567.html

### 特征筛选
- 意义
   - 少而精的自变量有助于提高模型的稳定性
   - 有效自变量可以提高模型预测能力
   - 提高运算速度和效率

 - 基础筛选

   - 去除常量
   - 去除缺失率高的变量
   - 取值过泛的变量，比如邮编

 - 结合业务经验的筛选：让业务专家来讨论变量的重要性，但需要用数据再验证
   
 - RF/xgboost等树模型
   
 - L1正则

 - 不推荐以下方法
   
    - PCA不适合用来防止过拟合，适用于数据降维（为了减少存储开销等），可视化
    - 求自变量和因变量的相关性：也许自变量和因变量有非线性的关系，仅根据相关性不能作为筛选依据 
    - sklearn.RFECV：计算量大
    
 - 自变量与因变量  
      - 相关性过低的（对于连续型）
      - 卡方检验不显著（对于离散型）
      - 信息价值IV（information value）过低
        - [IV & WOE](https://blog.csdn.net/kevin7658/article/details/50780391)	
      - 基尼系数（Gini Score）过低
      - 这些检验可能表示自变量不重要，但不能排除交互作用
      
- https://www.cnblogs.com/hhh5460/p/5186226.html  
- https://mp.weixin.qq.com/s?__biz=Mzg5ODAzMTkyMg==&mid=2247486594&idx=1&sn=7800b2aabac7d208921be259478ac620&chksm=c06984dff71e0dc9410037d407393153411294ae7adec11d7bef1b7996c064a854d0909cae6e&mpshare=1&scene=1&srcid=&sharer_sharetime=1564447357577&sharer_shareid=49581f7bdbef8664715f595bc62d7044&key=8a4c04f4ab18b288d2023df92965edae448fa68263e4981c3ff42a601e22dfe6e9751e9dc425f59672be06cf394405fa6b3ba34674b51dd4db9c5e020948f360dd3c91b39c74092b7b5a74bfd58130f6&ascene=1&uin=MjM1OTMwMzkwMA%3D%3D&devicetype=Windows+7&version=62060833&lang=zh_CN&pass_ticket=pBdN32AlyZ77Ty%2BYvZUYJTqAYVrcJZCIivujUOoaFs75nQNfGLTNMuQcGwRf7Pzp

### 组合特征
- gbdt+lr
- FM，FFM


### 文本特征
- bag of words
- tf-idf
- 主题模型
- word2vec
- https://www.jiqizhixin.com/articles/2017-04-30-9
- 张俊林：放弃幻想，全面拥抱Transformer：NLP三大特征抽取器（CNN/RNN/TF）比较


### 类别变量的处理 
- 序号编码：类别之间存在大小关系，如成绩“高”、“中”、“低”
- 独热编码（one-hot encoding）
  - 类别取值较多的情况下可以考虑：用稀疏向量存储、配合特征选择来降低维度
  - 缺点：数据稀疏、所有的类别同样重要，无法体现出不同样本之间的关系
  - multi-hot encoding：可以取到同一个类别中的多个值，如用户看过的电影
- 二进制编码：维度少于独热编码，节省了存储空间
- **embedding**
  - https://mp.weixin.qq.com/s?__biz=Mzg5ODAzMTkyMg==&mid=2247486758&idx=1&sn=1a5b2558511fc52621300ca45f27356c&chksm=c069857bf71e0c6d866b6b8cbef4fed0225f1e4acc3208fd44fbef737a9901704f4b3a89306e&mpshare=1&scene=1&srcid=&sharer_sharetime=1566085108359&sharer_shareid=602b1ccf63ca4ea52755ecd058f6d407&key=8a4c04f4ab18b288a2b1e2103adeeb4e7952007b30884c7484c137968fd745d35a8a4b501869275fe676f69010d14f457a372ef4786f5df8b08b9c956eb27d54b931cd34464b8ba75546b8af6ec2e9e7&ascene=1&uin=MjM1OTMwMzkwMA%3D%3D&devicetype=Windows+10&version=62060833&lang=en&pass_ticket=IUUUfbK16TvO1gSfqeJNL4tVQmZmZcGoL1QKs%2BBeaVEhl7p4xYuuhsscZVM1JWZn
  - https://www.zhihu.com/question/34819617
- Hash成64位：前16位空置，中间12位为Slot_id,最后36位为FeatureID（即原ID的二进制表示）
      
### 特征衍生

- 特征扩充
>(1) 用户兴趣向量：lda的中间产物之一用户潜在兴趣向量，以及基于w2v的词向量和用户行为历史统计出的用户兴趣向量，可丰富用户维度上的兴趣特征。 
  (2) 资源embedding向量：基于用户行为数据进行的embedding的w2v/lda的词向量，以及基于资源本身title的doc2vector都可丰富最近模型使用的特征维度。且word2vecor和lda由于原理上的区别，可拓展出不同意义上的特征向量。 
  (3) 资源封面AutoEncode向量：基于资源封面图像，离线采用AutoEncode训练，提取隐层向量，作为资源本身特征。

- 统计特征细化
>(1) 特征工程时间窗口细化： 将资源的统计特征，按不同的时间窗口，如1,3,5,7,15天进行特征细化，可在丰富资源特征的同时，融入时间衰减的因素。 
(2) 在线特征交叉：对于一些资源统计特征，在与用户特征以及上下文特征进行交叉之后效果会更加明显，根本原因在于增加了样本特征的区分度；具体交叉方法的落地，建议由服务端是执行，这里比如一个男性用户发来请求，服务端可实时获取用户的性别特征（男）以及召回队列中每个资源的在不同性别上统计特征（如历史CTR），基于特征编码表，新增加交叉特征(sex_cross_ctr)，并将每个资源在男性上的ctr上作为新增交叉特征的特征值，记录成log，后面回流给模型样本。其实所有的在线特征都和交叉特征一样，最后都由服务端的日志回流给模型训练过程。
- 基于原有特征构造
  - 单变量：x^2、e^x、偏度较大的特征：log、Box－Cox变换
  - 多变量：线性组合、非线性组合、x/y、x*y
  - 核函数
- 定量特征分桶
  ■	之后不需要再做one-hot-encoder
  ■	其同一区间的数字能够代表相同的特征
  ■	减少过拟合
- 连续值排序：df.rank
- 按时间偏移：对于部分时间序列变量存在“时滞”效应
- K-means对样本聚类，以所属类别的哑编码为新特征
- Auto-encoder
  - GBDT算法的特性，在面对高维稀疏矩阵的时候效果并不好。因为为了控制过拟合，GBDT通常会限制叶子节点的样本数目，稀疏矩阵经过少量分支就已经到达了减枝条件，也就无法体现深度挖掘组合关系的特点。
  - 采用autoencoder在没有人工干预的情况下自动将稀疏特征编码
  - 通过一个深度学习网络结构，用特征经过一系列网络结构预测自己。这样的模型设定通过设定最优化目标是是否可以通过自己预测自己，保证在降低数据噪音、降低数据维度的同时，最大限度的保留原有数据的信息量。
- 特征自动生成：`featuretools`


### 其它操作

- [卡方分箱](https://www.cnblogs.com/wqbin/p/10547167.html)
- 抽样
  - 训练集样本至少1000，保证模型的稳定性
  - 验证集可以是训练集的1/10~1/2
  - 训练集必须是自变量数的10倍以上
  - 目标事件必须是自变量数的6~8倍以上
  - 自变量数一般在8~20个，过多、过少都不一定好



### 参考资料

- Coursera How to Win a Data Science Competition: Learn from Top Kagglers
- 百面机器学习，第一章 特征工程
- https://www.zhihu.com/question/29316149/answer/607394337
- https://www.slideshare.net/HJvanVeen/feature-engineering-72376750
- https://zhuanlan.zhihu.com/p/71609765
- Dipanjan Sarkar：不会做特征工程的 AI 研究员不是好数据科学家
https://www.leiphone.com/news/201801/T9JlyTOAMxFZvWly.html
https://www.leiphone.com/news/201801/KTVu68zA6szteVmS.html
- 《特征工程入门与实践》
https://mp.weixin.qq.com/s?__biz=MzI1MzY0MzE4Mg==&mid=2247485232&idx=1&sn=e83004347d6b5c993af3cfa64f533a0c&chksm=e9d017f1dea79ee7f2e399884b90f3e55c33c1c98747fb55f9a4f5257bcce22374465a344f64&scene=0&xtrack=1&key=c86a338f58bd007cdffd48c99e9e2298a13ebf736243b5d96f69a9361632ffa5a9388e60759653767b14c83729bcbdc54fb6f30806f1c212ff1fb14b99f43248d0ece5001c9cbc78052f7ba5cda89774&ascene=1&uin=MjM1OTMwMzkwMA%3D%3D&devicetype=Windows+7&version=62060833&lang=zh_CN&pass_ticket=fZx3R6hAbiq5JXQFJM5ZrEYTbNGY4PKgyOa4uD91VEjM4%2FQUCMtw94n3V1bJeCDB
- https://github.com/Pysamlam/Tips-of-Feature-engineering
