---
layout:     post
title:      【转载】Spark性能优化之二
subtitle:   资源调优
date:       2019-09-03
author:     bjmsong
header-img: img/spark/Spark_logo.png
catalog: true
tags:
    - spark
---

> 
<!-- > 感谢[Huxpro](http://qiubaiying.top)提供的博客模板 -->
> 
<!-- > [我的的博客](http://qiubaiying.top) -->


### Spark作业的基本原理

<ul> 
<li markdown="1"> 
![spark作业基本原理]({{site.baseurl}}/img/spark/spark优化/Spark作业基本原理.png) 
</li> 
</ul> 


详细原理见上图。我们使用spark-submit提交一个Spark作业之后，这个作业就会启动一个对应的Driver进程。根据你使用的部署模式（deploy-mode）不同，Driver进程可能在本地启动(default: client)，也可能在集群中某个工作节点上启动(cluster)。Driver进程本身会根据我们设置的参数，占有一定数量的内存和CPU core。而Driver进程要做的第一件事情，就是向集群管理器(Standalone，YARN等)申请运行Spark作业需要使用的资源，这里的资源指的就是Executor进程。YARN集群管理器会根据我们为Spark作业设置的资源参数，在各个工作节点上，启动一定数量的Executor进程，每个Executor进程都占有一定数量的内存和CPU core。

在申请到了作业执行所需的资源之后，Driver进程就会开始调度和执行我们编写的作业代码了。Driver进程会将我们编写的Spark作业代码分拆为多个stage，每个stage执行一部分代码片段，并为每个stage创建一批task，然后将这些task分配到各个Executor进程中执行。task是最小的计算单元，负责执行一模一样的计算逻辑（也就是我们自己编写的某个代码片段），只是每个task处理的数据不同而已。一个stage的所有task都执行完毕之后，会在各个节点本地的磁盘文件中写入计算中间结果，然后Driver就会调度运行下一个stage。下一个stage的task的输入数据就是上一个stage输出的中间结果。如此循环往复，直到将我们自己编写的代码逻辑全部执行完，并且计算完所有的数据，得到我们想要的结果为止。

**Spark是根据shuffle类算子来进行stage的划分。**如果我们的代码中执行了某个shuffle类算子（比如reduceByKey、join等），那么就会在该算子处，划分出一个stage界限来。可以大致理解为，shuffle算子执行之前的代码会被划分为一个stage，shuffle算子执行以及之后的代码会被划分为下一个stage。因此一个stage刚开始执行的时候，它的每个task可能都会从上一个stage的task所在的节点，去通过网络传输拉取需要自己处理的所有key，然后对拉取到的所有相同的key使用我们自己编写的算子函数执行聚合操作（比如reduceByKey()算子接收的函数）。这个过程就是shuffle。

当我们在代码中执行了cache/persist等持久化操作时，根据我们选择的持久化级别的不同，每个task计算出来的数据也会保存到Executor进程的内存或者所在节点的磁盘文件中。

因此Executor的内存主要分为三块：
- 第一块是让task执行我们自己编写的代码时使用，默认是占Executor总内存的**20%**；
- 第二块是让task通过shuffle过程拉取了上一个stage的task的输出后，进行聚合等操作时使用，默认也是占Executor总内存的**20%**；
- 第三块是让RDD持久化时使用，默认占Executor总内存的**60%**。

task的执行速度是跟每个Executor进程的CPU core数量有直接关系的。一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。如果CPU core数量比较充足，而且分配到的task数量比较合理，那么通常来说，可以比较快速和高效地执行完这些task线程。

以上就是Spark作业的基本运行原理的说明，大家可以结合上图来理解。理解作业基本原理，是我们进行资源参数调优的基本前提。

### 资源参数调优

- num-executors
- executor-cores
    - num-executors*executor-cores 代表每次可以执行task的最大数量
- executor-memory
- driver-memory
    - Driver的内存通常来说不设置，或者设置1G左右应该就够了
- spark.sql.shuffle.partitions，spark.default.parallelism
    - spark.sql.shuffle.partitions用于设置shuffle操作时的分区数量
    - spark.default.parallelism用于设置RDD经过transform操作后默认的分区数量
    - 设置过小可能造成：Spill,OOM
    - 设置过大可能造成：更长的任务调度时间，更多IO请求，更多小文件输出
    - 同一个Shuffle Partition个数无法适应所有的Stage
    - Spark官网建议的设置原则是，设置**该参数为num-executors * executor-cores的2~3倍较为合适**，如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！
- spark.storage.memoryFraction
    - 该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。
- spark.shuffle.memoryFraction
    - 该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。


### 参考资料
- https://tech.meituan.com/2016/04/29/spark-tuning-basic.html
