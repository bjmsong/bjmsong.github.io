---
layout:     post
title:      新竹清华大学：并行计算与并行编程
subtitle:   Part III GPU Programming
date:       2022-11-16
author:     bjmsong
header-img: 
catalog: true
tags:
    - 并行计算
---
## 八: Heterogeneous Computing
### Heterogeneous Computing
- Heterogeneous Computing is an integrated system that consists of different types (programmable) computing units
    + DSP,FPGA,ASIC,GPU,Co-processor
- CPU
    + 容易编程
    + 计算能力弱
- Acclerator
    + 不容易编程：tool chain不成熟
    + 计算能力强
- 架构
    + 最常用：GPU Servers：same HW architecture as commodity server, but memory copy between CPU and GPU becomes the main bottleneck
        * GPU 通过PCIE和主板连接，GPU内部有内存&控制单元，CPU无法控制GPU，只是把数据放到内存中，GPU来取
    + Heterogeneous System Architecture (HSA)
        * Unified coherent memory
            - Single virtual memory address space
            - Prevent memory copy
        * Aim to provide a common system architecture for design high-level programming modeles for all devices
        * APU: AMD Accelerated Processing Unit

### GPU
- SIMD architecture
- Massively multithreaded manycore chips
- GPGPU: General-Purpose Graphic Processing Unit
    + Expose the horse power of GPUs for general purpose computations
        * exploit data parallelism for solving embarrssingly parallel tasks and numeric computations
    + Programmable
        * early: using the libraries in computing graphics, such as OpenGL and DirectX, to perform the tasks other than the original hardware designed for 
        * now: CUDA and openCL provides an extension to C and C++ that enables parallel programming on GPUs
            - OpenCL：跟OpenMP类似，可以自动把程序转换成GPU可执行的，比较容易上手，但是执行效率不高
            - CUDA：需要熟悉底层架构，才能较好地优化效率
- System Architecture
    + bottleneck
        * gpu和cpu通信带宽低
        * GPU memory size比较小
- Manycore GPU - Block Diagram
    + processor hierachic
        * stream multi-processors (SM) -> multi core
    + memory hierachic
        * global memory(slow, but large&shared) -> PBSM/shared memory -> logic register(fast, but small & local)
    + Host传输指令，GPU Thread Execution Manager执行调度
- Stream Multiprocessor
    + Each SM is a vector machine 
    + Register & shared memory 共用同一块memory chip，只是访问限制不同 
    + shared memory带有L1 cache
    + hardware scheduling for thread execution and hardwre context switch 
- NVIDIA CUDA-Enabled GPUs Products：产品矩阵
    + 架构：Volta>Pascal>Maxwell>Kepler>...
    + 应用：Embedded(Deep Learning Inference),Consumer Desktop/Laptop(Visualization, single precision, GeForce系列),Profession workstation,Data Center(HPC, double precison, V100/H100)
    + 主要参数
        * #Cores
        * Core Clock
        * GPU memory bandwidth
        * GPU memory size：Shared Memory / L1 Cache Size / Registers
        * Interconnect Bandwidth(GPU之间)
            - NVLink >> PCIE
            - GPU和CPU之间由于产商不统一，通信比较难协调
        * Single Precision & Double Precision
            - GeForce系列的Single Precision比V100系统差距不大，Double Precision差距很大，深度学习很多时候不需要Double Precision，因此用GeForce系列性价比高
            - FLOPs就是每秒浮点操作数，其中单精度浮点可以用于深度学习模型训练。双精度浮点计算可以用于数值模拟工作
        * Compute Capability: Programming ability of a GPU device
            - 主要是适配CUDA的版本，CUDA版本越高，支持的功能越多，编程能力越强
        * Price
        * Power
        * https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/
- Architecture Roadmap：主要技术演进
    + Stacked DRAM: 3D，增加带宽
    + NVLink
    + CUDA版本更新
    + 芯片制程
    + Tensor Core
    + 架构演进
    https://zhuanlan.zhihu.com/p/413145211
- https://hothardware.com/reviews/nvidia-gf100-architecture-and-feature-preview

## 九: GPU Parallel Programming：CUDA
- CUDA Toolkits：Software Development Kit(SDK, 软件开发工具包) for CUDA Programming 
    + https://developer.nvidia.com/cuda-toolkit
    + C/C++ compiler: nvcc
    + debugging and optimization tools: IDE,Debugger,Profilers,Utilities(辅助工具)
    + GPU-accelerated libraries: cuBLAS, cuFFT ...
    + Sample code
    + Documentaion
    
### Programm Model
- CUDA: Compute Unified Device Architecture
    + CUDA is a compiler and toolkit for programming NVIDIA GPUs
    + Enable heterogeneous computing and horsepower of GPUs
    + CUDA API extends the C/C++ language
    + Express SIMD parallelism
    + Given a high level abstraction from hardware
- CUDA program flow
    1. Copy data to GPU(device) memory from CPU(host) memory
    2. Call GPU functions
    3. GPU executes program
    4. Copy result from GPU memory to CPU memory
- CUDA Programming Model
    + Serial C code executes in a host(CPU) thread
    + Parallel kernel C codes execute in many device(GPU) threads across multiple processing elements
        * kernel有很多个block组成，block里面有很多个thread
        * 一个block对应一个Stream Multiprocessor
- CUDA program framework

```C
#include <cuda_runtime.h>
__global__ void my_kernel(...){
    ...
}
int main(){
    ...
    cudaMalloc();
    cudaMemcpy();
    ...
        my_kernel<<<nblock, blocksize>>>(...);
    ...
    cudaMemcpy();
    ...
}
```

- Kernel = Many Concurrent Threads
    + One kernel is executed at a time one the device
    + Many threads execute in each kernel
        * each thread executes the same code
        * ... on the different data based on its threadID
    + CUDA thread is Physical threads
        * GPU thread creation and context switching(线程间上下文切换) are essentially free
            - active concurrent threads 数量 >> gpu core 的数量 
            - 每个core有好几组register，每个thread有自己独立的register，上下文切换时不需要重新读写register（cpu需要）
                + 这样在一个thread读写memory的时候（通常耗时比较久），GPU可以计算下一个thread
- Hierarchy of Concurrent Threads
    + Threads are grouped into thread blocks
        * Kernel = grid of thread blocks
    + By definition, threads in the same block may synchronized with barriers, but not between blocks(效率很低，官方不支持这样做)
- Software Mapping
    + Software: grid/kernel -> blocks -> threads
        * 同一个blcok里面的threads可以通过share memory沟通，不同block的threads只能通过global memory沟通
    + Hardware: GPU(device) -> SM(multicore processor) -> core
- Block level Scheduling
    - Blocks are independent to each other to give scalability
        + A kernel scales across any number of parallel cores by scheduling blocks to SMs
- Thread level scheduling - Warp
    + Inside the SM, threads are launched in groups of 32(不同GPU可能会有不同), called warps
        * warps share the control part(wrap scheduler)
        * threads in a warp will be executing the same instruction(SIMD)
            - execute physically in parallel
- Thread group limits
    + use deviceQuery.cpp to find your limits
    + total number of threads per kernel = threads per block * number of blocks
        * max dimension size of a thread block : e.g. (1024, 1024, 64)
        * max dimension size of a grid size: e.g. (2^31-1, 2^16-1, 2^16-1)
    + maximum execution concurrency
        * maximum number of resident grids per device: 32
            - host起多线程，可以在device上同时跑多个kernel
        * maximum number of threads per multiprocessor
- Memory Hierarchy
    - Thread独有的：寄存器，kernel里面定义的local variable，per-thread local memory
    - Block内共享的：per-block shared memory
    - 不同Block，不同kernel可以访问：per-device global memory
- Memory size limits
    + use deviceQuery.cpp to find your limits
    + global memory: e.g. 11171 MB
    + const memory: 64 MB
        * 属于global memory
    + shared memory per block: 48 MB
    + number of registers avaliable per block: 65536
- CUDA Programming Terminology
    + Host: CPU
    + Device: GPU
    + Kernel: funcions executed on GPU
    + Thread: the basic execution unit
    + Block: a group of threads
    + Grid: a group of blocks

### CUDA Language
- Philosophy: provide minimal set of extensions necessary
- Kernel launch
    + kernelFunc<<<nB, nT, nS, Sid>>>
        * nB: number of blocks per grid 
        * nT: number of threads per block
        * nS(optional): shared memory size
            - runtime不能修改了
        * Sid(optional): stream ID
- Build-in device variables: threadIdx, blockIdx, blockDim, gridDim
    + Thread and Block IDs
    + The index of threads and blocks can be denoted by a 3 dimension struct
- Intrinsic functions that expose operations in kernel code
    + \___syncthreads()
- Function Qualifiers
    + __device__: executed on the device, callable from the device only
    + __global__: executed on the device, callable from the host only, return type is void
    + __host__: executed on the host, callable from the host only
        * 一般不写，默认是host
- Variable Type Qualifiers
    + __device__: resides in device's global memory space 
    + __constant__: resides in device's constant memory space 
    + __shared__: resides in the shared memory space of a thread block
- Device memory opertions
    + cudaMalloc(void **devPtr, size_t size)
        * similar to C's malloc() 
        * devPtr: return the address of the allocated device memory pointer
        * size: the allocated memory size in bytes
    + cudaFree(void *devPtr)
        * similar to C's free() 
    + cudaMemcpy(void *dst, const void *src, size_t count, enum cudaMemcpyKind kind)
        * similar to C's memcpy()
        * count: size in bytes to copy
        * cudaMemcpyKind
            - cudaMemcpyHostToHost
            - cudaMemcpyHostToDevice
            - cudaMemcpyDeviceToHost
            - cudaMemcpyDeviceToDevice
- Program Compilation
    + Two-stage Compilation
        * Generic: NVCC把code分成CPU code和PTX code
            - PTX：Parallel Threads eXecutions
        * Specialized: PTX to Target Complier
            - Device-specific binary object

### Example Code Study
- kernel function 里面最好不要加条件判断等，gpu处理的效率很低

### CPU & GPU Synchronization
- Most CUDA function calls are asynchronous
    + Control is returned to the host thread before the device has completed the requested task
    + But function calls from a kernel are serialized on GPU
    + 只有同时涉及CPU和GPU的function才是同步的，如cudaMemcpy()
- Asynchronous functions:
    + Kernel launches
    + Asynchronous memory copy and set options: cudaMemcpyAsync, cudaMemsetAsync
    + cudaMemcpy within the same device
    + H2D cudaMemcpy of less than 64KB
    + cudaEvent Functions
- Why use Asynchronous functions?
    + Overlap CPU computation with the GPU computation or data transfer
        * CPU和GPU的计算或者数据传输可以同步进行 

    ```C
    void main() {
        cudaMemcpy (/**/, H2D);
        kernel<<< grid, block >>>();
        cudaMemcpy (/**/, D2H);     
        cpu_method();   
    }

    void main() {
        cudaMemcpy (/**/, H2D);
        kernel<<< grid, block >>>();
        cudaMemcpyAsync (/**/, D2H);     
        cpu_method();   
    }

    ```

- Synchronization between CPU & GPU
    + Device based: cudaDeviceSynchronize()
        * Block a CPU thread until all issued CUDA calls to a device completed
    + Context based: cudaThreadSynchronize()
        * Block a CPU thread until all issued CUDA calls from the thread completed
    
    ```C
    void main(){
        cudaSetDevice(0);
        kernel1 <<< grid, block >>> ();
        kernel2 <<< grid, block >>> ();
        cudaSetDevice(1);
        kernel3 <<< grid, block >>> ();
        cudaDeviceSynchronize();     // 等这个device上所有的CUDA call(也就是kernel3)都执行完成，才会执行下面的CPU命令
        cpu_method();
    }

    void main(){
        cudaSetDevice(0);
        kernel1 <<< grid, block >>> ();
        kernel2 <<< grid, block >>> ();
        cudaSetDevice(1);
        kernel3 <<< grid, block >>> ();
        cudaThreadSynchronize();     // 等这个thread下面所有的CUDA call(也就是kernel1，kernel2，kernel3)都执行完成，才会执行下面的CPU命令
        cpu_method();
    }
    ```

    + Stream based: cudaStreamSynchronize(stream-id)
        * Block a CPU thread until all CUDA calls in stream stream-id completed
    + Event based：类似于打断点 
        * cudaEventSynchronize(event)
            - Block a CPU thread until event is recorded
        * cudaStreamWaitSynchronize(stream-id, event)
            - Block a CPU stream until event reports completion
    
- CUDA event
    + Data type: cudaEvent_t
    + cudaError_t cudaEventCreate(cudaEvent_t event)
        * Create cuda event
    + cudaError_t cudaEventRecord(cudaEvent_t event, cudaStream_t stream=0)
        * Record cuda event
        * If stream is non-zero, the event is recorded after all preceding operations in the stream have been complted
        * Since operation is asynchronous, cudaEventQuery() and/or cudaEventSynchronize() must be used to determine when the event has actually been recorded
    + cudaError_t cudaEventSynchronize(cudaEvent_t event)
        * Wait until the completion of all device work preceding the most recent call to cudaEventRecord() 
        
    ```C
    void main(){
        cudaSetDevice(0);
        kernel1 <<< grid, block >>> ();
        cudaEventRecord(event);
        kernel2 <<< grid, block >>> ();
        cudaSetDevice(1);
        kernel3 <<< grid, block >>> ();
        cudaEventSynchronize(event);     // cpu会一直block，直到kernel1执行完为止
        cpu_method();
    }

    // Kernel Time Measurement Example
    cudaEvent_t start,stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);
    cudaEventRecord(&start);

    kernel <<< grid, block>>> ();

    cudaEventRecord(&stop);  
    cudaEventSynchronize(&stop);

    float time;
    cudaEventElapsedTime(&time, start, stop);   
    ```

### Multi-GPU
- Single Thread multi-GPUs
    + All CUDA calls are issued to the current GPU
        * cudaSetDevice(): sets the current GPU
    + Asynchronous calls don't block switching GPUs
- multi-threads multi-GPUs within a node
    + multiple CPU threads belongs to the same process, such as pthread or openMP
    + Put CUDA functions inside the parallel region
    + The number of CPU threads is the same as the number of CUDA devices. Each CPU thread controls a different device,processing its portion of data
    + It's possible to use more CPU threads than the devices
        * 不同的cpu thread在同一个device上执行function，这些functions可以同时执行，但是要共享这个device上的资源
- muti-GPUs on multi-nodes
    + Need to go through network API, such as MPI, MPI负责数据通信
    
    ```C
    int main(int argc, char* argv[]){
        int rank, size;
        int A[32];
        MPI_init(&argc, &argv);
        MPI_Comm_rank(MPI_COMM_WORLD, &rank);
        MPI_Comm_size(MPI_COMM_WORLD, &size);
        printf("I am %d of %d\n" rank, size);
        for (int i=0; i<32; i++)
            A[i] = rank+1;
        launch(A);   // a call to launch CUDA kernel
        MPI_Barrier(MPI_COMM_WORLD);
        MPI_Finalie();
    }
    ```

    - share data between GPUs
        + Explicit copies via host
            * gpu1->host->gpu2
            
            ```C
            cudaSetDevice(0);
            cudaMemcpy(DM1, HM, n, D2H);
            cudaSetDevice(1);
            cudaMemcpy(HM, DM2, n, H2D);            
            ```

        + Zero-copy shared host array 
            * Device threads can read directly from host memory ove PCI-e without using cudaMemcpy H2D or D2H
                - 数据不需要在CPU和GPU之间搬来搬去
                - 但是如果数据要经常在GPU上运算，GPU每次读取会很耗时
            * The host memory must be pinned
        + Peer-to-peer memory copy
            * Direct copy from pointer on GPU A to pointer on GPU B
            
            ```C
            cudaError_t cudaMemcpyPeer(void *dst, int dstDevice, const void* src, int srcDevice, size_t count)

            cudaError_t cudaMemcpyPeerAsync()
            ```

### Dynamic Parallelism
- The ability to launch new grids from the GPU
    + kernel里面还可以launch kernel
- 优点
    + Reduce the number of kernel launches

## 十: GPU Architecture
### Thread execution
- Execution model
    + Threads are executed by scalar processor
    + Thread blocks are executed on SM 
        * Several concurrent thread block can reside on one SM
    + A kernel is launched as a grid of thread blocks  
- Thread Execution
    + CUDA threads are grouped into blocks
    + Hardware schedules thread blocks onto avaliable SMs
- Warp
    + Inside the SM, threads are launched in groups of 32, called warps
        * Warps share the control part: Warp Scheduler
        * At any time, only one warp is executed per SM
        * Threads in a warp will be executing the same instruction(SIMD)
        * active Warp的数量比SIMD processor的数量要多，因为Warp不会一直计算，在内存读取的时候其它warp可以并发计算
- Warp Divergence
    + if different threads in a warp need to do different things
        * 有分支判断语句
        * 这种情况下warp里面所有的thread会把所有的分支都执行一遍：效率很低

    ```C
    if(foo(threadIdx.x)){
        do_A();
    } else{
        do_B();
    }
    ```

    ```C
    __global__ void per_thread_sum(int *indices, float *data, float *sums){
        ...
        // 每个线程的迭代次数不一样(跟线程id有关)，同一个warp里面所有的线程要迭代跟最慢的线程一样的次数
        int i = threadIdx.x;
        for(j = indices[i]; j<indices[i+1]; j++)
            sum += data[j];
        
        sums[i] = sum
    }
    ```

        * 如何优化
            - 避免分支判断语句
            - 分支的粒度大于warp的大小(32)
                + Different warps can execute different code with no impact on performance
    
        ```C
        if (threadIdx.x / WARP_SIZE > 2) {...}
        else {...}
        ```

            - Unroll the statements can reduce the branches and increase the pipeline
            
            ```C
            for (i=0; i<n; i++){
                a = a + i;
            }

            // Unrolled 3 times
            for (i=0; i<n; i+=3){
                a = a + i;
                a = a + i + 1;
                a = a + i + 2;
            }

            // 也可以直接用
            #pragma unroll
            ```

- Atmoic Operations
    + Occasionally,an application may need threads to update a counter in a shared or global memory
        * 一堆线程要同时修改同一个变量
        * 这样会遇到Synchronization的问题
    + Solution: use atomic instructions supported by GPU (GPU会自动做加锁等动作,但是效率很低)
        * addition/subtraction
        * max/min
        * increment/decrement
        * compare-and-swap
    
    ```C
    __global__ void hist(int* color, int* bin){
        int i = threadIdx.x + blockDim.x * blockIdx.x;
        int c = color[i];
        atomicAdd(&bin[c], 1);
    }
    ```

### Memory hierarchy
- GPU Memory hierarchy
    + Registers
        * read/write per-thread
        * Low latency & High BW
    + Shared memory
        * read/write per-block
        * Similar to register performance
    + Global/Local memory(DRAM)
        * Global memory is per-grid
        * Local memory: register存不下的数据，放到这里，跟global memory是同一块硬件空间，但是访问权限不一样，是per-thread 
        * High Latency & Low BW
        * 不可以被cached
    + Constant memory
        * read only per-grid
        * 可以被cached
- Software to Hardware Mapping
    + Local variable(per thread): kernel function里面定义的变量 
        * Register
        * Local Memory
    + Shared variable(per block): kernel function里面定义的变量 __shared__ 标识
        * Shared memory
    + Global variable(per grid): kernel function里面定义的变量 __device__ 标识，或者是kernel function传入的变量
        * Global memory
        * Constant memory
- Register
    + Registers aren't indexable
        * Array variables always are allocated in local memory
    + Register spilling
        * Local memory is used if the register limit is met
            - Number of registers avaliable per block(CUDA6.1): 64k
                + 如果瓶颈是在存储而不是计算，增加thread，并不一定会加速
- Local memory
    - only allowed static array: array的大小不能是动态的 
- Shared memory
    + 作为global memory的Programmable cache 
        * Almost as fast as registers
    + Scope: shared by all the threads in a block
        * The threads in the same block can communicate with each other through the shared memory
    + Size: at most 48K per block(CUDA 6.1)
    + General Strategy
    1. Load data from global memory to shared memory
    2. Process data in the shared memory
    3. Write data back from shared memory to global memory
    - 实操
    
    ```C
    FW_APSP<<<1, n*n, n*n*sizeof(int)>>>(...);  // 第三个参数是shared memory的大小

    extern __shared__ int s[];    // 动态内存分配
    __global__ void FW_APSP(int* k, int* D, int* n){
        int i = threadIdx.x;
        int j = threadIdx.y;

        S[i*(*n) + j] = D[i*(*n) + j]; // 每个线程搬运一个data到share memory
        __syncthreads();   // 同步所有线程的搬运动作

        // do computation
        if (S[i*(*n) + j] > S[i*(*n) + k] + S[k*(*n) + j])
            D[i*(*n) + j] = S[i*(*n) + k] + S[k*(*n) + j]
    }
    ```

    - Limit of Dynamic Allocation
        + 只能分配一块内存空间（只能有一个指针）
        + 如果实在需要多个变量，需要自己管理内存空间
            * when calling kernel, launch it with size of sAs+sBs, where sAs and sBs are the size of As and Bs respectively
        + 只能分配一维数组，不能分配多维数组
    - Static Shared Memory Allocation
        + If the size of shared memory is known in compilation time, shared memory can be allocated statically
- Global memory
    + allocate global memory

    ```C
    // devPtr: return the address of the allocated memory on device
    cudaMalloc(void **devPtr, size_t size)

    cudaFree(void *devPtr)
    ```

    - Synchronous Global Memory Copy

    ```C
    cudaMemcpy()
    ```

- Constant memory
    - Same usage and scope as the global except 
        - read only
        - __constant__ 标识
        - Move by cudaMemcpyToSymbol() & cudaMemcpyFromSymbol()
    - Each SM has its own constant memory

    ```C
    __constant__ int constData[10];  // 数量是固定的
    int main(void){
        int A[100];
        cudaMemcpyToSymbol(constData, A, sizeof(A));
        add<<<grid_size, blk_size>>>();
        cudaMemcpyFromSymbol(A, constData, sizeof(A));
    }

    __global__ kernel(){
        int v = constData[threadIdx];
    }
    ```

## 十一: Performance(Memory) Optimizaiton for CUDA
### 
