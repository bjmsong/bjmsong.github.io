---

layout:     post
title:      主题模型
subtitle:   
date:       2020-02-18
author:     bjmsong
header-img: img/nlp/LDA.png
catalog: true
tags:
    - NLP
---



### 主题模型（Topic Model）

- 无监督的机器学习技术
- 用来在一系列文档中发现抽象主题的一种统计模型
- 主题模型自动分析每个文档，统计文档内的词语，根据统计的信息来断定当前文档含有哪些主题，以及每个主题所占的比例各为多少
- 精度不如主题分类（Topic Classification）高，毕竟主题分类加入了人工的经验（打标签）



### LDA

- 在机器学习领域，LDA是两个常用模型的简称：Linear Discriminant Analysis 和 Latent Dirichlet Allocation。本文的LDA仅指代Latent Dirichlet Allocation. LDA 在主题模型中占有非常重要的地位，常用来文本分类
- LDA是最常见的主题模型，是一般化的PLSI。由Blei, David M.、Ng, Andrew Y.、Jordan于2003年提出
- 将每篇文章被看成是一系列主题的混合
- 算法步骤（Collapsed Gibbs sampling）
  - 设定主题数K
  - 遍历文档，把每个单词随机地跟一个主题关联
  - 优化，迭代以下步骤，直到稳定
    - 对于每一篇文档`d`，遍历每个单词`w`，计算
      - `p(topic t|document d)`：proportion of words in document d that are assigned to topic t
      - `p(word w|topic t)`：proportion of assignments to topic t, over all documents d, that come from word w
      
    - 将单词`w`重新分配主题`t‘`
      $$
      argmax p(topic t’ | document d) * p(word w | topic t’)
      $$
- Question
  - 如何将文档的前后顺序考虑进来
    - 可以将前后n个单词看成是一个单词
    - lstm？
  - 如何将不同重要性的词区分出来



### TF-IDF（Term Frequency - Inverse Document Frequency）

- 关键词提取算法

- 衡量一个关键词对于文档的重要性

- 词频（Term Frequency, TF）表示关键词w在文档Di中出现的频率

- 逆文档频率（Inverse Document Frequency, IDF）反映关键词的普遍程度：当一个词越普遍（即有大量文档包含这个词）时，其IDF值越低

  <ul> 
  <li markdown="1"> 
  当一个词在文档频率越高并且新鲜度高（即普遍度低），其TF-IDF值越高。
  ![]({{site.baseurl}}/img/nlp/tfidf.png) 
  </li> 
  </ul> 

- TF-IDF兼顾词频与新鲜度，过滤一些常见词，保留能提供更多信息的重要词。

  

### TextRank

- 关键词提取算法

- 算法原理
  - Graph-based ranking algorithms
  - 通过结点之间的关系计算图中结点的重要性

  <ul> 
  <li markdown="1"> 
  每个结点的分数可以由以下公式计算得到（PageRank）
  ![]({{site.baseurl}}/img/nlp/textrank.png) 
  </li> 
  </ul> 

  - d是衰减系数，用作平滑

- 算法步骤

  - 通过词之间的相邻关系构建图
    - 将某一个词与其前面的N个词、以及后面的N个词均具有图相邻关系（类似于N-gram语法模型）
    - 具体实现：设置一个长度为N的滑动窗口，所有在这个窗口之内的词都视作词结点的相邻结点；则TextRank构建的词图为无向图
  - 在图上运行TextRank算法直到收敛
  - 根据最终得分将结点排序，得到关键词

- 应用
  - 提取关键词短语：如果提取出的若干关键词在文本中相邻，那么构成一个被提取的关键短语
  - 生成摘要
- TextRank的效果并不优于TF-IDF
- TextRank虽然考虑到了词之间的关系，但是仍然倾向于将频繁词作为关键词



### 背景知识

#### 词袋模型







### 参考资料

- https://zhuanlan.zhihu.com/p/41091116
- https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24

- https://www.kdnuggets.com/2016/07/text-mining-101-topic-modeling.html

- https://monkeylearn.com/blog/introduction-to-topic-modeling/

- https://en.wikipedia.org/wiki/Topic_model

- https://zhuanlan.zhihu.com/p/31470216

  

  


