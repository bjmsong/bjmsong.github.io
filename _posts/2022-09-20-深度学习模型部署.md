---
layout:     post
title:      深度学习模型部署
subtitle:   
date:       2022-09-20
author:     bjmsong
header-img: 
catalog: true
tags:
    - 模型优化与部署
---
>对推理资源、速度的优化，可以大大拓展AI模型的应用范围：不需要很强大的算力、资源支持也可以部署模型

## 平台
- 服务端：大模型，延迟不敏感
- 移动端/嵌入式/边缘端：小模型，延迟/资源敏感
    - Jetson Nano
    https://github.com/dusty-nv/jetson-inference
    https://www.bilibili.com/video/BV1g44y1J7zS/?spm_id_from=333.337.search-card.all.click&vd_source=7798c62f92ce545f56fd00d4daf55e26
    https://zhuanlan.zhihu.com/p/319292104
    https://aistudio.baidu.com/aistudio/projectdetail/3451173
    https://blog.csdn.net/ting_qifengl/article/details/111246874
    https://www.cxyzjd.com/article/qianbin3200896/108949723
    https://space.bilibili.com/649901866/channel/seriesdetail?sid=214107
    - 树莓派
    https://www.bilibili.com/video/BV1Mv4y1w7Pd/?spm_id_from=333.337.search-card.all.click&vd_source=7798c62f92ce545f56fd00d4daf55e26
    https://qengineering.eu/deep-learning-with-raspberry-pi-and-alternatives.html
    https://blog.csdn.net/ZhaoDongyu_AK47/article/details/106059943
    https://www.bilibili.com/video/BV1RV411x7BN/?spm_id_from=333.337.search-card.all.click&vd_source=7798c62f92ce545f56fd00d4daf55e26

## 方式
- 原始训练框架
    - tf-serving、torchserve
    - 需要安装整个框架
    - 推理性能差
    - 很多冗余功能
    - 内存占用大
- 训练框架的部署引擎
    - tf-lite，TFLite Micro，pytorch-mobile
    - 只支持自己框架训练的模型
    - 支持硬件/OS有限
- 手动模型重构
    - 手写c++，实现计算图，导入权重
    - 有一定技术难度
- 高性能移动端推理引擎
    - 可以直接加载主流深度学习框架的模型文件，无需进行模型转换
    - 只依赖C/C++库，无任何第三方依赖
    - 支持Android/Linux/RTOS/裸板环境
    - API:Python/C等

## 深度学习框架 -> 中间表示 -> 推理引擎
- 中间表示：ONNX，torchscript
    - 只描述网络结构
    - 一些针对网络结构的优化会在中间表示上进行
- 推理引擎：TensorRT，OpenVINO，NCNN
    - 用面向硬件的高性能编程框架(如 CUDA，OpenCL）编写，能高效执行深度学习网络中算子
- ONNX(Open Neural Network Exchange)
    - 对不同框架和不同的移动端、边缘设备Runtime进行了标准化，从而降低了模型在不同框架和不同 Runtime 之间转换的成本，现在主流的框架和设备都支持ONNX
    - https://github.com/onnx/onnx
- https://pytorch.org/docs/stable/onnx.html
    - Tracing
        - executes the model once with the given args and records all operations that happen during that execution
        - If your model is dynamic, e.g., changes behavior depending on input data, the exported model will not capture this dynamic behavior.
        - If you want to export your model with dynamic control flow, you will need to use scripting.
    - Scripting
    - Pitfall(陷阱)
        - Avoid NumPy and built-in Python types
        - Avoid Tensor.data
        - Avoid in-place operations when using tensor.shape in tracing mode
    - Limitations
        - Types
        - Differences in Operator Implementations
        - Unsupported Tensor Indexing Patterns
    - Adding support for operators
- https://zhuanlan.zhihu.com/p/477743341
    - 通常只用 ONNX 表示更容易部署的静态图
        - PyTorch 提供了一种叫做追踪（trace）的模型转换方法：给定一组输入，再实际执行一遍模型，即把这组输入对应的计算图记录下来，保存为 ONNX 格式
    - ONNX Runtime
        - 推理引擎: 跨平台机器学习推理加速器
    - 自定义算子
        - PyTorch 转 ONNX，实际上就是把每个 PyTorch 的操作映射成了 ONNX 定义的算子
        - 很多场景下，需要支持模型的动态化：为了让模型的泛用性更强，部署时需要在尽可能不影响原有逻辑的前提下，让模型的输入输出或是结构动态化
        - 例如：现有实现插值的 PyTorch 算子有一套规定好的映射到 ONNX Resize算子的方法，这些映射出的Resize算子的 scales只能是常量，无法满足需求。需要自己定义一个实现插值的PyTorch算子，然后让它映射到一个我们期望的 ONNX Resize 算子上。
- https://zhuanlan.zhihu.com/p/396781295

### ONNX
https://onnx.ai/onnx/intro/
https://github.com/onnx/onnx
https://www.bilibili.com/video/BV14L411L7QR/?spm_id_from=333.337.search-card.all.click&vd_source=7798c62f92ce545f56fd00d4daf55e26
- 历史
    - 最初作者是贾扬清
    - 发起原因是pytorch，cntk等当时（2017年）比较小的框架，想联合对抗tensorflow
    - 但是后来pytorch做大做强了，就不太愿意带上onnx了，后面自己开发了torchscript
        - pytorch对onnx的支持目前是微软的人在维护
    - 公司不太支持了，名义上托管到开源基金会了
- 理想很美好：深度学习框架 -> 中间表示(onnx) -> 推理引擎
- 现实中还有各种其它方案：TorchScript，Pytorch->PNNX->NCNN 。。。
- 存在的问题
    - ProtoBuffer作为模型表达的文件格式：二进制文件，对数据传输友好，可读性不强，直接改不方便
    - 同一个算子用很多参数去兼容不同的框架:增加了下游厂商开发的难度
    - 新的算子跟进不够及时
    - 对社区需求响应不够积极
- ONNX Runtime
https://github.com/microsoft/onnxruntime

### torchscript
- Torch Script reference
    - https://pytorch.org/docs/stable/jit.html
- https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html
    - code: pytorch_in_action/torchScript
    - an intermediate representation(IR,计算图) of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++.
    - TorchScript provides tools to capture the definition of model, even in light of the flexible and dynamic nature of PyTorch
        - Tracing Modules
            - recorded the operations that occured when the Module was run, and created an instance of torch.jit.ScriptModul，which includes its definitions in an Intermediate Representation (or IR), commonly referred to in Deep learning as a graph
            - why did we do all this ?
                - TorchScript code can be invoked in its own interpreter, which is basically a restricted Python interpreter. This interpreter does not acquire the Global Interpreter Lock, and so many requests can be processed on the same instance simultaneously.
                - This format allows us to save the whole model to disk and load it into another environment, such as in a server written in a language other than Python
                - TorchScript gives us a representation in which we can do compiler optimizations on the code to provide more efficient execution
                - TorchScript allows us to interface with many backend/device runtimes that require a broader view of the program than individual operators.
        - Using Scripting (compiler) to Convert Modules
            - direct analysis of Python source code to transform it into TorchScript
        - Mixing Scripting and Tracing
- LOADING A TORCHSCRIPT MODEL IN C++
    - https://pytorch.org/tutorials/advanced/cpp_export.html
    - Step 1: Convert PyTorch Model to Torch Script
    - Step 2: Serialize Script Module to a File
    - Step 3: Load Script Module in C++
    - Step 4: Executing the Script Module in C++    
- REAL TIME INFERENCE ON RASPBERRY PI 4 (30 FPS!)
    - https://pytorch.org/tutorials/intermediate/realtime_rpi.html
- Production Inference Deployment with PyTorch
    - https://www.youtube.com/watch?v=Dk88zv1KYMI&ab_channel=PyTorch
    - Evaluation mode
        - Turn off autograd: computation history tracking can be expensive
        - Affects the activity of certain layers with training-only activity
            - Dropout is only active during training
            - BatchNorm only tracks running mean & variance stats during training

        ```python
        model.eval()
        # or
        model.train(False)
        ```

    - torchscript
        - a statically-typed subset of Python meant for ML
        - meant for consumption by the Pytorch Just-in-time Compiler(JIT), which performs run-time optimizations on your TorchScript model
        - the perferred method for serializing your trained model and deploy it for production inference
    - Pytorch JIT: an optimizing just-in-time compiler for PyTorch programs
        - Lightweight,thread-safe interpreter
        - Easy to write custom transformations
        - Runtime optimizaitons: operator fusion, loop unrolling, batch matrix multiplicaiton
    - torchscript and C++
    - deployment with torchserve
- TorchScript and PyTorch JIT | Deep Dive
    - https://www.youtube.com/watch?v=2awmrMRf0dA&ab_channel=PyTorch
    - why do people like pytorch?
        - Simple: models are object oriented Python programs 
        - Debuggable: print,pdb,REPL(交互式解释器)
        - Hackable: use any Python library
    - production requirement
        - portability: C++ servers, mobile ...
        - performance: inference latency, throughput
    - Tools to transition from eager to script
        - eager mode: for prototyping, training, experimenting
        - script mode: for production deployment
    - optimization
        - Algebraic(代数) rewriting
            - constant folding: graph中常量的计算合并起来
            - common subexpression elimination
            - dead code elimination
        - out-of-order execution
            - re-order operations to reduce memory pressure and make efficient use of cache locality
        - Fusion
            - combine several operators into a single kernel to avoid overheads from round-trips to memory,PCIe,etc
        - target-dependent code generation
            - taking parts of the program and compiling them for specific hardware 
        - dynamism in script mode
            - whenever you have something can be dynamic, but is likely static, just-in-time compilation may be useful
        - JIT的步骤
            - collect statistics and information that are happening in the program at runtime
                - e.g.: shape of tensor, device(host or device?), requires_grad?
            - optimization
                - fusion
                - Algebraic rewriting
                - loop unrolling
                - code generation
            - execution
                - scheduling
                - parallelism
- https://pytorch.org/blog/optimizing-cuda-rnn-with-torchscript/
    - 
- https://zhuanlan.zhihu.com/p/486914187
- https://www.zhihu.com/question/526092590
    - torchscript本质上是一门特殊语言。它的解释器可以把python代码翻译成一个静态计算图。这个torchscript模型不需要python即可运行。
    - 当你开始使用torchscript，痛苦就来了：它只支持部分python语法。你的模型在python运行时下原本一切正常，喂给torchscript之后很可能要报错。你必须对照它的语法规则，用等效的表达改写原来的模型。

### torchserve
https://pytorch.org/serve/
https://github.com/pytorch/serve
https://www.youtube.com/watch?v=XlO7iQMV3Ik&ab_channel=PyTorch
https://blog.csdn.net/qq_28613337/article/details/110438060


### PNNX
- https://www.bilibili.com/video/BV1Uv411u78D/?spm_id_from=333.999.0.0&vd_source=7798c62f92ce545f56fd00d4daf55e26
- op定义跟pytorch保持完全一致
- 模型的图是文本文件，可以直接修改
- PNNX -> torchscript -> 推理引擎

## PYTORCH MOBILE
https://pytorch.org/mobile/home/
https://www.youtube.com/watch?v=THSt5sCYzRs&ab_channel=PyTorch

## 原始训练框架
### DEPLOYING PYTORCH IN PYTHON VIA A REST API WITH FLASK 
- 不适用于生产环境
    - pytorch模型未经优化，推理较慢
    - flask的WSGI不适合生产环境，需要配合一个高性能的WSGI服务
- https://pytorch.org/tutorials/intermediate/flask_rest_api_tutorial.html
- https://www.bilibili.com/video/BV1Qv41117SR/?spm_id_from=333.337.search-card.all.click&vd_source=7798c62f92ce545f56fd00d4daf55e26
- code: pytorch_in_action/flask
- https://github.com/L1aoXingyu/deploy-pytorch-model
- https://blog.keras.io/building-a-simple-keras-deep-learning-rest-api.html
- https://www.kdnuggets.com/2019/03/deploy-pytorch-model-production.html
- https://blog.csdn.net/dou3516/article/details/82912458


### playtorch: for mobile
https://github.com/facebookresearch/playtorch

### tf-serving
- https://tensorflow.google.cn/tfx/guide/serving
- https://www.jianshu.com/p/2fffd0e332bc
- [深度学习在美团的工程实践](https://mp.weixin.qq.com/s?__biz=MjM5NjQ5MTI5OA==&mid=2651749161&idx=1&sn=2a34bc1e36fbf259ac63ffe7c94f528b&chksm=bd12a2648a652b72a9027a572a0038d3b8a6448949c53e7aefab617a12811da84efdc88bcd29&mpshare=1&scene=1&srcid=1025pjqMWAMqES7L6Vsb6QTE#rd)

## tf-lite，pytorch-mobile



## 使用React和Flask创建一个完整的机器学习Web应用程序
https://towardsdatascience.com/deploying-a-deep-learning-model-on-mobile-using-tensorflow-and-react-4b594fe04ab
https://python.plainenglish.io/how-to-build-a-predictive-machine-learning-site-with-react-and-python-part-three-frontend-72c063e8716e
https://cloud.tencent.com/developer/article/1449685

## 
https://zhuanlan.zhihu.com/p/477743341
https://zhuanlan.zhihu.com/p/477743341
https://www.zhihu.com/column/c_1497987564452114432
https://mp.weixin.qq.com/s/1xcjOcdZGOOQtSbHa0K20g

- 端侧AI框架加速优化方法
    - 编译优化、缓存优化、多线程、稀疏存储和计算、NEON指令应用、算子优化等
- 系统优化: 指在特定系统平台上，通过Runtime层面性能优化，以提升AI模型的计算效率
    - Op-level的算子优化：FFT Conv2d (7x7, 9x9), Winograd Conv2d (3x3, 5x5) 等；
    - Layer-level的快速算法：Sparse-block net [1] 等；
    - Graph-level的图优化：BN fold、Constant fold、Op fusion和计算图等价变换等；
    - 优化工具与库（手工库、自动编译）：TensorRT (Nvidia), MNN (Alibaba), TVM (Tensor Virtual Machine), Tensor Comprehension (Facebook) 和OpenVINO (Intel) 等；
    

https://blog.csdn.net/lixuejun2008/article/details/103897626
https://zhuanlan.zhihu.com/p/534708101
https://codeantenna.com/a/yyqRoA11ED



## 代码
https://github.com/open-mmlab/mmdeploy
https://github.com/DefTruth/lite.ai.toolkit
https://github.com/jjw-DL/Model_Deployment
https://github.com/CYang828/dl-deploy
https://github.com/PaddlePaddle/FastDeploy
    - https://mp.weixin.qq.com/s/oHZ7l71RwVA_VdoQyeZDag

## Triton
- https://www.zhihu.com/question/480170452/answer/2064096810
- https://openai.com/blog/triton/


## 参考资料
- b站：有三AI
- https://zhuanlan.zhihu.com/p/367042545?utm_campaign=shareopn&utm_medium=social&utm_oi=30249563717632&utm_psn=1565442828280557568&utm_source=wechat_session
https://www.zhihu.com/question/411393222?utm_campaign=Sharon&utm_medium=social&utm_oi=30249563717632&utm_psn=1565442722865135616&utm_source=wechat_session&utm_content=group3_supplementQuestions
https://www.zhihu.com/question/329372124?utm_campaign=shareopn&utm_medium=social&utm_oi=30249563717632&utm_psn=1565442331993591808&utm_source=wechat_session&utm_content=group3_questions
https://www.zhihu.com/question/428800593/answer/1560594742
https://www.zhihu.com/column/c_1497987564452114432
https://www.zhihu.com/column/c_1359601708180529152
https://zhuanlan.zhihu.com/p/82540025