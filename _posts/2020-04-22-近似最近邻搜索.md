---
layout:     post
title:      近似最近邻搜索
subtitle:   
date:       2020-04-22
author:     bjmsong
header-img: img/IR/ir.jpg
catalog: true
tags:
    - 信息检索
---
>无论是在学界还是工业界，`向量索引`是一个研究得比较多的问题，学术上对应的专有名词叫`Approximate Nearest Neighbor Search (ANNS)`，即`近似最近邻搜索`。为什么是近似，而不是我们想要的精确？这就是精度与时间、算力资源的折中，采用了牺牲精度换取时间和空间的方式，从海量的样本中实时获取跟查询最相似的样本。


### 概览
- 向量索引是指通过某种数学量化模型，对向量构建一种时间和空间都比较高效的数据索引结构，使得我们能够实时地获取跟查询向量尽可能最相近的K个向量。从定义可以看到，要设计一种高效的向量索引模型，应该满足3个基本条件，即：
    - 实时查询，支持海量（百亿、千亿级别）规模库量级的实时查询；
    - 存储高效，要求构建的向量索引模型数据压缩比高，达到大幅缩减内存使占用的目的；
    - 召回精度好，top@K有比较好的召回率，跟暴力搜索（brute-force search）的结果相比，暴力搜索的召回，是ANNS召回率的上限；
- 最邻近搜索问题在很多领域中都有应用，包括：
    - 模式识别，特别是光学字符识别; 
    - 统计分类，参见KNN（k-nearest neighbor algorithm); 
    - 计算机视觉;数据库，如基于内容的图像检索; 
    - 编码理论，见最大似然编码 ; 
    - 数据压缩，见MPEG-2标准; 
    - 向导系统; 
    - 网络营销; 
    - DNA测序; 
    - 拼写检查，建议正确拼写;
    - 剽窃侦查; 
    - 相似比分算法，用来推断运动员的职业表现
- 在具体到不同类的索引方法分类前，从宏观上对向量索引有下面一个直观的认识显得很有必要：**brute-force搜索的方式是在全空间进行搜索，为了加快查找的速度，几乎所有的ANNS方法都是通过对全空间分割，将其分割成很多小的子空间，在搜索的时候，通过某种方式，快速锁定在某一（几）子空间，然后在该（几个）子空间里做遍历。**可以看到，正是因为缩减了遍历的空间大小范围，从而使得ANNS能够处理大规模数据的索引。
- 可以将向量索引方法分为四大类：**基于树的方法、哈希方法、矢量量化方法、图索引量化方法**。这四大类方法里面，本篇文章着重总结典型方法，其中哈希方法以`LSH`、矢量量化方法以`PQ/OPQ/IVFOPQ`、图索引量化方法以`HNS`W为典型代表，分别进行重点介绍



### 基于树的方法
- 基于树的方法采用树这种数据结构的方法来表达对全空间的划分，其中又以KD树最为经典

#### KD树
<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/IR/anns/kd树.png) 
</li> 
</ul> 

- 将查找空间不断将父节点包含的区域分为相邻的两部分，每部分包含原来区域中的一半点，从方差最大的那个维度开始划分
    - 为何要选择方差作为维度划分选取的标准？我们都知道，方差的大小可以反映数据的波动性。方差大表示数据波动性越大，选择方差最大作为划分空间标准的好处在于，可以使得所需的划分面数目最小，反映到树数据结构上，可以使得我们构建的KD树的树深度尽可能的小
- 查找时间复杂度为O（log N）
- 一般而言，在空间维度比较低的时候，KD树是比较高效的，当空间维度较高时，可以采用哈希方法或者矢量量化方法


#### Annoy
- Annoy是Erik Bernhardsson写的一个以树为作为索引结构的近似最近邻搜索库，并用在Spotify的推荐系统中。
- Annoy的核心是不断用选取的两个质心的法平面对空间进行分割，最终将每一个划分的子空间里面的样本数据限制在K以内。对于待插入的样本xi，从根节点依次使用法向量跟xi做内积运算，从而判断使用法平面的哪一边（左子树or右子树）。对于查询向量qi，采用同样的方式（在树结构上体现为从根节点向叶子节点递归遍历），即可定位到跟qi在同一个子空间或者邻近的子空间的样本，这些样本即为qi近邻。
- 为了提高查询召回率，Annoy采用建立多棵子树的方式，这种方式其实有点类似AdaBoost的意思，即用多个弱分类器构成强单元分类器，NV-tree也采用了这种方式，哈希方法也同样采用了这种方式（构建多表）的方式

### 哈希方法
- 通过哈希函数把向量x变换成二值码（海明码）b(例如0101110101011100)，然后将距离dist(x_1,x_2)近似成二值码距离（海明距离）dist(b_1,b_2)。二值码距离可以通过popcount快速计算
- 根据学习的策略，可以将哈希方法分为无监督、有监督和半监督三种类型
- 工程中在要使用到哈希方法的场景下一般都会选用的局部敏感哈希

#### 局部敏感哈希(Local Sensitive Hashing，LSH)
- 对目标进行多次哈希处理，使得相似项会比不相似项更可能哈希到同一个桶中。然后将至少有一次哈希到同一个桶中的文档看做是候选对
- 具体做法是:
    - 在Min Hashing所得的signature向量的基础上，将每一个向量分为几段，称之为band
    - 对各个用户的signature向量在每一个band上分别进行哈希分桶，在任意一个band上被分到同一个桶内的用户就互为candidate相似用户，这样只需要计算所有candidate用户的相似度就可以找到每个用户的相似用户群了
        - 其基本想法是：如果两个向量的其中一个或多个band相同，那么这两个向量就可能相似度较高；相同的band数越多，其相似度高的可能性越大。
- ![spark的实现](https://spark.apache.org/docs/latest/ml-features#locality-sensitive-hashing)

#### 开源实现
- LSHash
- FALCONN


### 矢量量化方法
- 通过聚类把向量集聚成若干类，每类里面的向量用对应的类中心来近似。这样子，每个向量只需要用其对应的聚类中心的索引ID来表示，其与查询向量间的距离用其对应的聚类中心与查询向量间的距离来近似。
- 乘积量化（PQ）
- 倒排乘积量化（IVFPQ）

### 图索引量化方法
- 将图引入向量索引的方法
- HNSW
- 开源实现
    - https://github.com/Microsoft/SPTAG


### 其它开源工具
#### faiss
- https://blog.csdn.net/kanbuqinghuanyizhang/article/details/80774609
- https://time.geekbang.org/column/article/7204

```python
import numpy as np

d = 64                           # dimension
nb = 100000                      # database size
nq = 10000                       # nb of queries
np.random.seed(1234)             # make reproducible
xb = np.random.random((nb, d)).astype('float32') # 训练数据
xb[:, 0] += np.arange(nb) / 1000.
xq = np.random.random((nq, d)).astype('float32') # 查询数据
xq[:, 0] += np.arange(nq) / 1000.

# 创建索引,faiss创建索引对向量预处理，提高查询效率
index = basic_faiss.IndexFlatL2(d)   # build the index
print(index.is_trained)

index.add(xb)                  # add vectors to the index
print(index.ntotal)

# 传入搜索向量查找相似向量
k = 4                          # we want to see 4 nearest neighbors
D, I = index.search(xq, k)     # actual search
print(I[:5])                   # neighbors of the 5 first queries
print(D[-5:])                  # neighbors of the 5 last queries
```


#### nmslib

#### KGraph

#### Milvus 


### 参考资料
- 局部敏感哈希
    - https://zhuanlan.zhihu.com/p/46164294
    - https://www.cnblogs.com/maybe2030/p/4953039.html
    - https://zhuanlan.zhihu.com/p/80638247
- https://yongyuan.name/blog/vector-ann-search.html
- https://www.msra.cn/zh-cn/news/features/approximate-nearest-neighbor-search
- https://www.jiqizhixin.com/graph/technologies/2ece0beb-329e-4700-87b9-92af8d21d2b7
- https://scikit-learn.org/stable/modules/neighbors.html
- https://zh.wikipedia.org/wiki/%E6%9C%80%E9%82%BB%E8%BF%91%E6%90%9C%E7%B4%A2
- An Investigation of Practical Approximate Nearest Neighbor Algorithms
