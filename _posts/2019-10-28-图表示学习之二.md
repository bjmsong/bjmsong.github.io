---
layout:     post
title:      图表示学习（Graph Embedding）之二
subtitle:   经典算法介绍
date:       2019-10-28
author:     bjmsong
header-img: img/Graph/kg.jpg
catalog: true
tags:
    - 图算法
---
>本文将介绍图表示学习的几种经典算法，包括DeepWalk、LINE、GNN等。

<ul> 
<li markdown="1"> 
这幅图展示了图表示学习这个领域算法的发展脉络
![]({{site.baseurl}}/img/Graph/network embedding/study.jpg) 
</li> 
</ul> 


<ul> 
<li markdown="1"> 
大体上可以将图表示方法分为三大类：( 1 )基于因子分解的方法，( 2 )基于随机游走的方法，以及( 3 )基于深度学习的方法。
![]({{site.baseurl}}/img/Graph/network embedding/图表示学习方法分类.jpg) 
</li> 
</ul> 


本文介绍其中的几种算法。

### DeepWalk
<ul> 
<li markdown="1"> 
首先来介绍2014年提出来的DeepWalk，其思想源于自然语言处理领域的word2vec。这两件事情确实是有很多相似的地方，在图中经过一定距离的随机游走，所经过的节点是满足幂律（或者说长尾）分布的，如左图所示。同样，在自然语言处理中，词的分布也满足幂律分布，如右图所示。
![]({{site.baseurl}}/img/Graph/network embedding/law distribution.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
word2vec通过语料构建语言模型，得到词的向量化表示。
![]({{site.baseurl}}/img/Graph/network embedding/w2v.jpg) 
</li> 
</ul> 


<ul> 
<li markdown="1"> 
我们知道，wordvec训练需要语料，那么DeepWalk的语料从何而来呢？答案是：随机游走
DeepWalk分为两个阶段，第一个阶段，在图上进行固定距离的随机游走，通过这种方式采样，得到节点序列。
![]({{site.baseurl}}/img/Graph/network embedding/deepwalk.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
第二个阶段，借鉴word2vec里的SkipGram算法，计算每一个节点上下文的似然概率，极大化似然概率，最后得到节点的向量化表示。
![]({{site.baseurl}}/img/Graph/network embedding/Representation Mapping.jpg) 
</li> 
</ul> 



### LINE

第二个要介绍的算法是LINE，全名叫做Large-scale Information Network Embedding。顾名思义，这个算法可以应用在大规模的数据场景中。


#### LINE的优点


<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/Graph/network embedding/LINE优点.jpg) 
</li> 
</ul> 


#### LINE的原理

<ul> 
<li markdown="1"> 
对于由边（u，v）连接的每一对顶点，边上的权重wuv表示u和v之间的相似度，如果在u和v之间没有观察到边，则它们的一阶相似度为0。一阶邻近通常意味着现实世界网络中两个节点的相似性。例如，在社交网络中相互交友的人往往有着相似的兴趣;在万维网上相互链接的页面倾向于谈论类似的话题
![]({{site.baseurl}}/img/Graph/network embedding/一阶相似度.jpg) 
</li> 
</ul> 


<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/Graph/network embedding/二阶相似度.jpg) 
</li> 
</ul> 


<ul> 
<li markdown="1"> 
LINE设计了同时保留一阶相似度和二阶相似度的目标函数
![]({{site.baseurl}}/img/Graph/network embedding/如何保留一阶相似度.jpg) 
</li> 
</ul> 



### GNN（图神经网络）

<ul> 
<li markdown="1"> 
最近几年，图神经网络是一个比较热门的研究领域。下面这张图是近些年GNN论文发表数量的情况，我们可以看到从2016年开始，GNN论文发表数量呈指数上升趋势。
![]({{site.baseurl}}/img/Graph/network embedding/gnn paper.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
GNN结合了信息传播机制和神经网络，核心思想是信息在网络上传播，可以改变相邻节点的状态，直到达到平衡。先定义一些符号，点跟边的属性用l表示，每个节点的状态用x表示。每个顶点的状态，受跟它相邻的边，相邻的顶点影响。
![]({{site.baseurl}}/img/Graph/network embedding/gnn1.jpg) 
</li> 
</ul> 


<ul> 
<li markdown="1"> 
参数函数fw称为局部变换函数，描述了节点和其邻域的依赖性。gw称为局部输出函数，刻画了输出值的生成过程。
![]({{site.baseurl}}/img/Graph/network embedding/gnn公式.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
通过经典的迭代方式对状态进行计算，函数fw和gw可以使用前馈神经网络实现。
![]({{site.baseurl}}/img/Graph/network embedding/gnn公式2.jpg) 
</li> 
</ul> 

下面两幅图形象地展示了具体过程。


<ul> 
<li markdown="1"> 
上面那个是原图，下面那个是与之对应的图传播机制的展示，把每个节点表示成fw和gw的计算过程，fw和gw都可以用前向神经网络来表示。
![]({{site.baseurl}}/img/Graph/network embedding/gnn2.jpg) 
</li> 
</ul> 


<ul> 
<li markdown="1"> 
然后，这幅图把节点状态的更新过程通过循环神经网络来表示，每一层代表某一个时刻所有节点的状态，层与层之间的连接由节点之间的连接决定。
![]({{site.baseurl}}/img/Graph/network embedding/gnn3.jpg) 
</li> 
</ul> 



### 参考资料
- Tutorial on Graph Representation Learning, AAAI 2019
- CNCC报告 唐杰 
https://www.aminer.cn/cncc19-classicml
- 《DeepWalk: Online Learning of Social Representations》
- LINE 
《LINE: Large-scale Information Network Embedding》
https://www.jianshu.com/p/8bb4cd0df840
- GNN
《The graph neural network model》
https://blog.csdn.net/u011748542/article/details/86289511
https://towardsdatascience.com/graph-neural-networks-20d0f8da7df6
https://zhuanlan.zhihu.com/p/75307407
- 香侬科技 吴天龙 图神经网络分享
https://github.com/talorwu/Graph-Neural-Network-Review



