---
layout:     post
title:      推荐系统之bandit
subtitle:   
date:       2019-10-24
author:     bjmsong
header-img: img/Recommendation System/bandit.jpg
catalog: true
tags:
    - 推荐系统
---

### 问题定义
bandit，或者MAB，即多臂老虎机问题（multi-armed bandit）。问题的提出和研究最早可以追述到上世纪三十年代，该问题可以使用一种较为形象的场景进行解释：

<ul> 
<li markdown="1"> 
假设你进入了一家赌场，这家赌场的大厅里有n个老虎机，当你往老虎机中投入一枚硬币，老虎机有一定概率掉落奖励，每个老虎机掉落奖励的概率不同且未知，那么在现有硬币有限的情况下，使用什么策略才能使你最后所得的总体奖励最大？
![]({{site.baseurl}}/img/Recommendation System/多臂老虎机.jpg) 
</li> 
</ul> 

在尝试了几次后，你对每个赌博机的掉落概率有了一个初步的估计，但是受尝试次数的限制，这个估计可能是不准确的，且未来该概率还有可能改变。接下来，是选择一直坚持目前已知的最好选择（exploitation，守成），还是继续尝试，以达到更准确的估计，找到奖励概率更高的老虎机（exploration，探索），这便是多臂老虎机理论解决的问题。通过一系列在线学习反馈算法，平衡守成与探索的比例，以达到最后总体收益的最大化。

实质是一类简化的强化学习问题，也是在线学习问题。



### 应用场景

- 金融领域的资产组合
- 动态规划网络路径使得网络的延迟最小
- **假设一个用户对不同类别的内容感兴趣程度不同，那么我们的推荐系统初次见到这个用户时，怎么快速地知道他对每类内容的感兴趣程度？这就是推荐系统的冷启动。**
- 假设我们有若干广告库存，怎么知道该给每个用户展示哪个广告，从而获得最大的点击收益？是每次都挑效果最好那个么？那么新广告如何才有出头之日？
- 我们的算法工程师又想出了新的模型，有没有比AB test更快的方法知道它和旧模型相比谁更靠谱？
- 。。。

**特别提出，在计算广告和推荐系统领域，针对这个问题，还有个说法叫做EE问题：exploit－explore问题。**

>exploit意思就是：比较确定的兴趣，当然要用啊。好比说我们已经挣到的钱，当然要花啊；

>explore意思就是：不断探索用户新的兴趣才行，不然很快就会出现一模一样的反复推荐。就好比我们虽然有一点钱可以花了，但是还得继续搬砖挣钱啊，不然花完了喝西北风啊。

**一切通过数据收集而得到的概率预估任务，都能通过Bandit系列算法来进行在线优化。这里的“在线”，指的不是互联网意义上的线上，而是指算法模型参数根据观察数据不断演变。**



### 准备

#### 如何衡量bandit算法

T轮过后的累计遗憾

$$\rho=T\mu^*-\sum^T_{t=1}{\hat{r_t}}$$

其中，$$\mu^*$$是最大的回报的平均值，$$\hat{r_t}$$是每一轮的实际回报



#### Bernoulli multi-armed bandit

一种典型的场景，每一轮都有概率p得到reward为1,1-p的概率得到0。



### 策略

#### Random
每次随机选择一枚硬币进行投掷。如果不能胜过这个策略，就不必玩了。



#### Naive

先给每个硬币一定次数的尝试，比如每个硬币掷10次，根据每个硬币正面朝上的次数，选择正面频率最高的那个硬币，作为最佳策略。这也是大多人能想到的方法。

但是这个策略有几个明显问题：

- 10次尝试真的靠谱吗？最差的硬币也有可能在这10次内有高于最好硬币的正面次数。

- 假设你选到的这个硬币在投掷次数多了后发生了问题（比如掉屑），改变了其属性，导致其正面的概率大大降低，如果你还死守着它，那不是吃大亏了？（这是对变量的考虑）

- 就算你给一个硬币10次机会，如果硬币真的很多，比如K>100，给每个硬币10次机会是不是也太浪费了呢？等所有硬币都尝试过，再回来“赚钱”，花儿都谢了！



#### epsilon-Greedy

有了前两个垫背，可以开始让Bandit登场了。ε-Greedy就是一种很机智的Bandit算法：它让每次机会以ε的概率去“探索”，1-ε的概率来“开发”。也即，如果一次机会落入ε中，则随机选择一个硬币来投掷，否则就选择先前探索到正面概率最大的硬币。这个策略有两个好处：

- 它能够应对变化，如果硬币“变质”了，它也能及时改变策略。
- ε-Greedy机制让玩的过程更有趣，有时“探索”，有时“赚钱”。

在此基础上，又能引申出很多值得研究的问题，比如ε应该如何设定呢？它应不应该随着时间而变？因为随着探索次数的增多，好的选择自然浮现得比较明显了。ε大则使得模型有更大的灵活性（能更快的探索到未知，适应变化），ε小则会有更好的稳定性（有更多机会去“开发”）。



#### Thompson Sampling（1933）

每个臂是否产生收益的概率 p 的背后都对应一个 beta 分布。我们将 beta 分布的 α 参数看成是推荐后用户的点击次数， β 参数看成是推荐后用户未点击的次数。

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/Statistics/beta.png) 
</li> 
</ul> 

来看下使用汤普森算法的流程：

1. 每个臂都维护一个 beta 分布的参数，获取每个臂对应的参数 α 和 β，然后使用 beta 分布生成随机数；
2. 选取生成随机数最大的那个臂作为本次结果；
3. 观察用户反馈，如果用户点击则将对应臂的 α 加 1，否则 β 加 1。
在实际的推荐系统中，需要为每个用户保存一套参数，假设有 m 个用户， n 个臂（选项，可以是物品，可以是策略）， 每个臂包含 α 和 β 两个参数，所以最后保存的参数的总个数是 2 m n。

可以直观的理解下为什么汤普森采样算法有效：

- 当尝试的次数较多时，即每个臂的 α + β 的值都很大，这时候每个臂对应的 beta 分布都会很窄，也就是说，生成的随机数都非常接近中心位置，每个臂的收益基本确定了。
- 当尝试的次数较少时，即每个臂的 α + β 的值都很小，这时候每个臂对应的 beta 分布都会很宽，生成的随机数有可能会比较大，增加被选中的机会。
- 当一个臂的 α + β 的值很大，并且 α/(α + β) 的值也很大，那么这个臂对应的 beta 分布会很窄，并且中心位置接近 1 ，那么这个臂每次选择时都很占优势。
```
import numpy as np
import pymc
# wins 和 trials 都是一个 N 维向量，N 是臂的个数
# wins 表示所有臂的 α 参数，loses 表示所有臂的 β 参数
choice = np.argmax(pymc.rbeta(1 + wins, 1 + loses, len(wins)))
wins[choice] += 1
loses[choice] += 1
```



#### UCB(2002)

UCB 算法全称是 Upper Confidence Bound，即置信区间上界。它是计算每个臂的平均收益与该收益的不确定性来作为最终得分。公式如下；

$$\overline{x_i(t)}+\sqrt{\frac{2*lnt}{T_{i,t}}}$$

i 表示当前的臂，t 表示目前的尝试次数，Ti,t 表示臂 i 被选中的次数。公式加号左边表示臂 i 当前的平均收益，右边表示该收益的 Bonus ，本质上是均值的标准差，反应了候选臂效果的不确定性，就是置信区间的上边界。

使用 UCB 算法的流程如下：

- 对所有臂先尝试一次
- 按照公式计算每个臂的最终得分
- 选择得分最高的臂作为本次结果

直观理解下 UCB 算法为什么有效？

- 当一个臂的平均收益较大时，也就是公式左边较大，在每次选择时占有优势
- 当一个臂被选中的次数较少时，即 Tit 较小，那么它的 Bonus 较大，在每次选择时占有优势

所以 UCB 算法倾向选择被选中次数较少以及平均收益较大的臂。

UCB 算法需要对所有的臂进行一次尝试，当臂比较多时，可能会比较耗时，如果 UCB 算法的参数是确定的，那么输出结果就是确定的，也就是说它本质上仍然是一个“确定性”的算法，这会导致它的 explore 能力受限。



#### LinUCB（linear UCB）

- 《A Contextual-Bandit Approach to Personalized News Article Recommendation》
- 结合上下文信息
- CB算法加入特征信息，可以实现个性化



#### 其它

- **COFIBA：Bandit和协同过滤结合**
- Maximal Marginal Relevance
    - 强行加入一些非相关的推荐
- Determinantal Point Process
- Social Curiosity Inspired Recommendation 



### 实现

- https://github.com/bgalbraith/bandits
- https://github.com/johnmyleswhite/BanditsBook



### 实际情况

- 除了 Bandit 算法之外，还有一些其他的探索兴趣的办法，比如在推荐时，随机地去掉一些用户历史行为（特征）。
- 解决兴趣探索，势必要冒险，势必要面对用户的未知，而这显然就是可能会伤害当前用户价值的：明知道用户肯定喜欢 A，你还偏偏以某个小概率给推荐非 A。
- **实际上，很少有公司会采用这些理性的办法做探索，反而更愿意用一些盲目主观的方式。**
- 究其原因，可能是因为：
  - 互联网产品生命周期短，而探索又是为了提升长期利益的，所以没有动力做；
  - 用户使用互联网产品时间越来越碎片化，探索的时间长，难以体现出探索的价值；
  - 同质化互联网产品多，用户选择多，稍有不慎，用户用脚投票，分分钟弃你于不顾；
  - 已经成规模的平台，红利杠杠的，其实是没有动力做探索的。
- 基于这些，我们如果想在自己的推荐系统中引入探索机制，需要注意以下几点：
  - 用于探索兴趣的物品，要保证其本身质量，纵使用户不感兴趣，也不至于引起其反感，损失平台品牌价值；
  - 探索兴趣的地方需要产品精心设计，让用户有耐心陪你玩儿；
  - 深度思考，这样才不会做出脑残的产品，产品不会早早夭折，才有可能让探索机制有用武之地。



### 参考资料

- Introduction to Bandits- Algorithms and Theory Part 1- Bandits with small sets of actions
- Introduction to Bandits- Algorithms and Theory Part 2- Bandits with large sets of actions
- A Contextual-Bandit Approach to Personalized News Article Recommendation
  - LinUCB
- Collaborative Filtering Bandits
  - COFIBA

- https://en.wikipedia.org/wiki/Multi-armed_bandit
- https://en.wikipedia.org/wiki/Thompson_sampling
- https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html
- https://zhuanlan.zhihu.com/p/21388070
- http://hunch.net/~coms-4771/lecture20.pdf
- http://blog.findshine.com/2015/01/03/bandit.html
- https://cloud.tencent.com/developer/article/1042606
- https://mp.weixin.qq.com/s?__biz=MzI2MDU3OTgyOQ==&mid=2247486157&idx=1&sn=45f5ed2f9a6def9743ce48d5c8fd45e1&chksm=ea66c257dd114b4178683d865fbcde06b800f9fc36f3e2539ddeec40d63ec4bc14ca5d1ab71f&scene=0&xtrack=1&key=92ab454e6cb49d97bfa2505796a9b8e1a2b96f9a6a3795235c97a307f8226696bf65bb7ba5f97605c6f74d6844a27f4d5739eec73e83ac6acd766c171ab5f05ba36644661956658f5c3016c50904125d&ascene=1&uin=MjM1OTMwMzkwMA%3D%3D&devicetype=Windows+7&version=62070141&lang=zh_CN&pass_ticket=WauZxOWQkYozzbjeSiIPkTivNfRRjdG7JbVr7yaLtIqtyjGbzDE0p97%2BlyDQdVLs
- 《Contextual Combinatorial Bandit and its Application on Diversified Online Recommendation》
- 《推荐系统三十六式》原理篇 MAB问题
