---
layout:     post
title:      新竹清华大学：并行计算与并行编程
subtitle:   Part III GPU Programming
date:       2022-11-16
author:     bjmsong
header-img: 
catalog: true
tags:
    - 并行计算
---
## 八: Heterogeneous Computing
### Heterogeneous Computing
- Heterogeneous Computing is an integrated system that consists of different types (programmable) computing units
    + DSP,FPGA,ASIC,GPU,Co-processor
- CPU
    + 容易编程
    + 计算能力弱
- Acclerator
    + 不容易编程：tool chain不成熟
    + 计算能力强
- 架构
    + 最常用：GPU Servers：same HW architecture as commodity server, but memory copy between CPU and GPU becomes the main bottleneck
        * GPU 通过PCIE和主板连接，GPU内部有内存&控制单元，CPU无法控制GPU，只是把数据放到内存中，GPU来取
    + Heterogeneous System Architecture (HSA)
        * Unified coherent memory
            - Single virtual memory address space
            - Prevent memory copy
        * Aim to provide a common system architecture for design high-level programming modeles for all devices
        * APU: AMD Accelerated Processing Unit

### GPU
- SIMD architecture
- Massively multithreaded manycore chips
- GPGPU: General-Purpose Graphic Processing Unit
    + Expose the horse power of GPUs for general purpose computations
        * exploit data parallelism for solving embarrssingly parallel tasks and numeric computations
    + Programmable
        * early: using the libraries in computing graphics, such as OpenGL and DirectX, to perform the tasks other than the original hardware designed for 
        * now: CUDA and openCL provides an extension to C and C++ that enables parallel programming on GPUs
            - OpenCL：跟OpenMP类似，可以自动把程序转换成GPU可执行的，比较容易上手，但是执行效率不高
            - CUDA：需要熟悉底层架构，才能较好地优化效率
- System Architecture
    + bottleneck
        * gpu和cpu通信带宽低
        * GPU memory size
- Manycore GPU - Block Diagram
    + processor hierachic
        * stream multi-processors (SM) -> multi core
    + memory hierachic
        * global memory(slow, but large&shared) -> PBSM/shared memory -> logic register(fast, but small & local)
    + Host传输指令，GPU Thread Execution Manager执行调度
- Stream Multiprocessor
    + Each SM is a vector machine 
    + Register & shared memory 共用同一块memory chip，只是访问限制不同 
    + shared memory带有L1 cache
    + hardware scheduling for thread execution and hardwre context switch 
- NVIDIA CUDA-Enabled GPUs Products：产品矩阵
    + 架构：Volta>Pascal>Maxwell>Kepler>...
    + 应用：Embedded(Deep Learning Inference),Consumer Desktop/Laptop(Visualization, single precision, GeForce系列),Profession workstation,Data Center(HPC, double precison, V100/H100)
    + 主要参数
        * #Cores
        * Core Clock
        * GPU memory bandwidth
        * GPU memory size：Shared Memory / L1 Cache Size / Registers
        * Interconnect Bandwidth(GPU之间)
            - NVLink >> PCIE
            - GPU和CPU之间由于产商不统一，通信比较难协调
        * Single Precision & Double Precision
            - GeForce系列的Single Precision比V100系统差距不大，Double Precision差距很大，深度学习很多时候不需要Double Precision，因此用GeForce系列性价比高
            - FLOPs就是每秒浮点操作数，其中单精度浮点可以用于深度学习模型训练。双精度浮点计算可以用于数值模拟工作
        * Compute Capability: Programming ability of a GPU device
            - 主要是适配CUDA的版本，CUDA版本越高，支持的功能越多，编程能力越强
        * Price
        * Power
        * https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/
- Architecture Roadmap：主要技术演进
    + Stacked DRAM: 3D，增加带宽
    + NVLink
    + CUDA版本更新
    + 芯片制程
    + Tensor Core
    https://blog.paperspace.com/understanding-tensor-cores/
    + 架构演进
    https://zhuanlan.zhihu.com/p/413145211


## 九: GPU Parallel Programming：CUDA
- CUDA Toolkits：Software Development Kit(SDK, 软件开发工具包) for CUDA Programming 
    + https://developer.nvidia.com/cuda-toolkit
    + C/C++ compiler: nvcc
    + debugging and optimization tools: IDE,Debugger,Profilers,Utilities(辅助工具)
    + GPU-accelerated libraries: BLAS, CUDA Device Runtime, FFT ...
    + Sample code
    + Documentaion
    
### Programm Model
- CUDA: Compute Unified Device Architecture
    + CUDA is a compiler and toolkit for programming NVIDIA GPUs
    + Enable heterogeneous computing and horsepower of GPUs
    + CUDA API extends the C/C++ language
    + Express SIMD parallelism
    + Given a high level abstraction from hardware
- CUDA program flow
    1. Copy data to GPU(device) memory from CPU(host) memory
    2. Call GPU functions
    3. GPU executes program
    4. Copy result from GPU memory to CPU memory
- CUDA Programming Model
    + Serial C code executes in a host(CPU) thread
    + Parallel kernel C codes execute in many device(GPU) threads across multiple processing elements
        * kernel有很多个block组成，block里面有很多个thread
        * 一个block对应一个Stream Multiprocessor
- CUDA program framework

```C
#include <cuda_runtime.h>
__global__ void my_kernel(...){
    ...
}
int main(){
    ...
    cudaMalloc();
    cudaMemcpy();
    ...
        my_kernel<<<nblock, blocksize>>>(...);
    ...
    cudaMemcpy();
    ...
}
```

- Kernel = Many Concurrent Threads
    + One kernel is executed at a time one the device
    + Many threads execute in each kernel
        * each thread executes the same code
        * ... on the different data based on its threadID
    + CUDA thread is Physical threads
        * GPU thread creation and context switching(线程间上下文切换) are essentially free
            - active concurrent threads 数量 >> gpu core 的数量 
            - 每个core有好几组register，每个thread有自己独立的register，上下文切换时不需要重新读写register（cpu需要）
                + 这样在一个thread读写memory的时候（通常耗时比较久），GPU可以计算下一个thread
- Hierarchy of Concurrent Threads
    + Threads are grouped into thread blocks
        * Kernel = grid of thread blocks
    + By definition, threads in the same block may synchronized with barriers, but not between blocks(效率很低，官方不支持这样做)
- Software Mapping
    + Software: grid/kernel -> blocks -> threads
        * 同一个blcok里面的threads可以通过share memory沟通，不同block的threads只能通过global memory沟通
    + Hardware: GPU(device) -> SM(multicore processor) -> core
- Block level Scheduling
    - Blocks are independent to each other to give scalability
        + A kernel scales across any number of parallel cores by scheduling blocks to SMs
- Thread level scheduling - Warp
    + Inside the SM, threads are launched in groups of 32(不同GPU可能会有不同), called warps
        * warps share the control part(wrap scheduler)
        * threads in a warp will be executing the same instruction(SIMD)
            - execute physically in parallel
- Thread group limits
    + use deviceQuery.cpp to find your limits
    + total number of threads per kernel = threads per block * number of blocks
        * max dimension size of a thread block : e.g. (1024, 1024, 64)
        * max dimension size of a grid size: e.g. (2^31-1, 2^16-1, 2^16-1)
    + maximum execution concurrency
        * maximum number of resident grids per device: 32
            - host起多线程，可以在device上同时跑多个kernel
        * maximum number of threads per multiprocessor
- Memory Hierarchy
    - Thread独有的：寄存器，kernel里面定义的local variable，per-thread local memory
    - Block内共享的：per-block shared memory
    - 不同Block，不同kernel可以访问：per-device global memory
- Memory size limits
    + use deviceQuery.cpp to find your limits
    + global memory: e.g. 11171 MB
    + const memory: 64 MB
        * 属于global memory
    + shared memory per block: 48 MB
    + number of registers avaliable per block: 65536
- CUDA Programming Terminology
    + Host: CPU
    + Device: GPU
    + Kernel: funcions executed on GPU
    + Thread: the basic execution unit
    + Block: a group of threads
    + Grid: a group of blocks

### CUDA Language
- Philosophy: provide minimal set of extensions necessary
- Kernel launch
    + kernelFunc<<<nB, nT, nS, Sid>>>
        * nB: number of blocks per grid 
        * nT: number of threads per block
        * nS(optional): shared memory size
            - runtime不能修改了
        * Sid(optional): stream ID
- Build-in device variables: threadIdx, blockIdx, blockDim, gridDim
    + Thread and Block IDs
    + The index of threads and blocks can be denoted by a 3 dimension struct
- Intrinsic functions that expose operations in kernel code
    + \___syncthreads()
- Function Qualifiers
    + __device__: executed on the device, callable from the device only
    + __global__: executed on the device, callable from the host only, return type is void
    + __host__: executed on the host, callable from the host only
        * 一般不写，默认是host
- Variable Type Qualifiers
    + __device__: resides in device's global memory space 
    + __constant__: resides in device's constant memory space 
    + __shared__: resides in the shared memory space of a thread block
- Device memory opertions
    + cudaMalloc(void **devPtr, size_t size)
        * similar to C's malloc() 
        * devPtr: return the address of the allocated device memory pointer
        * size: the allocated memory size in bytes
    + cudaFree(void *devPtr)
        * similar to C's free() 
    + cudaMemcpy(void *dst, const void *src, size_t count, enum cudaMemcpyKind kind)
        * similar to C's memcpy()
        * count: size in bytes to copy
        * cudaMemcpyKind
            - cudaMemcpyHostToHost
            - cudaMemcpyHostToDevice
            - cudaMemcpyDeviceToHost
            - cudaMemcpyDeviceToDevice
- Program Compilation
    + Two-stage Compilation
        * Generic: NVCC把code分成CPU code和PTX code
            - PTX：Parallel Threads eXecutions
        * Specialized: PTX to Target Complier
            - Device-specific binary object

### Example Code Study
- 

### CPU & GPU Synchronization
- 

### Multi-GPU
- 

## 十: GPU Architecture & Multi-GPU
###


## 十一: Optimizaiton
### 
