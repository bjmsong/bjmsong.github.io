---
layout:     post
title:      推荐系统之
subtitle:   FM（Factorization Machine）
date:       2020-04-08
author:     bjmsong
header-img: img/Recommendation System/th.jpg
catalog: true
tags:
    - 推荐系统
---
> FM算法2010年由Rendle提出，最近几年在各大厂CTR预估和推荐领域得到广泛使用

### 背景

- LR缺点：人工特征工程，耗时耗力
- 将任意两个特征进行组合，等价于多项式核SVM：对于常见的特征稀疏场景，特征组合更加稀疏，很难学到特征组合的权重



### FM原理

<ul> 
<li markdown="1">
引入任意两个特征的组合，和SVM模型最大的不同，在于特征组合权重的计算方法
![]({{site.baseurl}}/img/Recommendation System/FM/fm0.png) 
</li> 
</ul> 

- FM对于每个特征，学习一个大小为k的一维向量，于是，两个特征`x_i`和`x_j`的特征组合的权重值，通过特征对应的向量`v_i`和`v_j`的内积`<v_i,v_j>`来表示
<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/Recommendation System/FM/fm.png) 
</li> 
</ul> 

- 本质上是在对特征进行embedding化表征，和目前非常常见的各种实体embedding本质思想是一脉相承的
- 泛化能力强
    - 即使在训练数据里两个特征并未同时在训练实例里见到过，意味着`x_i`和`x_j`一起出现的次数为0，如果换做SVM的模式，是无法学会这个特征组合的权重的
    - 但是因为FM是学习单个特征的embedding，并不依赖某个特定的特征组合是否出现过，所以只要特征`x_i`和其它任意特征组合出现过，那么就可以学习自己对应的embedding向量`v_i`
    - 于是，尽管`x_i`和`x_j`这个特征组合没有看到过，但是在预测的时候，如果看到这个新的特征组合，因为`x_i`和`x_j`都能学会自己对应的embedding，所以可以通过内积算出这个新特征组合的权重
    - 本质上，这也是目前很多花样的embedding的最核心特点，就是从0/1这种二值硬核匹配，切换为向量软匹配，使得原先匹配不上的，现在能在一定程度上算密切程度了，具备很好的泛化性能



### FM的应用

- 回归：预测目标是连续值，损失函数可以是平方误差
- 分类：预测目标是分类值，损失函数可以是交叉熵
- 排序：pairwise分类损失



### 矩阵分解（MF）和FM的关系

- **MF模型是FM模型的特例**，MF可以被认为是只有User ID 和Item ID这两个特征Fields的FM模型，MF将这两类特征通过矩阵分解，来达到将这两类特征embedding化表达的目的
- **FM则可以看作是MF模型的进一步拓展**，除了User ID和Item ID这两类特征外，很多其它类型的特征，都可以进一步融入FM模型里，它将所有这些特征转化为embedding低维向量表达，并计算任意两个特征embedding的内积，就是特征组合的权重
- FM的优点：
1. 可以在实现等价功能的基础上，很方便地融入其它任意你想加入的特征
2. 从实际大规模数据场景下的应用来讲，在排序阶段，绝大多数只使用ID信息的模型是不实用的，没有引入Side Information，也就是除了User ID／Item ID外的很多其它可用特征的模型，是不具备实战价值的
<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/Recommendation System/FM/mf2fm.png) 
</li> 
</ul> 



### FM的算法效率

- 通过数学公式推导，可以把时间复杂度从`O(kn*2)`降到`O(kn)`，其中n是特征数量，k是特征embedding的size
- 真实的推荐数据的特征值是极为稀疏的，就是说大量`x_i`其实取值是0，意味着真正需要计算的特征数n是远远小于总特征数目n的，无疑这会进一步极大加快FM的运算效率

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/Recommendation System/FM/fm计算效率优化.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/Recommendation System/FM/FM公式改写.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/Recommendation System/FM/FM公式改写1.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/Recommendation System/FM/FM公式改写2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/Recommendation System/FM/FM公式改写3.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/Recommendation System/FM/FM公式改写4.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
SGD梯度计算如下
![]({{site.baseurl}}/img/Recommendation System/FM/sgd_fm.png) 
</li> 
</ul> 



### FM统一多路召回

#### 统一召回和多路召回优缺点比较
- 统一召回的优点
1. 采用多路召回，每一路召回因为采取的策略或者模型不同，所以各自的召回模型得分不可比较，比如利用协同过滤召回找到的候选Item得分，与基于兴趣标签这一路召回找到的候选Item得分，完全是不可比较的。这也是为何要用第二阶段Ranking来将分数统一的原因。而如果采取统一的召回模型，比如FM模型，那么不论候选项Item来自于哪里，它们在召回阶段的得分是完全可比的
2. 多路召回分数不可比会直接引发一个问题：对于每一路召回，我们应该返回多少个Item是合适的呢？如果在多路召回模式下，这个问题就很难解决。既然分数不可比，那么每一路召回多少候选项K就成为了超参，需要不断调整这个参数上线做AB测试，才能找到合适的数值。而如果召回路数特别多，于是每一路召回带有一个超参K，就是这一路召回多少条候选项，这样的超参组合空间是非常大的。所以到底哪一组超参是最优的，就很难定。其实现实情况中，很多时候这个超参都是拍脑袋上线测试，找到最优的超参组合概率是很低的，而且不同用户的K应该是不一样的，统一召回可以实现个性化
3. 对于工业界大型的推荐系统来说，有极大的可能做召回的技术人员和做Ranking的技术人员是两拨人。可能发生问题：比如召回阶段新增了一路召回，但是做Ranking的哥们不知道这个事情，在Ranking的时候没有把能体现新增召回路特性的特征加到Ranking阶段的特征中。在召回和排序之间可能存在信息鸿沟的问题，因为目前召回和排序两者的表达模式差异很大，排序阶段以特征为表达方式，召回则以“路／策略／具体模型”为表达方式，两者之间差异很大，是比较容易产生上述现象的
- 多路召回的优点
上线一个新召回方式比较灵活，对线上的召回系统影响很小，因为不同路召回之间没有耦合关系。但是如果采用统一召回，当想新增一种召回方式的时候，表现为新增一种或者几种特征，可能需要完全重新训练一个新的FM模型，整个召回系统重新部署上线，灵活性比多路召回要差



### FM如何做召回

- 基础工作
    - 离线训练：得到每个特征和这个特征对应的训练好的embedding向量
    - 把特征划分为三个子集合，用户相关特征集合，物品相关特征集合以及上下文相关的特征集合。而用户历史行为类特征，比如用户过去点击物品的特征，可以当作描述用户兴趣的特征，放入用户相关特征集合内
- 召回（极简版）
    - embedding聚合
        - 对于某个用户，我们可以把属于这个用户子集合的特征，查询离线训练好的FM模型对应的特征embedding向量，然后将n个用户子集合的特征embedding向量累加，形成用户兴趣向量U，这个向量维度和每个特征的维度是相同的
        - 对于物品特征也是一样处理
    - 计算内积
        - 将每个用户的兴趣向量离线算好，存入在线数据库中比如Redis（用户ID及其对应的embedding），把物品的向量逐一离线算好，存入Faiss(Facebook开源的embedding高效匹配库)数据库中
        - 当用户登陆或者刷新页面时，可以根据用户ID取出其对应的兴趣向量embedding，然后和Faiss中存储的物料embedding做内积计算，按照得分由高到低返回得分Top K的物料作为召回结

- 考虑上下文特征（比如什么时间在什么地方用的什么设备在刷新）
    -  之所以把上下文特征单独拎出来，是因为它有自己的特点，有些上下文特征是近乎实时变化的，比如刷新微博的时间，再比如对于美团嘀嘀这种对地理位置特别敏感的应用，用户所处的地点可能随时也在变化，而这种变化在召回阶段就需要体现出来。所以，上下文特征是不太可能像用户特征离线算好存起来直接使用的，而是用户在每一次刷新可能都需要重新捕获当前的特征值。动态性强是它的特点



### FM模型能否将召回和排序阶段一体化

- 主要看精度和效率



### FFM（Field-aware Factorization Machine）

- 最初的概念来自Yu-Chin Juan（阮毓钦）与其比赛队员，是他们借鉴了来自Michael Jahrer的论文中的field概念提出了FM的升级版模型
- 通过引入field的概念，FFM把相同性质的特征归于同一个field
- 简单来说，同一个categorical特征经过One-Hot编码生成的数值特征都可以放到同一个field，包括用户性别、职业、品类偏好等
- 每一维特征`xi`，针对其它特征的每一种field`fj`，都会学习一个隐向量`vi,fj`。因此，隐向量不仅与特征相关，也与field相关。也就是说，“Day=26/11/15”这个特征与“Country”特征和“Ad_type”特征进行关联的时候使用不同的隐向量，这与“Country”和“Ad_type”的内在差异相符，也是FFM中“field-aware”的由来
- FFM模型作为排序模型，效果确实是要优于FM模型的，但是FFM模型对参数存储量要求太多，以及无法能做到FM的运行效率，如果中小数据规模做排序没什么问题，但是数据量一旦大起来，对资源和效率的要求会急剧升高，这是严重阻碍FFM模型大规模数据场景实用化的重要因素



### 开源库
- xlearn：比libfm和libffm库快得多

- libfm：官方

- fastFM

- pyFM



### 参考资料

- https://zhuanlan.zhihu.com/p/58160982
- 《Factorization Machines》
- https://tech.meituan.com/2016/03/03/deep-understanding-of-ffm-principles-and-practices.html
- http://www.cs.cmu.edu/~wcohen/10-605/2015-guest-lecture/FM.pdf
- https://liam.page/2019/03/25/Factorization-Machine/
- https://www.analyticsvidhya.com/blog/2018/01/factorization-machines/
- https://pnyuan.github.io/blog/ml_practice/Kaggle%E6%BB%91%E6%B0%B4%20-%20CTR%E9%A2%84%E4%BC%B0%EF%BC%88FM_FFM%EF%BC%89/
- https://github.com/jizhihui/fm_python
- https://zhuanlan.zhihu.com/p/59528983
- 《Field-aware Factorization Machines for CTR Prediction》
- https://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&mid=2247490460&idx=1&sn=86e99c95184e454a98f88b36bc75d58c&chksm=fbd4a9f0cca320e69dc608b757c934734dffe43c5e96d48abdd1598eb4c86d1383cd5f8e9c03&scene=0&xtrack=1&key=ded5b9760a24807ba84f3f4446aa6fbaef12ecf4c87c9fc7851415d71fac4e631c2f227cf689559b2261945d2413551444eab5dfc7a51847c2e3cb4caefea1ed05bfece2017526ddf457e538c8da82c3&ascene=1&uin=MjM1OTMwMzkwMA%3D%3D&devicetype=Windows+7&version=62060739&lang=zh_CN&pass_ticket=jE0997Q0BxXAfQfcdlIFtBqzMNmRLEsiD%2BWftwC3d2amLGQESRu5jEW3o%2F21HdFV
- https://blog.csdn.net/google19890102/article/details/45532745