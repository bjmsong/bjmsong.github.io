---
layout:     post
title:      图表示学习（Graph Embedding）之二
subtitle:   经典算法介绍
date:       2019-10-28
author:     bjmsong
header-img: img/Graph/kg.jpg
catalog: true
tags:
    - 图算法
---
>本文将介绍图表示学习的几种经典算法，包括DeepWalk、LINE、GNN、GCN等。

<ul> 
<li markdown="1"> 
这幅图展示了图表示学习这个领域算法的发展脉络
![]({{site.baseurl}}/img/Graph/network embedding/study.jpg) 
</li> 
</ul> 

### DeepWalk
<ul> 
<li markdown="1"> 
首先来介绍2014年提出来的DeepWalk，其思想源于自然语言处理领域的word2vec。这两件事情确实是有很多相似的地方，在图中经过一定距离的随机游走，所经过的节点是满足幂律（或者说长尾）分布的，如左图所示。同样，在自然语言处理中，词的分布也满足幂律分布，如右图所示。
![]({{site.baseurl}}/img/Graph/network embedding/law distribution.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
word2vec通过语料构建语言模型，得到词的向量化表示。
![]({{site.baseurl}}/img/Graph/network embedding/w2v.jpg) 
</li> 
</ul> 


<ul> 
<li markdown="1"> 
我们知道，wordvec训练需要语料，那么DeepWalk的语料从何而来呢？答案是：随机游走
DeepWalk分为两个阶段，第一个阶段，在图上进行固定距离的随机游走，通过这种方式采样，得到节点序列。
![]({{site.baseurl}}/img/Graph/network embedding/deepwalk.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
第二个阶段，借鉴word2vec里的SkipGram算法，计算每一个节点上下文的似然概率，极大化似然概率，最后得到节点的向量化表示。
![]({{site.baseurl}}/img/Graph/network embedding/Representation Mapping.jpg) 
</li> 
</ul> 



### LINE

第二个要介绍的算法是LINE，全名叫做Large-scale Information Network Embedding。顾名思义，这个算法可以应用在大规模的数据场景中。


#### LINE的优点

- 快：百万节点，十亿条边，单机可以数小时内跑完
  - 矩阵分解的方法，时间复杂度是节点的平方
- 图的类型没有限制：有向、无向、有权重、没有权重 --- 都可以！
  - DeepWalk：没法考虑边的权重
- 效果好：实验为证



#### LINE的原理

- 一阶相似度：两个顶点之间的自身相似（不考虑其他顶点）。对于由边（u，v）连接的每一对顶点，边上的权重wuv表示u和v之间的相似度，如果在u和v之间没有观察到边，则它们的一阶相似度为0。
一阶邻近通常意味着现实世界网络中两个节点的相似性。例如，在社交网络中相互交友的人往往有着相似的兴趣;在万维网上相互链接的页面倾向于谈论类似的话题。
- 二阶相似度：顶点邻近网络结构之间的相似性。在数学上，设pu=（wu，1，...，wu，| V |）表示u与所有其他顶点的一阶相似度，则u和v之间的二阶相似度 由pu和pu决定。如果没有顶点与u和v都连接，则u和v之间的二阶相似度为0。
因为有些边观察不到等原因，一阶相似度不足以保存网络结构。因此提出共享相似邻居的顶点倾向于彼此相似，即二阶相似度。 例如，在社交网络中，分享相似朋友的人倾向于有相似的兴趣，从而成为朋友; 在词语共现网络中，总是与同一组词语共同出现的词往往具有相似的含义。

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/Graph/network embedding/LINE相似度.jpg) 
</li> 
</ul> 

- LINE设计了同时保留一阶相似度和二阶相似度的目标函数

- 以一阶相似度为例：

  <ul> 
  <li markdown="1"> 
  为了模拟一阶相似度，对于每个无向边（i，j），我们定义顶点vi和vj之间的联合概率如下：（sigmoid function，向量越接近，点积越大，联合概率越大）
  ![]({{site.baseurl}}/img/Graph/network embedding/LINE1.jpg) 
  </li> 
  </ul> 

  其中，ui表示节点vi对应的向量。这样就定义了一个V*V的分布p（·，·）

  <ul> 
  <li markdown="1"> 
  经验概率可以定义为：（两点之间边的权值越大，经验概率越大）
  ![]({{site.baseurl}}/img/Graph/network embedding/LINE2.jpg) 
  </li> 
  </ul> 

  其中，W=Σwi。得到了另一个分布。

  <ul> 
  <li markdown="1"> 
  为了保持一阶相似性，一个简单的办法是最小化下面的目标函数：
  ![]({{site.baseurl}}/img/Graph/network embedding/LINE3.jpg) 
  </li> 
  </ul> 

  其中d（·，·）是两个分布之间的距离。

  <ul> 
  <li markdown="1"> 
  我们选择最小化两个概率分布的KL散度，用KL散度代替d（·，·）并省略一些常数，得到：
  ![]({{site.baseurl}}/img/Graph/network embedding/LINE4.jpg) 
  </li> 
  </ul> 



### 图神经网络（GNN）

<ul> 
<li markdown="1"> 
最近几年，图神经网络是一个比较热门的研究领域。下面这张图是近些年GNN论文发表数量的情况，我们可以看到从2016年开始，GNN论文发表数量呈指数上升趋势。
![]({{site.baseurl}}/img/Graph/network embedding/gnn paper.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
节点代表实体或者概念，边代表它们之间的关系。GNN结合了信息传播机制和神经网络，核心思想是信息在网络上传播，可以改变相邻节点的状态，直到达到平衡。先定义一些符号，点跟边的属性用l表示，每个节点的状态用x表示。每个顶点的状态，受跟它相邻的边，相邻的顶点影响。
![]({{site.baseurl}}/img/Graph/network embedding/gnn1.jpg) 
</li> 
</ul> 


<ul> 
<li markdown="1"> 
参数函数fw称为局部变换函数，描述了节点和其邻域的依赖性。gw称为局部输出函数，刻画了输出值的生成过程。
![]({{site.baseurl}}/img/Graph/network embedding/gnn公式.jpg) 
</li> 
</ul> 

下面两幅图展示了具体过程。


<ul> 
<li markdown="1"> 
上面那个是原图，下面那个是与之对应的图传播机制的展示，把每个节点表示成fw和gw的计算过程，fw和gw都可以用前向神经网络来表示。
![]({{site.baseurl}}/img/Graph/network embedding/gnn2.jpg) 
</li> 
</ul> 


<ul> 
<li markdown="1"> 
这幅图把状节点态更新的过程通过循环神经网络来表示。
![]({{site.baseurl}}/img/Graph/network embedding/gnn3.jpg) 
</li> 
</ul> 

​ 

### GCN




### 参考资料
- Tutorial on Graph Representation Learning, AAAI 2019
- CNCC报告 唐杰 
https://www.aminer.cn/cncc19-classicml
- 《DeepWalk: Online Learning of Social Representations》
- 《LINE: Large-scale Information Network Embedding》
- 《The graph neural network model》
- LINE 
https://www.jianshu.com/p/8bb4cd0df840
- GNN
https://blog.csdn.net/u011748542/article/details/86289511
- 香侬科技 吴天龙 图神经网络分享
https://github.com/talorwu/Graph-Neural-Network-Review



