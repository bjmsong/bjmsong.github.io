---
layout:     post
title:      新竹清华大学：并行计算与并行编程
subtitle:   Part I Parallel Programming
date:       2022-11-16
author:     bjmsong
header-img: 
catalog: true
tags:
    - 并行计算
---
- 台湾老师的课一直很喜欢（比如李宏毅老师），质量高，中文讲述容易理解，老师很有水平教得又好，国内很少找到相同质量的课程
- https://www.bilibili.com/video/BV1Yt411W7td?p=1&vd_source=7798c62f92ce545f56fd00d4daf55e26
  + 2018 Fall
  + 周志遠 教授
  + 拆成了小段(20-30min)，一个主题一段，方便食用
- 课程主页
  + https://ocw.nthu.edu.tw/ocw/index.php?page=course&cid=231
- slides
- 参考书不重要，重要的是动手，learning by doing，代码优化，性能分析
  - Lecture，textbook：Fundamental knowledge，algorithm and theory
  - Project: coding, performance
  - Report: Performance analysis, Presentation

## Intro to Parallel Computing
### Parallel Computing Introduction
1. What is parallel computing
- Solve a single problem by using multiple processors working together 
- parallel computing vs distributed computing
  + parallel computing
    * different activities happen at the same time
    * spread out a single application over many cores/processors/processes to get it done bigger or faster
  + distributed computing
    * activities across systems or distanced servers
    * focus more on concurrency and resource sharing
- The Universe is Parallel
  + 生活中并行无处不在
  
2. Why we need parallel computing
- Save time
- Solve larger problem
- Make better use of the underlying parallel hardware
- The Death of CPU Scaling 

3. Trend of Parallel Computing
- Single-Core Era
- Multi-Core Era
  + Phread -> OpenMP
- Distributed System Era
  + MPI -> MapReduce
- Heterogeneous Systems Era
  + Shader -> CUDA -> OpneCL

### Classifications of Parallel Computers & Programming Models
1. Flynn's classic taxonomy
- SISD: single-core processor
- SIMD: GPU, vector processor
- MISD
- MIMD: multi-core CPU

2. Memory architecture classification
- Shared Memory
  + Single computer with multiple internal multi-core processors
  + 根据内存是否可以直接读取，分为两种
    * Uniform Memory Access (UMA)
      - equal access times to memory
    * Non-Uniform Memory Access (NUMA)
      - memory access across link is slower
- Distributed Memory
  + Connect multiple computers to form a computing platform without sharing memory
    * Cluster,Supercomputer,Datacenter
  + require a communication network to connect inter-processor memory
  + processors have their own memory & address space
  + memory change made by a processor has no effect on the memory other processors
  + programmers and programming tools are responsible to explicitly define how and when data is communicated between processors 

3. Programming model classification
- Parallel programming models exist as an abstraction above hardware & memory architecture
- In general(不绝对) programming models are designed to match the computer architecture
  + Shared memory prog. model for shared memory machine
  + Message passing prog. model for distributed memory machine
- but programming models are not restricted by the machine or memory architecure
- Shared memory programming model
  + A single process can have multiple, concurrent execution paths
  + Threads have local data, but also, shared resources
  + Threads communicate with each other through global memory
  + Threads can come and go, but the main program remains
  + Implementation
    * A library of subroutines called from parallel source code
      - POSIX Thread(Pthread)
    * A set of compiler directives imbedded in either serial or parallel source code
      - OpenMP: 实现更简单，由编译器转换为Pthread的实现
  + Convenient
    * can share data structures
    * just annotate loops
    * Closer to serial code
  + Disadvantages
    * No locality control
    * Does not scale
    * Race conditions
- Message passing programming model
  + A set of tasks that use their own local memory during computation
  + Task exchange data through communications by sending and receiving messages(memory copy) 
  + MPI API: Send,Recv,Bcast,Gather,Scatter,etc
  + Scalable
    * Locality control
    * Communication is all explicit in code
  + Disadvantages
    * Need to rethink ehtire application/data structures
    * Lots of tedious pack/unpack code
    * Don't know when to say "receive" for some problems
- Summary
  + The designs and popularity of programming model and parallel systems are highly influenced by each other
  + openMP,MPI,Pthreads,CUDA are just some of the programming languages for users to do parallel programming
  + In reality, knowing what is parallel computing is more important than knowing how to do parallel programming, because that's how you can...

### Supercomputer & Latest Technology
1. Supercomputer
- A computer with a high-level computation capacity compared to a general-purpose computer
- Its performance is measured in FLOPS instead of MIPS
- top500.org

2. Processor technology
- CPU
  + A general purpose CPU can do anything, but its design is against the goal of achieving the best performance for a specific applicaiton 
  + 核数少，算力低，内存带宽小，内存大
- GPU
  + 核数多，算力高，内存带宽大，内存小
  + 异构架构，CUDA Programming Model
    * Host(CPU), Device(GPU)
    * Hierachical Memory Structure
      - global memory
      - PBSM：分布在独立的Stream processor中，Stream processor内部的core共享
      - register：寄存器 
  + Thread Execution Manager
    * 由硬件来调度执行
- TPU
  + 专门用于深度学习，主要是优化矩阵乘法
  + CPU按value进行计算，GPU按vector进行计算，TPU按matrix进行计算
  + https://cloud.google.com/blog/products/ai-machine-learning/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu
    * quantization: floating point operation -> integer operation
      - reduce the amount of memory and computing resource
    * instruction set
    * Parallel Processing on the Matrix Multiplier Unit
    * The heart of the TPU: A systolic array

3. Interconnect & Network technology
- Commnuication has the most impact to the performance of parallel programs(Even more critical to computing or memory)
  + Network is generally slowly than CPU
  + Commnuication is common to parallel programs
  + Synchronization is expensive and could grow exponentially to the number of servers
- Network design considerations
  + Scalability
  + Performance
  + Resilience：复原力，当系统遇到失败时候，能够从失败中自动恢复，提供服务的能力
  + Cost
- 自底向上
  + Network Devices: Cable,Switch,Adapter...
    * Bandwidth: #bits transferred per second 
    * Latency: time to pack, unpack and send a message
    * Scalability: # of ports on the adapter and switch
  + Interconnection Network Topology
    * Network diameter
    * Re-routing path for fault tolerance
    * #fan-in & #fan-out degree per node
  + Applicaiton
    * Commnuication pattern & protocol: MPI
- Network Topology
  + 评判指标
    * Diameter(latency): 最长距离
    * Bisection(resilience): 需要砍断多少个edge，可以使拓扑网络变成两个互不相干的网络
    * #Links(Cost)
    * Degree(scalability): #fan-in & #fan-out degree per node
  + Linear Array(p nodes)
    * Diameter: p-1
    * Bisection: 1
    * #Links: p-1
    * Degree: 2
  + Ring(把Linear Array首尾相接)
    * Diameter: p/2
    * Bisection: 2
    * #Links: p
    * Degree: 2
  + Tree
  + 2-D Mesh(网格，可以扩展到三维，最常用)
    * Diameter: 2($\sqrt{p}$-1)
    * Bisection: $\sqrt{p}$ 
    * #Links: 2$\sqrt{p}$($\sqrt{p}$-1)
    * Degree: 4
  + 2-D Torus(把2-D Mesh首尾相接)
  + HyberCube
    * more suitable for smaller scale systems
- InfiniBand
  + A computer newtork communications link used in high-performance computing featuring very high throughput
  + It is the most commonly used interconnect in supercomputers
    * 超过了Ethernet
  + RDMA技术

4. I/O & Storage technology
- I/O的效率落后于计算的效率
- Memory hierarchy
  + 容量小->大，访问速度慢->快
    * register -> (L1/L2/L3)cache -> main memory -> Flash(Non-volatile memory) -> Hard Disk Drive -> Magnetic tape Storage Systems
- Parallel file and IO systems: 文件系统
  + Lustre file system
  + MPI-IO
- Burst buffering
  + Add non-volatile RAM at the IO server nodes as a buffer to smooth the burst(突然爆发的) traffic pattern for improving the IO performance of storage systems, and reduce the IO latency 
  
### Parallel Program Analysis
1. Speedup & Efficiency
- speedup factor：execution time using the best sequential algorithm / execution time using p processors
- system efficiency: speedup factor/p
  + for Linear speedup: system efficiency is 100%
- Difficult to reach ideal speedup
  + Not every part of a computation can be parallelized
  + Need extra computations in the parallel version
  + Commnuication time between processor
2. Strong scalability vs weak scalability
- Strong Scaling
  + The problem size stays fixed but the number of processing elements are increased
  + Linear scaling is harder to achieve, because of the commnuication overhaed may increase proportional to the scale 
- Weak Scaling 
  + The problem size assigned to each processing element stays fixed and additional processing elements are used to slove a larger total problem
  + Linear scaling is easier to achieve because programs typically employ nearest-neighbor commnuication patterns where the commnunication overhead is relatively constant regardless of the number of processes used

3. Time complexity & Cost optimality 
- computaion part
- commnuication part
  + message latency: assumed constant
  + transmission time to send data
- Trade off between computation & commnuication 

## Message-Passing Programming: MPI 
### MPI Introduction
1. Histroy & Evolution

### Commnuication Methods
1. Synchronous(同步) / Asynchronous(异步)
2. Blocking / Non-Blocking

### MPI API
1. Point-to-Point Communication Routines
2. Collective Commnuication Routines
3. Group and Commnuicator Management Routines

### MPI-IO


## Shared-Memory Programming: Pthread
- 

## OpenMP Programming
- 
