---
layout:     post
title:      昇腾AI处理器架构与编程
subtitle:   
date:       2022-09-02
author:     bjmsong
header-img: 
catalog: true
tags:
    - 模型优化与部署
---
## Chapter1.基础理论
### 多层感知机
- 参数太多，计算量太大，容易过拟合
- 难以捕捉图像局部丰富的特征
### 卷积神经网络
- LeNet5(1998) -> AlexNet(2012) -> VGG,GoogLeNet -> Resnet
- 输入层处理
    - 长×宽×像素数量
    - 归一化
- 卷积层
    - 卷积核
        - 三维矩阵：宽度，高度、深度（与输入特征的数据通道数一致）
        - 同一层有多个卷积核，用来提取不同的特征(如颜色、形状等)
        - 同一层卷积核大小相同，不同层的卷积核大小可以不同
        - 感受野：卷积核作用的卷积窗口对应于原始输入特征图的部分
            - 感受野越大，则表示卷积核作用在原始图像上的面积越大
    - 卷积核操作完成输出后，输出特征图中的每一个特征值还需要通过激活函数进行处理
        - 引入非线性，增强了对于非线性特征的拟合能力
    - 特性
        - 局部连接
            - 每个神经元只针对输入特征图中感受野范围内的数据进行处理，使得权重参数锐减
            - 在特征图像中，每一个像素和相邻像素的联系比较紧密，和距离较远的像素相关性较弱
        - 权重共享
            - 同一层卷积层中不同神经元使用了同一个卷积核进行卷积计算，极大精简了参数数量
    - 卷积层计算方法
        - 直接卷积
            - 大多数卷积神经网络加速器都采用这种方法
                - TPU：脉动阵列的计算方式
            - 步骤
                - 先对输入特征图补零(padding)
                    - 使得输出特征图和输入特征图大小保持一致
                    - 保护输入特征图的边缘特征信息
                - 卷积核在输入特征图中滑动，进行点积计算
        - 矩阵乘法实现卷积
            - 应用在CPU、GPU等具有通用编程性的计算芯片上
                - 也包括昇腾AI处理器
            - 步骤
                - 通过Img2Col将卷积层中的输入特征图和卷积核权重矩阵展开
                    - 在计算过程中将需要计算的特征子矩阵存放在连续的内存中，有利于一次将所需要计算的数据直接按照需要的格式取出进行计算，减少了内存的访问次数
                - 将卷积中输入特征子矩阵和权重矩阵对应元素的点积操作转换成矩阵运算中行与列向量的乘加运算
                    - CPU和GPU都提供专门的基本线性代数程序集(BLAS)来高效地实现向量和矩阵运算
        - 快速傅里叶变换(FFT)和Winograd算法
            - 将复杂的卷积运算等价变换成另一个空间的简单运算，从而降低了计算的复杂度(将卷积当中的乘法计算量降低到之前的九分之四)
                - cuDNN部分使用了Winograd算法
                - https://www.zhihu.com/question/376741737/answer/1059438493
                - https://www.bilibili.com/video/BV15S4y1B7kj/?spm_id_from=333.337.search-card.all.click&vd_source=7798c62f92ce545f56fd00d4daf55e26
                - https://www.cnblogs.com/shine-lee/p/10906535.html
                - https://zhuanlan.zhihu.com/p/260109670
- 池化层
    - 在相邻卷积层之间规律性地插入池化层，通过池化层进一步减少每一层特征值，保留为主要特征，减少了参数量
    - 池化滤波器
        - 最大值、平均值
        - 尺寸F，步长S
- 全连接层输出
- 优化加速
    - 突触并行
        - 同一个卷积窗口中的乘法运算可以并行执行
    - 神经元并行
        - 每个卷积窗口的计算过程没有数据依赖，可以并行计算
    - 输入特征图并行
        - 通道级别的并行处理
    - 输出特征图并行
        - 多个卷积核的操作可以实现并行计算
    - 批处理并行
        - 一次输入一个批次的图像进行批处理
### 应用示例


## Chapter2.行业背景
### 神经网络芯片现状
- CPU
    - 性能提升遇到瓶颈
    - 侧重于指令执行中的逻辑控制
- GPU
    - SIMT
    - 大规模硬件并行
    - 擅长大规模密集型数据并行计算
    - CUDA
        - 提供了GPU硬件的直接访问接口，可以直接使用一种类C语言的方式对GPU编程
    - 张量核(Tensor Core)
        - 
    - 缺点：功耗高
- TPU
    - 采用低精度计算
- FPGA

### 神经网络芯片加速理论
- GPU加速理论
    - 并行化，矢量化
    - 通用矩阵相乘(GEMM)
        - 对于各类神经网络，将其核心计算展开成矩阵运算
            - 例如卷积，通过Img2col方法，转换成矩阵运算
        - 矩阵相乘：单指令多线程，一个线程计算结果矩阵的一个点(对应着输入左矩阵的一行与输入右矩阵的一列相运算后得到的结果)
    - 现代GPU架构:Turing
        - 流处理器(Stream Multiprocessor)
            - 64个CUDA核
                - 负责单精度浮点数FP32的计算
            - 8个张量核
                - 为深度神经网络算法提供更强大、更专用、更高效的算力
                - 支持FP16,INT8,INT4等多精度计算: 精度较低，但满足绝大多数神经网络的需求
                - 新的编程接口：WMMA
        - 多层级(Hierarchy)的片上存储器结构: L1缓存(96KB)，共享内存，寄存器组(256KB)，L2缓存，外部存储等
        - 网络互联结构
- TPU加速理论
    - 脉动阵列
### 深度学习框架
- Mindspore
- Caffe
- TensorFlow
    - 静态图：定义完计算图后，运行计算图时才能获取数据
    - 静态数据流图来表示神经网络中数据的计算过程
        - 用户不关心底层硬件调度以及计算实现细节
        - 框架能够在运行时知道计算图的所有细节，能够对计算图进行更深层次的优化
    - 支持分布式计算
    - 不是一个普通的Python库，本质上是一种新的特定域语言(DSL)
- PyTorch
    - 动态图：每执行一条语句，系统变构建出相应的计算图，且数据在实时进行计算
### 深度学习编译框架：TVM
- 痛点：不同的软件编程方法和框架在不同的硬件架构上的优化实现差异很大
- 编译语言中间表示框架LLVM
    - 所有的软件框架都不直接映射到具体的硬件上，而是首先通过前端编译器编译成一种中间格式的指令表达(IR)
    - 硬件供应商可以分别提供对接到中间指令表达的后端编译器，实现从IR到具体硬件指令的通路
- 专门用于深度学习的中间表达形式、编译器和执行器
    - TVM

## Chapter3.硬件架构
### 昇腾AI处理器总览
### 达芬奇架构
### 卷积加速原理

## Chapter4.软件架构
### 昇腾AI软件栈总览
### 神经网络软件流
### 开发工具链

## Chapter5.编程方法
### 深度学习开发基础
#### 深度学习编程理论
- 声明式编程，符号式编程：静态图
    - 只需要对运算进行定义，在运行的时候再显性或隐性地进行编译，转换为实际的底层内核函数调用
    - 将计算图的定义步骤和实际编译运行步骤分隔
    - 告诉计算机要做什么(What),计算机自己决定怎么做(How)
    - 运行效率更好
    - TensorFlow，Caffe
- 命令式编程：动态图
    - 每一行代码在被执行时，相应的计算都会立即执行
    - 详细地告诉计算机怎么做(How),进一步地去完成什么样的任务(What)
    - 灵活度更好
    - python，PyTorch
- 不同框架之间互相借鉴
- 元编程：用代码来生成代码
    - 由TensorFlow构建的计算图更像一个程序，只不过其具体的计算由TensorFlow来实现和调度
- 特定域语言(DSL)
    - 专门设计一种语言来完成某一特定领域程序的开发
    - 正则表达式：专门用于处理字符匹配的语言
    - Latex：专门用于文本排版的语言
    - 一般都是非图灵完备的
        - 图灵完备：一切可计算的问题都能计算(C++,Python等)
- TensorFlow中用于构建计算图的代码本质上可以被看作一种基于张量计算、专用于深度学习的特定域语言
    - 原因：深度学习对计算性能有极高的要求，需要有完善的数值库；针对深度学习的算子库，要有非常低的解释器开销，同时也要对硬件有很好的支持
    - 基于某些假设，例如前馈神经网络不会出现判断、递归，整个计算图可以被看作有向无环图
    - 图灵完备：支持分支、判断功能
- 区分训练计算图和推理计算图
    - 推理流程只包括前向传播，训练过程既包含前向传播又包括反向传播
    - 某些功能层在训练过程和推理过程的计算特性会明显不同
- 模型保存
    - 计算图结构文件
    - 模型参数文件
    - 模型文件：计算图结构+模型参数
- 模型转换
    - ONNX(开发网络交换格式)：为不同模型转换提供了一种中间表示，只要深度学习框架支持，就可以实现与其他框架之间的转换
    - MMdnn: 微软
#### 深度学习推理优化原理
- 推理流程
    - 内核函数：具体硬件平台上实现算子计算的函数
    - 执行图：实际内核函数调用的过程
    - CPU+GPU异构计算系统
        - CPU: 负责内存管理、数据搬移、计算调度等
        - GPU: 运行具体的内核函数，执行相应计算
    - 加速库：cuBLAS,cuDNN,OpenBLAS
- 计算图优化：不涉及具体的硬件平台
    - 优化内存分配方式
        - 在推理任务开始前统一分配所需存储空间
    - 指定合适的批量大小
    - 通过精细的内存管理实现内存复用
    - 尽可能消除模型中的控制逻辑
    - 合理使用模型中的并行计算
    - 优化神器: 内核融合
        - 将多个内核函数的计算融合成一个内核函数：减少内核调用的开销
        - cuDNN：将深度学习领域常用的算子(卷积、池化等)高效实现，本质就是通过手工来融合内核
    - 用可以更高效实现的计算节点来替换现有节点
        - 模型量化
        - 使用高效实现的FFT、Winograd卷积等
- 内核函数优化：提高内核函数的性能
    - 循环展开和矩阵分块
        - 矩阵相乘的时间复杂度
            - 标量运算:O(N^3)
            - 向量运算:O(N^2)
            - 矩阵运算:O(1)
        - 硬件支持的并行程度有限，需要对矩阵进行拆分以适配实际硬件的运算能力
    - 优化数据的存储顺序
        - 根据实际的硬件结构和计算资源，对模型参数的存储顺序进行转换
    - 专用内核
        - 在某些配置参数上专用
        - cuDNN，cuBLAS
#### 深度学习推理引擎
- 不支持训练，只关注推理阶段
- 也称为深度学习编译器堆栈
    - 深度学习框架模型文件 -> 不同加速硬件平台
- 用途
    - 解析来自各个深度学习框架的模型文件，转换成统一的计算图级别的中间表示，并可进一步转换成其所支持硬件平台的执行图，提供从框架模型到各种硬件平台的部署能力
    - 基于计算图的中间表示，采用各种优化方法，以提高模型在实际硬件执行过程中的推理性能
- 产品
    - TVM
    - 英伟达：TensorRT
    - 谷歌：TensorFlow XLA
    - Facebook：Tensor Comprehensions
    - 阿里：MNN
    https://toutiao.io/posts/8250zb/preview
    - OpenVINO（Intel CPU）
    https://www.zhihu.com/column/c_1131328261854191616
        - nGraph
    - 小米：MACE
    - 腾讯：NCNN
    - TNN、Tengine
    https://www.cnblogs.com/LXP-Never/p/14840535.html
- DSL -> Deep Learning IR -> LLVM IR -> Target
- 不同推理引擎侧重点不同，且采用不同的抽象层次和优化策略
- TVM
    - 没看懂

### 昇腾AI软件栈中的技术
- 
### 自定义算法开发
- 
### 自定义应用开发
- 

## Chapter6.实战案例
### 评价标准
- 算法评价标准
    - 精度：Accuracy
    - 精准度: Precision
    - 召回率: Recall
    - F1
    - 交并比：预测的边界框和真实的边界框的交集
    - 均值平均精度
- 硬件评价标准
    - 吞吐量：每秒能够处理的图片数量
        - 1000/时延
    - 能效比：TFLOPS/Watt
        - 在芯片上每瓦特功耗所能提供的算力 
### 图像识别
- ImageNet
- ResNet
    - 解决了深层网络层数过高之后产生的性能退化问题
### 目标检测
- 识别出图片中存在的对象(目标)，并给出位置
- COCO数据集
- 工业界应用广泛：自动驾驶、安防、智慧城市等
- YoloV3(You Only Look Once)
    - 注重网络推理的速度
    - 当前性价比最佳的算法
    
## 参考资料
《昇腾AI处理器架构与编程》