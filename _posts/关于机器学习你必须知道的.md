---
layout:     post
title:      关于机器学习，你必须知道的
subtitle:   
date:       2019-09-30
author:     bjmsong
header-img: img/machineLearning/machineLearning.png
catalog: true
tags:
    - 机器学习
---
>最近读了《A Few Useful Things to Know about Machine Learning》， 这篇paper主要谈到了开发机器学习应用时需要关注的一些关键问题。

### 1.Learning = Representation+Evaluation+Optimization
很多机器学习书会按照算法一个个讲，这样给初学者的认知就是机器学习就是这么一个个算法组成的。算法的数量浩如烟海，很容易让人觉得困惑，在碰到实际问题时究竟用什么模型。这些算法之间的联系又在哪里。
因此，paper的第一条，穿透现象看本质，告诉我们学习算法是由三部分组成的：表示（Representation），评估（Evaluation），优化算法（Optimization）。
>李航老师的《统计学习方法》里面分成了模型、策略和算法。

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/The three components of learning algorithms..png) 
</li> 
</ul> 

### 2.It’S Generalization That Counts
>（训练集）死记硬背是容易的，真正考试考高分是不容易的（泛化能力）

机器学习最根本的目标是在训练数据以外的数据上取得不错的泛化效果。这是因为，无论我们的训练数据集有多么庞大，都不可能涵盖所有的可能性。机器学习初学者的常见错误，就是在训练集上做验证，这就会引起过拟合。
比较好的做法是将数据分成训练集/验证集/测试集，如果训练数据数量较少，可做交叉验证，但如果数据和时间相关，最好还是按时间线切分数据。

### 3.Data Alone is Not Enough
>NFL(no free lunch):在不考虑具体问题的情况下，没有任何一个算法比另一个算法更优，甚至没有胡乱猜测更好。

种植农作物，不仅需要种子，还需要营养。数据建模，不仅需要数据，也需要知识。真正的机器学习并不是从假设集合中一个一个挑，而是有一些**先验知识**帮助我们更好的筛选h。先验知识就是，针对不同领域，已经拥有的可以使机器学习更容易做出选择，选到好的知识。比如，如果概率很牛，那么用图模型（把概率分布中的条件，独立用图的形式表达出来）就稍微容易了。

如果没有这样的经验，就让机器自己去茫茫假设集合中选，还能挑到好的，还不如抛个硬币更容易。所以没有“经验+data”，就做选择，会说一句“no free lunch”来告诫。有了经验，就好像杠杆一样，选到靠前的支点，就能用更少的知识获得更好的效果。


### 4.Overfitting has many faces
如果模型在训练集上表现很好，在测试集上表现却很差，基本可以认为是过拟合了。造成过拟合的原因是训练数据太少或者模型太复杂（表现出来的就是特征很多）。
泛化误差有两个方面造成，Bias和Variance。（此外，还有噪声）
- Bias是用所有可能的训练数据集训练出的所有模型的输出的平均值与真实模型的输出值之间的差异。
- Variance是不同的训练数据集训练出的模型输出值之间的差异。

用投飞镖的例子来看，靶心就是好的h。下面的点X表示一次对一个数据集D，学习后得到的预测。可以看出，高bias表示离中心越远（但是很集中），高variance表示对不同数据集学习得到的预测很分散。

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/bias_variance.png) 
</li> 
</ul> 

如果模型比较简单，

线性模型的bias会很高，

模型越简单，bias会越高，越复杂，variance会越高。

- bias-variance tradeoff

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/bias-variance-tradeoff.png) 
</li> 
</ul> 

### 5.Intution Fails In High Dimensions
很多直觉在高维空间并不适用

### 6.Theoretical Guarantees Are Not What They Seem


### 7.Feature Engineering is The Key


### 8.More Data Beats A Cleverer ALgorithms


### 9.Learn Many Models, Not Just One


### 10.Simplicity Does Not Imply Accuracy


### 11.Representable Does Not Imply Learnable


### 12.Correlation Does Not Imply Causation



### 参考资料
- 《No Free Lunch Theorems for Optimization》
- 《Simple Explanation of the No-Free-Lunch Theorem and Its Applications》
- https://baike.baidu.com/reference/259825/639fmRM84ug0Hv3NXYCUH6H0EroltQCAuK7s0TkAqWmx9Vpech3j_9KGquzfkJr45Qy1YszRKvbhBw
- https://blog.csdn.net/danameng/article/details/21563093
- https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229