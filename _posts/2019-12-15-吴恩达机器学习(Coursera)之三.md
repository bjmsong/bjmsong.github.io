---
layout:     post
title:      吴恩达机器学习(Coursera)之三
subtitle:   
date:       2019-12-15
author:     bjmsong
header-img: img/machineLearning/machineLearning.png
catalog: true
tags:
    - 机器学习
---
> 本文是课程的第10~12章，将介绍机器学习应用的一些建议，以及SVM。



## 十、应用机器学习的建议(**Advice for Applying Machine Learning**) 

10.1 决定下一步做什么 

10.2 评估一个假设 

10.3 模型选择和交叉验证集

<ul> 
<li markdown="1"> 
如果在训练集上训练模型，挑选测试集上表现最好的模型，仍然可能过拟合，即这个模型过度拟合了测试集。
![]({{site.baseurl}}/img/CourseraML/不能只看测试集上的效果.png) 
</li> 
</ul> 

**最佳实践是将数据集分成训练集、交叉验证集、测试集。挑选在验证集上表现最好的模型，计算其在测试集上的误差。通过比较在测试集和验证集上的误差，可以看出模型在新样本上的泛化误差。**

10.4 诊断偏差和方差 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/bias_variance.jpg) 
</li> 
</ul> 

10.5 正则化和偏差/方差 

- 通过调节正则化项的权重，计算模型在验证集上的效果，可以使得模型在high bias 和 high variance 之间取得平衡

10.6 学习曲线 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/high_bias.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/high_variance.jpg) 
</li> 
</ul> 

10.7 决定下一步做什么 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/模型优化.jpg) 
</li> 
</ul> 



## 十一、机器学习系统的设计(**Machine Learning System Design**) 

11.1 首先要做什么 

11.2 误差分析 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/误差分析.jpg) 
</li> 
</ul> 

11.3 不平衡样本的误差度量 

11.4 查准率和查全率之间的trade-off 

11.5 机器学习的数据 



## 十二、支持向量机(**Support Vector Machines**) 

12.1 优化目标 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/逻辑回归损失函数的近似.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/svm.jpg) 
</li> 
</ul> 

12.2 大间隔分类器的直观理解 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/大间隔.jpg) 
</li> 
</ul> 

12.3 大间隔分类器背后的数学问题（SVM的优化问题）

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/SVM优化1.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/SVM优化2.jpg) 
</li> 
</ul> 

12.4 核函数1 

<ul> 
<li markdown="1"> 
构造非线性分类器的技巧
![]({{site.baseurl}}/img/CourseraML/kernel.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/kernel2.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/kernel3.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/kernel4.jpg) 
</li> 
</ul> 

12.5 核函数2 

<ul> 
<li markdown="1"> 
如何选择标记点
可以将核函数的技巧用于其它算法（如逻辑回归），但是运行效率很低
![]({{site.baseurl}}/img/CourseraML/SVM选择标记点.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
bias-variance tradeoff
![]({{site.baseurl}}/img/CourseraML/SVMtradeoff.png) 
</li> 
</ul> 

12.6 使用支持向量机 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/useSVM.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
如果使用高斯核，需要对feature先做归一化
![]({{site.baseurl}}/img/CourseraML/高斯核归一化.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
核函数必须满足默塞尔定理，其它核函数很少使用
![]({{site.baseurl}}/img/CourseraML/其它核函数.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/CourseraML/svm多分类.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
使用SVM不用太担心全局最优的问题，训练带kernel的SVM会比较慢
![]({{site.baseurl}}/img/CourseraML/LRorSVM.png) 
</li> 
</ul> 



## 参考资料

- https://github.com/fengdu78/Coursera-ML-AndrewNg-Notes

