---

layout:     post
title:      数据挖掘领域的瑞士军刀之一：XGBoost
subtitle:   
date:       2020-02-21
author:     bjmsong
header-img: img/machineLearning/machineLearning.png
catalog: true
tags:
    - 机器学习
---

>XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable
>
>It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way.
>
>The same code runs on major distributed environment (Hadoop, SGE, MPI) and can solve problems beyond billions of examples.





### 基本原理

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/xgboost/xgboost.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/xgboost/xgboost advantage.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/xgboost/model param.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/xgboost/bias-variance.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/xgboost/regression tree.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/xgboost/regression tree not just for regression.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/xgboost/regression tree ensemble.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/xgboost/tree ensemble methods.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/xgboost/objective vs heuristic.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/xgboost/boosting.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/xgboost/boosting2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/xgboost/boosting3.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/xgboost/boosting4.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/xgboost/search algo.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/xgboost/search algo2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/xgboost/search algo3.png) 
</li> 
</ul>

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/xgboost/search algo4.png) 
</li> 
</ul>

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/xgboost/category feature.png) 
</li> 
</ul>

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/xgboost/recap.png) 
</li> 
</ul>

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/xgboost/weighted.png) 
</li> 
</ul>



### XGBoost vs GBDT

- 传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）
- 传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。
- xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性
- Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间，防止过拟合。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）
- 列抽样（column subsampling）。xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性
- 对缺失值的处理。对于特征的值有缺失的样本，xgboost可以自动学习出它的分裂方向
- xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行不是tree粒度的并行，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行

- 可并行的近似直方图算法。树节点在进行分裂时，我们需要计算每个特征的每个分割点对应的增益，即用贪心法枚举所有可能的分割点。当数据无法一次载入内存或者在分布式情况下，贪心算法效率就会变得很低，所以xgboost还提出了一种可并行的近似直方图算法，用于高效地生成候选的分割点




### XGBoost使用经验总结
- 多类别分类时，类别需要从0开始编码
- Watchlist不会影响模型训练
- 类别特征必须编码，因为xgboost把特征默认都当成数值型的
- 调参：
  - Notes on Parameter Tuning 
  - [Complete Guide to Parameter Tuning in XGBoost with codes in Python](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)
- 训练的时候，为了结果可复现，记得设置随机数种子。
- XGBoost的特征重要性是如何得到的
  - 某个特征的重要性（feature score），等于它被选中为树节点分裂特征的次数的和，比如特征A在第一次迭代中（即第一棵树）被选中了1次去分裂树节点，在第二次迭代被选中2次…..那么最终特征A的feature score就是 1+2+….
- 缺失值不需要填充



### 其它boosting算法

#### AdaBoost

- AdaBoost是基于boosting的思想，通过多个弱分类器的线性组合来得到强分类器，训练时重点关注被错分的样本，准确率高的弱分类器权重大。

  

#### GBDT

- 



#### RF

- 





### 参考资料

- Introduction to Boosted Trees , Tianqi Chen

- [XGBoost 与 Boosted Tree](https://blog.csdn.net/haoxiaoxiaoyu/article/details/78463312)

- https://www.zhihu.com/question/41354392

- http://wepon.me/2016/05/07/XGBoost%E6%B5%85%E5%85%A5%E6%B5%85%E5%87%BA/

- XGBoost.pdf , Tong He

- [XGBoost官方文档](https://xgboost.readthedocs.io/en/latest/index.html)

  

  

  


