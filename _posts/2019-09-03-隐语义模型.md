---
layout:     post
title:      隐语义模型
subtitle:   
date:       2019-09-04
author:     bjmsong
header-img: img/Recommendation System/th.jpg
catalog: true
tags:
    - 推荐系统
---


>本文将介绍推荐领域的重要算法--隐语义模型，分析其在显式数据和隐式数据上的不同处理算法，最后，比较隐语义模型和基于近邻的方法的优劣。

### 引子
2006年，Netflix举办了一届推荐系统大赛，重奖百万美金。比赛吸引了众多大神参赛，也诞生了很多著名的算法，**隐语义模型（latent factor model）** 便是其中的佼佼者。
>说句题外话，2006年到现在，Netflix股价大概涨了1000倍吧。

隐语义模型通过挖掘用户和产品的隐藏特征，从而关联用户和产品。
互联网世界中，因为用户量和产品量都非常庞大，因此用户和产品的交互数据形成的矩阵是非常庞大而稀疏的。隐语义模型通过矩阵分解的方法近似得到用户矩阵和产品矩阵两个小矩阵

具体来讲，假设用户和产品的交互矩阵A是 m × n ，即一共有 m 个用户，n 个物品。我们选一个很小的数 k，通过矩阵分解到用户矩阵 U 和 产品矩阵V，矩阵 U 的维度是 m × k，矩阵V的维度是 n × k。
即：

$$A_{m*n} \approx U_{m*k} * V_{n*k}^T$$

通过矩阵分解，每个用户和产品都映射到一个k维向量（$$p_u,q_i$$）， 这个k维向量不代表具体的含义，因此称为隐因子。


<ul> 
<li markdown="1"> 
下图展示了一个隐因子的例子，图中有两个隐因子：female vs male，serious vs escapist。我们看到，图中的人和电影都展示了他们在这两个维度上的分布。
![隐因子]({{site.baseurl}}/img/Recommendation System/LFM/隐因子.png) 
</li> 
</ul> 


>用户向量跟产品向量点积，就代表用户对该产品的兴趣程度。即：
$$r_{ui}=q_i^T*p_u$$


----
>**Netflix公开的是用户评分数据，即显式（explicit）数据，因此初期的研究都围绕显式数据。**

### SVD
隐语义模型的算法有很多（pLSA，LDA，Restricted Boltzmann Machine等），其中最著名的是SVD算法。

SVD，即奇异值分解，是线性代数的概念。然而在推荐算法中实际上使用的并不是正统的奇异值分解，而是一个伪奇异值分解,来自信息检索领域。
SVD算法的目标是最小化下面这个式子：

<ul> 
<li markdown="1"> 
![隐因子]({{site.baseurl}}/img/Recommendation System/LFM/svd_loss.png) 
</li> 
</ul> 

其中$$r_{ui}$$是已知的用户和产品的交互记录（比如打分），那么第一项是误差项，第二项就是正则项，防止模型过拟合。
求解这个loss function，通常有两种策略：
1. 随机梯度下降（SGD）
将参数（p，q）沿着损失函数梯度下降的方向迭代，直到收敛。
2. 交替最小二乘法（ALS）
ALS的思想是p和q里面我先固定一个，将问题简化成二次方程求极值的问题，然后通过最小二乘法求出解析解，然后再迭代直至收敛。

>与SGD相比，ALS在以下两个方面具有优势：
>- 可以并行：每个用户向量和每个产品向量都可以单独计算
>- 对于隐式数据，因为矩阵相对稠密，SGD计算量会比较大

>**在基础SVD基础上，诞生了很多优化版本**

#### 考虑偏置信息
不同用户打分的习惯不一样，比如有人偏爱打高分，有人偏爱打低分。因此评分应该是由用户向量和产品向量的点积，以及偏置信息（包括用户、产品和整体）共同构成。因此loss function变成了：

<ul> 
<li markdown="1"> 
![隐因子]({{site.baseurl}}/img/Recommendation System/LFM/svd_loss_bias.png) 
</li> 
</ul> 

其中，u代表整体的平均分，$$b_u$$和$$b_i$$代表了用户和产品的偏置项。

#### 加入其它数据源（SVD++）
显示特征非常稀有，隐式(implicit)特征（购买，浏览、收藏、点赞）还是非常多滴。此外，用户属性也可以加入。这样，用户对产品的偏好的表达式为：

<ul> 
<li markdown="1"> 
![隐因子]({{site.baseurl}}/img/Recommendation System/LFM/svd_loss_implicit.png) 
</li> 
</ul> 


#### 考虑时间变化
随着时间变化，用户的兴趣偏好是会变化的，因此需要把用户的隐因子向量以及偏置项当成随时间变化的量

<ul> 
<li markdown="1"> 
![隐因子]({{site.baseurl}}/img/Recommendation System/LFM/svd_loss_temporal.png) 
</li> 
</ul> 

----
> **显式数据非常稀有，隐式数据则到处都有，隐式反馈才是实际场景中最常见的**

### 隐式反馈

#### 隐式数据的特点
与显式数据相比，隐式数据具有以下四个方面的特点：
- 没有负反馈
用户没有看过某部电影，并不一定代表用户不喜欢这不电影，可能仅仅是他并不知道这部电影。在显式数据场景下，我们通常只关注用户打过分的数据。而在隐式数据场景下，那些没有发生行为的用户-产品数据也很重要，他们是负样本的主要来源。

- 数据噪声多
用户看过某部电影，也并一定能代表用户喜欢这部电影，也许他当时正在打瞌睡。因此需要我们从纷繁复杂的数据中”拨云见雾“。

- 显式数据数值的大小反映了用户的偏好程度，隐式数据数值的大小反映了置信度
在隐式数据场景下，数值的大小表示行为发生的次数，反映了用户行为的置信度。但是数值越大并不能说明用户一定更喜欢，也许用户每天看一部电视剧，但其实他更喜欢某部只看过几遍的电影。

- 评价标准
显式数据有明确的打分，我们可以把缩小误差作为目标。但是隐式数据显然麻烦得多。

#### 模型细节
首先，跟显式模型一样，我们还是需要一个用户向量（x）和一个产品向量（y）表示用户和产品在隐因子上的分布。用户向量和产品向量的点击表示用户对该产品的偏好。所不同的是，由于没有评分数据，用户真实的偏好数据从何而来？
这里需要引出以下两个变量：
- 一个叫$$p_{ui}$$，表示用户u对产品i的偏好，是通过用户和产品的隐式交互（$$r_{ui}$$）转换得到的，方式有很多，比如：

<ul> 
<li markdown="1"> 
![隐因子]({{site.baseurl}}/img/Recommendation System/LFM/pui.png) 
</li> 
</ul> 

也可以修改阈值。

- 另一个叫置信度 $$c_{ui}$$
前面也说了，隐式反馈并不一定反映用户真实的偏好，但是如果用户购买了很多次，那就比较有说服力了，置信度就是这个意思。
$$c_{ui}=1+\alpha r_{ui}$$
或者：

<ul> 
<li markdown="1"> 
![隐因子]({{site.baseurl}}/img/Recommendation System/LFM/cui2.png) 
</li> 
</ul> 


综上，那么loss function也就呼之欲出了：
<ul> 
<li markdown="1"> 
![隐因子]({{site.baseurl}}/img/Recommendation System/LFM/implicitloss.png) 
</li> 
</ul> 


#### 优化算法
显式反馈场景下，凡是没有打标签的，统统归为缺失数据了，因此数据量不大，SGD可以大展身手。但是隐式反馈场景不一样了，没有发生交互的用户-产品对也要放到模型中训练的。用户量*产品量可以轻轻松松到达10亿以上的量级。这时候，就轮到ALS上场了。
尽管ALS可以直接求解析解，但是数据量太大的矩阵运算，还是很吃力的，因此这里还需要做一些优化（此处省略500字）。最终，计算的复杂度和用户量/产品量成线性关系。

#### 推荐理由
好的推荐理由可以加深用户对推荐结果的信任度。基于近邻的算法，推荐理由比较容易得到，因为推荐结果跟用户历史行为有直接的关系。
LFM的推荐理由可以从ALS的计算过程中推导而来：

<ul> 
<li markdown="1"> 
![隐因子]({{site.baseurl}}/img/Recommendation System/LFM/explain.png) 
</li> 
</ul> 


其中,$$s_{ij^u}$$,代表的用户u心中item i 和item j的相似度，这也就可以把用户对产品的偏好（$$p_{ui}$$）跟用户的历史行为（$$c_{ui}$$）线性地联系在一起了。

#### 评估标准 
由于无法得到用户的真实偏好，尤其是负反馈数据。因此准确率指标不适合作为评估标准，召回率相关指标倒是可以采用。
比较好的指标有rank，表示用户购买的产品在推荐列表中的排名：

<ul> 
<li markdown="1"> 
![隐因子]({{site.baseurl}}/img/Recommendation System/LFM/rank.png) 
</li> 
</ul> 

#### 其它trick
- 推荐用户还没有交互过的产品的意义会大于推荐用户已经交互过的产品，因此通常在推荐结果中会剔除训练集中用户已经有过交互的产品。
- 隐式反馈数据的range有时候会很大，比如有的用户可能浏览了几百次，有的只有一两次。因此做个log scale是比较合适的。


### 再论负样本的构造
在隐式反馈数据集上应用LFM解决TopN推荐的第一个关键问题就是如何给每个用户生成负样本。
对于这个问题，Rong Pan在$$文章^4$$中进行了深入探讨。他对比了如下几种方法。
1. 对于一个用户，用他所有没有过行为的物品作为负样本。
2. 对于一个用户，从他没有过行为的物品中均匀采样出一些物品作为负样本。
3. 对于一个用户，从他没有过行为的物品中采样出一些物品作为负样本，但采样时，保证每个用户的正负样本数目相当。
4. 对于一个用户，从他没有过行为的物品中采样出一些物品作为负样本，但采样时，偏重采样不热门的物品。
对于第一种方法，它的明显缺点是负样本太多，正负样本数目相差悬殊，因而计算复杂度很高，最终结果的精度也很差。对于另外3种方法，Rong Pan在文章中表示第三种好于第二种，而
第二种好于第四种。
后来，通过2011年的KDD Cup的Yahoo! Music推荐系统比赛，我们发现对负样本采样时应该遵循以下原则。
- 对每个用户，要保证正负样本的平衡（数目相似）。
- 对每个用户采样负样本时，要选取那些很热门，而用户却没有行为的物品
下面是python代码实现负采样的过程：
```
def RandomSelectNegativeSample(items, items_pool):
    """
    :param items: 用户有过行为的列表
    :param items_pool : 候选物品的列表，在这个列表中，物品i出现的次数和物品i的流行度成正比。
    :return: ret ： 推荐列表
    """
    import random
    ret = dict()
    for i in items.keys():
        ret[i] = 1
    n = 0
    for i in range(0, len(items) * 3):
        item = items_pool[random.randint(0, len(items_pool) - 1)]
        if item in ret:
            continue
        ret[item] = 0
        n += 1
        if n > len(items):  # 负样本：正样本 = 1:1
            break
    return ret
```

### 隐语义模型 vs 基于近邻（user-based，item-based）模型
- 理论基础：LFM具有比较好的理论基础，它是一种学习方法，通过优化一个设定的指标建立最优的模型。基于邻域的方法更多的是一种基于统计的方法，并没有学习过程。
- **离线计算的空间复杂度**：基于邻域的方法需要维护一张离线的相关表。在离线计算相关表的过程中，如果用户/物品数很多，将会占据很大的内存。而LFM在建模过程中，如果是F个隐类，那么它需要的存储空间是O(F*(M+N))，这在M和N很大时可以很好地节省离线计算的内存。
- 离线计算的时间复杂度:差不多
- 在线实时推荐：LFM不可以，user-based/item-based可以
- 隐语义模型与基于近邻模型都是属于协同过滤算法。与基于近邻的方法相比，隐语义模型可以集成更多的信息：如隐式反馈，时间效应，置信度等（后面会具体讲到）。同时，基于近邻的方法具有以下问题：
- 物品之间存在相关性，信息量并不随着向量维度增加而线性增加；
- 矩阵元素稀疏，计算结果不稳定，增减一个向量维度，导致近邻结果差异很大的情况时有发生。


### 参考资料
1. matrix factorization techniques for recommendation
2. collaborative filtering for implicit feedback datasets
3. 《推荐系统实战》 2.5 隐语义模型，8.2.3 隐语义模型与矩阵分解模型
4. LFM负样本构造方法：One-Class Collaborative Filtering
5. 微信公共号 大数据与人工智能 矩阵分解推荐算法
https://mp.weixin.qq.com/s?__biz=MzI1NjM1ODEyMg==&mid=2247484539&idx=1&sn=3667e8701a081cb5431baa403999060e&chksm=ea26a7fedd512ee80e9469c15df003fed2b8d961e84930ce7a1cc3f41d6fa602a4592eb72302&scene=0&xtrack=1&key=6049ea3782f5de547e28c6c0940d81c1041efdf6c74e3e704789755e8daead3a74b74b19c4c0df5dbd235b39d833582caaadb4aca4d063e4a64838e7b0151d6b079c4453859dba459bc9aa40a39e5605&ascene=1&uin=MjM1OTMwMzkwMA%3D%3D&devicetype=Windows+7&version=62060833&lang=zh_CN&pass_ticket=9jXl3pxRz5%2Bj4h1XqlOQyQGxdRDB8phezwIpWtTI5W1DeX37m0QFplrXUNPwUh48
6. Factorization Meets the Neighborhood: a Multifaceted Collaborative Filtering Model
7. Lessons from the Netflix Prize Challenge
8. 雅虎LFM实践：Latent Factor Models for Web Recommender Systems
9. 推荐系统三十六式  https://time.geekbang.org/column/article/
