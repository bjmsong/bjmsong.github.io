---
layout:     post
title:      命名实体识别
subtitle:   
date:       2020-03-19
author:     bjmsong
header-img: img/nlp/nlp.jpg
catalog: true
tags:
    - NLP
---

> Named-entity recognition (NER)，是信息抽取的子领域。



### 发展历史

- 早期方法
	- 基于规则
	- 基于字典
- 传统机器学习
	- HMM、MEMM、CRF
- 深度学习
	- RNN-CRF
	- CNN-CRF
- 近期方法
	- 注意力模型
	- 迁移学习
	- 半监督学习



### 实现方式

早期的命名实体识别方法基本都是基于规则的。之后由于基于大规模的语料库的统计方法在自然语言处理各个方面取得不错的效果之后，一大批机器学习的方法也出现在命名实体类识别任务。宗成庆老师在统计自然语言处理一书粗略的将这些基于机器学习的命名实体识别方法划分为以下几类：

有监督的学习方法：这一类方法需要利用大规模的已标注语料对模型进行参数训练。目前常用的模型或方法包括隐马尔可夫模型、语言模型、最大熵模型、支持向量机、决策树和条件随机场等。值得一提的是，基于条件随机场的方法是命名实体识别中最成功的方法。

半监督的学习方法：这一类方法利用标注的小数据集（种子数据）自举学习。

无监督的学习方法：这一类方法利用词汇资源（如WordNet）等进行上下文聚类。

混合方法：几种模型相结合或利用统计方法和人工总结的知识库。

值得一提的是，由于深度学习在自然语言的广泛应用，基于深度学习的命名实体识别方法也展现出不错的效果，此类方法基本还是把命名实体识别当做序列标注任务来做，比较经典的方法是LSTM+CRF、BiLSTM+CRF。





### 构建NER分类器

- 定义实体种类
- 准备训练数据
	- 标记实体
- 提取特征
	- 前后的词
	- 词性
	- 前后的词的词性
- 训练NER分类器

- 基于非时序模型
	- 不考虑语句的时序关系
	- LR，RF，SVM，GBDT
	- 适用数据量少的情况
	- 多分类问题
- 基于时序模型
	- CRF：无向图  （相对于HMM:有向图）
		- log-linear model
		- 维特比、前向，后向，EM
		- CRF++
	- LSTM-CRF
		- 端到端，就不用提取特征了
		- lstm相当于提取特征
		- 需要大量数据
	- biLSTM-CRF
		- add BERT	



### 开源工具

- 英文
	- StanfordCoreNLP
	  - 安装使用（python）
	  https://github.com/Lynten/stanford-corenlp
	  https://zhuanlan.zhihu.com/p/42200126
	  https://www.analyticsvidhya.com/blog/2019/02/stanfordnlp-nlp-library-python/
	  https://stanfordnlp.github.io/CoreNLP/other-languages.html
	  - java
	  https://www.cnblogs.com/sonofelice/p/8677001.html
	- Spacy：en_core_web_sm下载不动
	- NLTK：模型较为简单，效果较差，速度慢，上手困难
- 中文
- HanLP
- fastNLP
- HITNLP
- https://github.com/shiyybua/NER



[相关数据集](https://easyai.tech/ai-definition/ner/#tools)



### 应用

- 简历分析
- 优化搜索引擎算法：对文章运行一次NER模型，并永久存储与之相关的实体
- 推荐系统



### 参考资料

https://www.leiphone.com/news/201901/r3kFsHfFi6E2LBLg.html
https://en.wikipedia.org/wiki/Named-entity_recognition