---
layout:     post
title:      图表示学习
subtitle:   
date:       2019-10-28
author:     bjmsong
header-img: img/Graph/kg.jpg
catalog: true
tags:
    - 图算法
---
>

### 图无处不在
<ul> 
<li markdown="1"> 
自然界大量的系统都可以用图来描述：社交网络，知识图谱，生物世界的网络，互联网，人体内神经元的连接，推荐系统，上海地铁等等
![]({{site.baseurl}}/img/Graph/network embedding/networks.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
移动互联网时代，各种社交app把全世界的人连接到不同的图中，每个人都是图中的一个结点。Facebook已经有20亿的月活，微信也有超过11亿的月活。    
![]({{site.baseurl}}/img/Graph/network embedding/networked world.jpg) 
</li> 
</ul> 

### 图上的机器学习任务
<ul> 
<li markdown="1"> 
有了图的数据，可以做很多有意义的机器学习任务，比如：节点的分类，边的预测，社区发现，网络相似性分析等等
![]({{site.baseurl}}/img/Graph/network embedding/network ml.jpg) 
</li> 
</ul>


<ul> 
<li markdown="1"> 
比如，商品推荐，可以看做是一个链接预测的问题。
![]({{site.baseurl}}/img/Graph/network embedding/link_prediction.jpg) 
</li> 
</ul>



<ul> 
<li markdown="1"> 
为了解决网络中的机器学习问题，我们需要从图中挖掘出有用的信息，然后把这些信息和机器学习模型结合到一起。
传统的方法，需要手工从图上提取信息，生成特征。这样的做法不仅耗时耗力，也很不灵活。
因此，我们需要有一种方法可以自动地从图上学习到特征，直接喂给下游的机器学习任务。
![]({{site.baseurl}}/img/Graph/network embedding/ml on graph.jpg) 
</li> 
</ul> 

### 什么是图表示学习
<ul> 
<li markdown="1"> 
所谓图表示学习，就是把每个结点映射到一个低维、稠密的向量中，目标是向量之间的关系可以反映结点在图中的关系。如何把结点的特征（在全局的位置，跟周围结点的关系等等）提取出来，是一个基础而十分重要的工作，图表示学习就是来解决这个问题。
![]({{site.baseurl}}/img/Graph/network embedding/node embedding.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
下图是一个空手道俱乐部的例子，左图中的边说明两个结点是朋友，颜色代表社团。右图是通过图表示学习得到的结点的向量化表示。可以看到，经过表示学习后，结点向量之间的距离接近它们在原图中的距离，然后不同社团的结点也被较为清晰地分离开来。
![]({{site.baseurl}}/img/Graph/network embedding/karate.jpg) 
</li> 
</ul>


<ul> 
<li markdown="1">
比如，我们要把雨果的小说中出现的人物进行分类（node classification），下图展示了两种不同的分类结果，颜色代表不同的分类，边代表人物之间有联系。左边的图将人物基于社群进行分类，分类的依据主要是结点在全局的位置。右边的图基于人物在相邻人物中扮演的角色，比如社群之间的连接角色是蓝色的。
![]({{site.baseurl}}/img/Graph/network embedding/node classification.jpg) 
</li> 
</ul>

### 图表示学习的难点
<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/Graph/network embedding/why hard.jpg) 
</li> 
</ul>


<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/Graph/network embedding/why hard2.png) 
</li> 
</ul>



### 经典算法介绍
<ul> 
<li markdown="1"> 
这幅图展示了知识表示学习这个领域算法的发展脉络
![]({{site.baseurl}}/img/Graph/network embedding/study.jpg) 
</li> 
</ul> 

#### DeepWalk
<ul> 
<li markdown="1"> 
DeepWalk的思想源于自然语言处理领域的word2vec，word2vec通过语料构建语言模型，得到词的向量化表示。DeepWalk通过在图上进行随机游走，把游走得到的结点序列看作是语句。
![]({{site.baseurl}}/img/Graph/network embedding/w2v.jpg) 
</li> 
</ul> 


<ul> 
<li markdown="1"> 
这两件事情确实是有很多相似的地方，在图中经过一定距离的随机游走，所经过的结点是满足幂律（或者说长尾）分布的，如左图所示。同样，在自然语言处理中，词的分布也满足幂律分布，如右图所示。
![]({{site.baseurl}}/img/Graph/network embedding/law distribution.jpg) 
</li> 
</ul> 


<ul> 
<li markdown="1"> 
我们知道，wordvec训练需要语料，那么DeepWalk的语料从何而来呢？答案是：随机游走
DeepWalk分为两个阶段，第一个阶段，在图上进行固定距离的随机游走，通过这种方式采样，得到结点序列。
![]({{site.baseurl}}/img/Graph/network embedding/deepwalk.jpg) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
第二个阶段，借鉴word2vec里的SkipGram算法，计算每一个结点上下文的似然概率，极大化似然概率，最后得到结点的向量化表示。
![]({{site.baseurl}}/img/Graph/network embedding/Representation Mapping.jpg) 
</li> 
</ul> 



#### LINE

第二个要介绍的算法是LINE，全名叫做Large-scale Information Network Embedding。顾名思义，这个算法可以应用在大规模的数据场景中。



##### LINE的优点

- 快：百万结点，十亿条边，单机可以数小时内跑完
  - 矩阵分解的方法，时间复杂度是结点的平方
- 图的类型没有限制：有向、无向、有权重、没有权重 --- 都可以！
  - DeepWalk：没法考虑边的权重
- 效果好：实验为证



##### LINE的原理

<ul> 
<li markdown="1"> 
一阶相似度：两个顶点之间的自身相似（不考虑其他顶点）。对于由边（u，v）连接的每一对顶点，边上的权重wuv表示u和v之间的相似度，如果在u和v之间没有观察到边，则它们的一阶相似度为0。
一阶邻近通常意味着现实世界网络中两个节点的相似性。例如，在社交网络中相互交友的人往往有着相似的兴趣;在万维网上相互链接的页面倾向于谈论类似的话题。
二阶相似度：顶点邻近网络结构之间的相似性。在数学上，设pu=（wu，1，...，wu，| V |）表示u与所有其他顶点的一阶相似度，则u和v之间的二阶相似度 由pu和pu决定。如果没有顶点与u和v都连接，则u和v之间的二阶相似度为0。
因为有些边观察不到等原因，一阶相似度不足以保存网络结构。因此提出共享相似邻居的顶点倾向于彼此相似，即二阶相似度。 例如，在社交网络中，分享相似朋友的人倾向于有相似的兴趣，从而成为朋友; 在词语共现网络中，总是与同一组词语共同出现的词往往具有相似的含义。
![]({{site.baseurl}}/img/Graph/network embedding/Representation Mapping.jpg) 
</li> 
</ul> 

- LINE设计了同时保留一阶相似度和二阶相似度的目标函数

- 以一阶相似度为例：

  <ul> 
  <li markdown="1"> 
  为了模拟一阶相似度，对于每个无向边（i，j），我们定义顶点vi和vj之间的联合概率如下：（sigmoid function，向量越接近，点积越大，联合概率越大）
  ![]({{site.baseurl}}/img/Graph/network embedding/LINE1.jpg) 
  </li> 
  </ul> 

  其中，ui表示节点vi对应的向量。这样就定义了一个V*V的分布p（·，·）

  <ul> 
  <li markdown="1"> 
  经验概率可以定义为：（两点之间边的权值越大，经验概率越大）
  ![]({{site.baseurl}}/img/Graph/network embedding/LINE2.jpg) 
  </li> 
  </ul> 

  其中，W=Σwi。得到了另一个分布。

  <ul> 
  <li markdown="1"> 
  为了保持一阶相似性，一个简单的办法是最小化下面的目标函数：
  ![]({{site.baseurl}}/img/Graph/network embedding/LINE3.jpg) 
  </li> 
  </ul> 

  其中d（·，·）是两个分布之间的距离。

  <ul> 
  <li markdown="1"> 
  我们选择最小化两个概率分布的KL散度，用KL散度代替d（·，·）并省略一些常数，得到：
  ![]({{site.baseurl}}/img/Graph/network embedding/LINE4.jpg) 
  </li> 
  </ul> 

#### node2vec



### 图神经网络（GNN）

<ul> 
<li markdown="1"> 
我想大家对传统的图算法应该都有所了解，比如PageRank。但是突然之间，图神经网络算法火了起来。下面这张图是近些年GNN论文发表数量的情况，我们可以看到从2016年开始，GNN论文发表数量呈指数上升趋势。
![]({{site.baseurl}}/img/Graph/network embedding/gnn paper.jpg) 
</li> 
</ul> 

​	

- 



#### GCN




### 在工商数据中的应用
qxb



### DGL

- b站有视频
https://blog.csdn.net/dQCFKyQDXYm3F8rB0/article/details/102774613 



### 参考资料

- Tutorial on Graph Representation Learning, AAAI 2019
- CNCC报告 from 唐杰 
https://www.aminer.cn/cncc19-classicml
- 《Representation Learning on Graphs: Methods and Applications》
- 《DeepWalk: Online Learning of Social Representations》
- 《LINE: Large-scale Information Network Embedding》
- 《node2vec: Scalable Feature Learning for Networks》
- https://towardsdatascience.com/node2vec-embeddings-for-graph-data-32a866340fef
- https://github.com/shenweichen/GraphEmbedding
- https://github.com/thunlp/NRLPapers
- https://github.com/thunlp/openne
- 
https://zhuanlan.zhihu.com/p/33732033
https://zhuanlan.zhihu.com/p/53194407
https://zhuanlan.zhihu.com/p/64200072
https://zhuanlan.zhihu.com/p/33732033
https://zhuanlan.zhihu.com/p/58805184


