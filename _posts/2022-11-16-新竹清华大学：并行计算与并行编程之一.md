---
layout:     post
title:      新竹清华大学：并行计算与并行编程
subtitle:   Part I Parallel Programming
date:       2022-11-16
author:     bjmsong
header-img: 
catalog: true
tags:
    - 并行计算
---
- 台湾老师的课一直很喜欢（比如李宏毅老师），质量高，中文讲述容易理解，老师很有水平教得又好，国内很少找到相同质量的课程
- https://www.bilibili.com/video/BV1Yt411W7td?p=1&vd_source=7798c62f92ce545f56fd00d4daf55e26
  + 2018 Fall
  + 周志遠 教授
  + 拆成了小段(20-30min)，一个主题一段，方便食用
- 课程主页
  + https://ocw.nthu.edu.tw/ocw/index.php?page=course&cid=231
- slides
- 参考书不重要，重要的是动手，learning by doing，代码优化，性能分析
  - Lecture，textbook：Fundamental knowledge，algorithm and theory
  - Project: coding, performance
  - Report: Performance analysis, Presentation

## 一：Intro to Parallel Computing
### Parallel Computing Introduction
1. What is parallel computing
- Solve a single problem by using multiple processors working together 
- parallel computing vs distributed computing
  + parallel computing
    * different activities happen at the same time
    * spread out a single application over many cores/processors/processes to get it done bigger or faster
  + distributed computing
    * activities across systems or distanced servers
    * focus more on concurrency and resource sharing
- The Universe is Parallel
  + 生活中并行无处不在
  
2. Why we need parallel computing
- Save time
- Solve larger problem
- Make better use of the underlying parallel hardware
- The Death of CPU Scaling 

3. Trend of Parallel Computing
- Single-Core Era
- Multi-Core Era
  + Phread -> OpenMP
- Distributed System Era
  + MPI -> MapReduce
- Heterogeneous Systems Era
  + Shader -> CUDA/OpneCL

### Classifications of Parallel Computers & Programming Models
1. Flynn's classic taxonomy
- SISD: single-core processor
- SIMD: GPU, vector processor
- MISD
- MIMD: multi-core CPU

2. Memory architecture classification
- Shared Memory
  + Single computer with multiple internal multi-core processors
  + 根据内存是否可以直接读取，分为两种
    * Uniform Memory Access (UMA)
      - equal access times to memory
    * Non-Uniform Memory Access (NUMA)
      - memory access across link is slower
- Distributed Memory
  + Connect multiple computers to form a computing platform without sharing memory
    * Cluster,Supercomputer,Datacenter
  + require a communication network to connect inter-processor memory
  + processors have their own memory & address space
  + memory change made by a processor has no effect on the memory other processors
  + programmers and programming tools are responsible to explicitly define how and when data is communicated between processors 

3. Programming model classification
- Parallel programming models exist as an abstraction above hardware & memory architecture
- In general(不绝对) programming models are designed to match the computer architecture
  + Shared memory prog. model for shared memory machine
  + Message passing prog. model for distributed memory machine
  + But programming models are not restricted by the machine or memory architecure
- Shared memory programming model
  + A single process can have multiple, concurrent execution paths
  + Threads have local data, but also, shared resources
  + Threads communicate with each other through global memory
  + Threads can come and go, but the main program remains
  + Implementation
    * A library of subroutines called from parallel source code
      - POSIX Thread(Pthread)
    * A set of compiler directives imbedded in either serial or parallel source code
      - OpenMP: 实现更简单，由编译器转换为Pthread的实现
  + Convenient
    * can share data structures
    * just annotate loops
    * Closer to serial code
  + Disadvantages
    * No locality control
    * Does not scale
    * Race conditions
- Message passing programming model
  + A set of tasks that use their own local memory during computation
  + Task exchange data through communications by sending and receiving messages(memory copy) 
  + MPI API: Send,Recv,Bcast,Gather,Scatter,etc
  + Scalable
    * Locality control
    * Communication is all explicit in code
  + Disadvantages
    * Need to rethink ehtire application/data structures
    * Lots of tedious pack/unpack code
    * Don't know when to say "receive" for some problems
- Summary
  + The designs and popularity of programming model and parallel systems are highly influenced by each other
  + openMP,MPI,Pthreads,CUDA are just some of the programming languages for users to do parallel programming
  + In reality, knowing what is parallel computing is more important than knowing how to do parallel programming, because that's how you can...

### Supercomputer & Latest Technology
1. Supercomputer
- A computer with a high-level computation capacity compared to a general-purpose computer
- Its performance is measured in FLOPS instead of MIPS
- top500.org

2. Processor technology
- CPU
  + A general purpose CPU can do anything, but its design is against the goal of achieving the best performance for a specific applicaiton 
  + 核数少，算力低，内存带宽小，内存大
- GPU
  + 核数多，算力高，内存带宽大，内存小
  + 异构架构，CUDA Programming Model
    * Host(CPU), Device(GPU)
    * Hierachical Memory Structure
      - global memory
      - PBSM：分布在独立的Stream processor中，Stream processor内部的core共享
      - register：寄存器 
  + Thread Execution Manager
    * 由硬件来调度执行
- TPU
  + 专门用于深度学习，主要是优化矩阵乘法
  + CPU按value进行计算，GPU按vector进行计算，TPU按matrix进行计算
  + https://cloud.google.com/blog/products/ai-machine-learning/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu
    * 神经网络主要是矩阵乘法运算
    * Quantization:
      - floating point(32/16-bit) operation -> integer(8-bit) operation
      - reduce the amount of memory and computing resource
    * RISC, CISC and the TPU instruction set
      - chose the Complex Instruction Set Computer (CISC) style as the basis of the TPU instruction set
      - A CISC design focuses on implementing high-level instructions that run more complex tasks (such as calculating multiply-and-add many times) with each instruction
    * Parallel Processing on the Matrix Multiplier Unit
    * The heart of the TPU: A systolic array

3. Interconnect & Network technology
- Commnuication has the most impact to the performance of parallel programs(Even more critical to computing or memory)
  + Network is generally slowly than CPU
  + Commnuication is common to parallel programs
  + Synchronization is expensive and could grow exponentially to the number of servers
- Network design considerations
  + Scalability
  + Performance
  + Resilience：复原力，当系统遇到失败时候，能够从失败中自动恢复，提供服务的能力
  + Cost
- 自底向上
  + Network Devices: Cable,Switch,Adapter...
    * Bandwidth: #bits transferred per second 
    * Latency: time to pack, unpack and send a message
    * Scalability: # of ports on the adapter and switch
  + Interconnection Network Topology
    * Network diameter
    * Re-routing path for fault tolerance
    * #fan-in & #fan-out degree per node
  + Applicaiton
    * Commnuication pattern & protocol: MPI
- Network Topology
  + 评判指标
    * Diameter(latency): 最长距离
    * Bisection(resilience): 需要砍断多少个edge，可以使拓扑网络变成两个互不相干的网络
    * #Links(Cost)
    * Degree(scalability): #fan-in & #fan-out degree per node
  + Linear Array(p nodes)
    * Diameter: p-1
    * Bisection: 1
    * #Links: p-1
    * Degree: 2
  + Ring(把Linear Array首尾相接)
    * Diameter: p/2
    * Bisection: 2
    * #Links: p
    * Degree: 2
  + Tree
  + 2-D Mesh(网格，可以扩展到三维，最常用)
    * Diameter: 2($\sqrt{p}$-1)
    * Bisection: $\sqrt{p}$ 
    * #Links: 2$\sqrt{p}$($\sqrt{p}$-1)
    * Degree: 4
  + 2-D Torus(把2-D Mesh首尾相接)
  + HyberCube
    * more suitable for smaller scale systems
- InfiniBand
  + A computer newtork communications link used in high-performance computing featuring very high throughput
  + It is the most commonly used interconnect in supercomputers
    * 超过了Ethernet
  + RDMA技术

4. I/O & Storage technology
- I/O的效率落后于计算的效率
- Memory hierarchy
  + 容量小->大，访问速度慢->快
    * register -> (L1/L2/L3)cache -> main memory -> Flash(Non-volatile memory) -> Hard Disk Drive -> Magnetic tape Storage Systems
- Parallel file and IO systems: 文件系统
  + Lustre file system
  + MPI-IO
- Burst buffering
  + Add non-volatile RAM at the IO server nodes as a buffer to smooth the burst(突然爆发的) traffic pattern for improving the IO performance of storage systems, and reduce the IO latency 
  
### Parallel Program Analysis
1. Speedup & Efficiency
- speedup factor：execution time using the best sequential algorithm / execution time using p processors
- system efficiency: speedup factor/p
  + for Linear speedup: system efficiency is 100%
- Difficult to reach ideal speedup
  + Not every part of a computation can be parallelized
  + Need extra computations in the parallel version
  + Commnuication time between processor
  
2. Strong scalability vs weak scalability
- Strong Scaling
  + The problem size stays fixed but the number of processing elements are increased
  + Linear scaling is harder to achieve, because of the commnuication overhaed may increase proportional to the scale 
- Weak Scaling 
  + The problem size assigned to each processing element stays fixed and additional processing elements are used to slove a larger total problem
  + Linear scaling is easier to achieve because programs typically employ nearest-neighbor commnuication patterns where the commnunication overhead is relatively constant regardless of the number of processes used

3. Time complexity & Cost optimality 
- computaion part
- commnuication part
  + message latency: assumed constant
  + transmission time to send data
- Trade off between computation & commnuication 

## 二：Message-Passing Programming: MPI 
### MPI Introduction
- Message Passing Interface
- A specification for the developers and users of message passing libraries
  + By itself, it is a interface not a library
  + 是一个统一定义的接口，可以有各种不同的实现(library)
- Commonly used for distributed memory system & high-perfromance computing 
- Goal
  + Portable: Run on different machines and platforms
  + Scalable: Run on million of computing nodes
  + Flexible: Isolate MPI developers from MPI programmers (users)
- Programming Model
  + SPMD: Single Program Multiple Data
    * Allow tasks to branch or conditionally execute only parts of the program they are designed to execute
    * 运行时不能调整MPI任务的任务数量
  + Distributed memory
    * MPI provide method of sending & receiving message 

### Commnuication Methods
1. Synchronous(同步) / Asynchronous(异步): the view of the part of communicated processes
- Synchronous commnuication: sending and receiving data occurs simultaneously
- Asynchronous commnuication: sending and receiving data occurs non-simultaneously

2. Blocking / Non-Blocking : the view of individual function call 
- Blocking: describe routines that do not return until the transfer is completed
- Non-Blocking: describe routines that return whether or not the meassage had been received

3. Synchronous vs Blocking
- Synchronous comm. commonly implemented by blocking call
- Synchronous comm. intrinsically performs two action: transfer data & Synchronous processes

4. Synchronous/Blocking Message Passing
- Sender: wait until the complete message can be accepted by the receiver before sending the message
- Receiver: wait until the message it is expecting arrives

5. Asynchronous/Non-Blocking Message Passing
- message buffer is a memory space at the send and/or receiver side 
- For send rountine, once the local actions have been completed and the message is safely on its way, the process rountine can continue with subsequent work

### MPI API
1. Getting start
- Commnuicators and Groups
  + Groups define which collection of processes may communicate with each other
  + Each group is associated with a communicator to perform its commnuication function cals
  + pre-defined communicator for all processors: MPI_COMM_WORLD
- Rank
  + an unique identifier for each process in a communicator 
  
2. Environment Management Routine
- MPI_Init()
  + initializes the MPI execution environment
  + must be called before any other MPI functions
  + must be called only once in an MPI program
- MPI_Finalize()
  + Terminates the MPI execution environment
  + No other MPI routines may be called after it
- MPI_Comm_size(comm, &size)
  + Determines the number of processes in the group associated with a communicator 
- MPI_Comm_rank(comm, &rank)
  + Determines the rank of the calling process within the communicator
  
3. Point-to-Point Communication Routines: 点对点通信
- API
  + Blocking send: MPI_Send
  + Non-Blocking send: MPI_lsend
  + Blocking receive: MPI_Recv
  + Non-Blocking receive: MPI_lrecv
- 参数解释
  + buffer: Address space that references the data to be sent or received
  + type: MPI_CHAR,MPI_INT....
  + count: indicates the number of data elements of a particular type to be sent or received
  + comm: indicates the commnuication context
  + source/dest: the rank(task ID) of the sender/receiver
  + tag: arbitrary non-negative integer assigned by the programmer to uniquely identify a message
  + status: status after operation
  + request: used by non-blocking send and receive operations
- MPI_Wait(): blocks until the operation has actually completed
- MPI_Test(): returns with a flag set indicating whether operation completed at that time 

4. Collective Commnuication Routines: 集体通信
- MPI_Barrier(comm)
  + Creates a barrier synchronization in a group
  + Blocks until all tasks in the group reach the same MPI_Barrier call
- MPI_Bcast(&buffer, count, datatype, root, comm)
  + Broadcasts(sends) a message from the process with rank "root" to all other processes in the group
- MPI_Scatter
  + Distributes distinct messages from a source task to all tasks
- MPI_Gather
  + Gather distinct messages from each task in the group to a single destination task
  + reserve operation of MPI_Scatter
- MPI_Reduce
  + Applies a reduction operation on all tasks in the group and places the result in one task
- MPI_Allgather
  + Concatenation of data to all tasks
  + equivalent to an MPI_Gather followed by an MPI_Bcast
    * but more efficient 
- MPI_Allreduce
  + Applies a reduction operation and places the result in all tasks
  + equivalent to an MPI_Reduce followed by an MPI_Bcast

5. Group and Commnuicator Management Routines
- Group & Communicator data type
  + MPI_Group
  + MPI_Comm
- MPI_Comm_group(Comm, &Group)
  + Access the group associated with a given communicator
- MPI_Group_incl(Group, size, ranks[], &NewGroup)
  + Produce a group by including a subset of members from an existing group
- MPI_Comm_create(Comm, NewGroup, &NewComm)
  + Create a new communicator

### MPI-IO
- 需要分布式文件系统 
- An HPC platform's I/O subsystems are typically slow as compared to other part
  + I/O gap between memory speed and average disk access stands at roughly 10^-3
- MPI_File_open()
  + File is opened only once in a collective manner
  + MPI library will share and synchronize with each other to use the same file handler
  + Can handle both read and write together
- MPI_File_close()
- Collective I/O: MPI_File_read/write_all()
  + Read/write to a shared memory buffer, then issue ONE file request
  + 优点：reduce numbers of I/O request
    * 把所有I/O合并成一个I/O, Good for small I/O
  + Require synchronization
- Independent I/O: MPI_File_read/write()
  + Read/write individually
  + Prevent synchronization
  + One request per process
  + Request is serialized if acccess the same OST
  + Good for large I/O
  + 如果是写，需要程序员控制写入的顺序及位置
- MPI_File_sync()

## 三：Shared-Memory Programming: Pthread
### Shared-memory Programming
- Defination
  + Processes communicate or work together with each other through a shared memory space which can be accessed by all processes 
    * Faster & more efficient than message passing
- Many issues as well
  + Synchronization
  + Deadlock
  + Cache coherence(一致性)
- Programming techniques
  + Parallelizing compiler: OpenMP
  + Threads: Pthread, Java等语言也有自己的实现
    * low-level的API
  + Unix processes
- Threads(线程) vs Processes(进程)
  + Process: heavyweight process, complete separate program with its own variables,stack,heap and everything else 
  + Thread: lightweight process, share the same memory sapce for global variables, resources
  + In Linux: Threads are created via clone a process with a flag to indicate the level a sharing
    * Linux不区分Thread和process，两者只是share程度的差异
  + The creation of a thread is much more efficient than fork a process
  + The inter-communication(Memory-to-CPU) in thread is much more faster to MPI

### Pthread
- Pthread is the implementation of POSIX standard for thread
  + POSIX(Portable Operating System Interface) standard is specific for portability across Unix-like system
    * similar concept as MPI for message passing libraries 
  + same relation between MPICH and MPI
  + 其API就是用来create，delete，join，不需要commnuicate的api，因为是share memory
- pthread_creation(thread,attr,routine,arg)
  + thread: An unique identifier (token) for the new thread
  + attr: 
  + routine: The routine/function that the thread will execute once it is created
  + arg: 函数的参数
- pthread_join(threadid, status)
  + Blocks until the specified threadid thread terminates
  + One way to accomplish synchronization between threads
  + 有点类似MPI_Barrier
- pthread_detach(threadid)
  + Once a thread is detached, it can never be joined
  + Detach a thread could free some system resources
- c++，java有更high level的api
  + 底层实现就是pthread
- 多线程每个线程可以做不同的事情，MPI都是做一样的事情(?存疑)

### Synchronization Problem & Tools
- Synchronization Problem
  + The outcome of data content should not be decided by the execution order among processes
    * 受CPU调度等的影响，实际指令执行的顺序可能跟程序预想的不一致
    * 一行程序会被编译成多行指令，不同thread之间的指令可能会穿拆执行（并发），造成不可预期的结果
    * 因为是shared memory，不同线程都可以修改相同的变量
- Critical Section & Mutual Exclusion: 一种理论的解决方案
  + Critical Section is a piece of code that can only be accessed by one process/thread at a time
  + Mutual Exclusion is the problem to insure only one process/thread can be in a critical section
- Phread
  + Mutually exclusion Lock
    * Lock: the simplest mechanism for ensuring mutual exclusion of critical section  
    * locks are implemented in Pthreads by a special type of variables "mutex"
      - Mutex is abbreviation of "mutual exclusion"  
    
    ```C
    #include "pthread.h"
    pthread_mutex_t mutex;
    pthread_mutex_init(&mutex, NULL);
    pthread_mutex_lock(&mutex);

      Critical Secion

    pthread_mutex_unlock(&mutex);
    pthread_mutex_destory(&mutex);
    ```

  + Condition variable(CV)
    * CV represent some condition that a thread can:
      - Wait on, until the condition occurs; or
      - Notify other waiting threads that the condition has occured
  
  + Thread Pools
    * Create a number of threads in a pool where they await work
    * Advantages
      - Usually slightly faster to service a request with an existing thread than creating a new thread
      - Allows the number of threads in the application to be bound to the size of the pool
- POSIX Semaphore
  + A tool to generalize the synchronization problem
  + a record of how many units of a particular resource are available 
    * if #record = 1 -> mutex lock
    * if #record > 1 -> counting semaphore
  + Semaphore is part of POSIX standard but it's not belonged to Pthread
    * It can be used with or without thread
  + Drawback
    * its correctness is depending on the programmer
      - All processes access a shared data object must execute wait() and signal() in the right order and right place
- JAVA Monitor
  + High-level synchronization construct that allows the safe sharing of an abstract data type among concurrent processes
  + synchronized method uses the method receiver as a lock

  ```Java
  public class SynchronizedCounter {
    private int c = 0;
    public synchronized void increment() {c++;}
    public synchronized void decrement() {c--;}
    public synchronized int value() {return c;}
  }
  ```

- How to pick between locks, semaphores, convars, monitors?
  + Locks are very simple for many cases
    * But may not be the most efficient solution
  + Condition variables allow threads to sleep while holding a lock
    * event driven
  + Semaphores provide general functionality
  + Monitors are a pattern for using locks and condition variables
    * 比较容易描述，但是执行效率低


## 四：Shared-Memory Programming: OpenMP Programming
- OpenMP: Open specification for Multi-Processing
- An API：multi-thread，shared memory parallelism
- Portable: the API is specified for C/C++ and Fortran
- Fork-Join model: the master thread forks specified number of slave threads and divides among them
- Compiler Directive Based: Compiler takes care of generating code that forks/joins thread and divide tasks to threads

```C
#include <omp.h>
int A[10], B[10], C[10];

// Beginning of parallel section. Fork a team of threads
#pragma omp parallel for num_threads(10)
{
  for(int i =0; i<10; i++)
    A[i] = B[i] + C[i]; 
} // All threads join master thread and terminate
```

- OpenMP Directive
  + #pragma omp
  + directive-name: 用哪一种方式做并行
    * parallel
    * do
    * for
  + \[clause...\]: Optional
  + newline
- 参考资料
  - https://gcc.gnu.org/onlinedocs/libgomp.pdf








