---
layout:     post
title:      深度学习框架的加速方法
subtitle:   
date:       2022-11-26
author:     bjmsong
header-img: 
catalog: true
tags:
    - 深度学习框架
---
## 
- 多线程
- 分布式
- GPU加速
- 《动手学深度学习》里面讲到的方法
- ...

## 使用tensorflow进行分布式计算
- 六个并行计算框架
    - MirroredStrategy：适合一台机器上有多个GPU
        - 同步随机梯度下降
        - 原理：类似MapReduce
            - Ring All-Reduce
    - TPUStrategy
    - MultiWorkerMirroredStrategy
    - CentralStorageStrategy
    - ParameterServerStrategy
    - OneDeviceStrategy
- code
    - learn/MachineLearning/tf_distribute.py
- 《Hands on ml2》Chapter 19
- 业界实践
https://zhuanlan.zhihu.com/p/430383324
- 默认使用全部CPU
https://stackoverflow.com/questions/42845261/does-tensorflow-job-use-multiple-cores-by-default
https://stackoverflow.com/questions/38836269/does-tensorflow-view-all-cpus-of-one-machine-as-one-device
- 数据并行：更容易实现，tensorflow目前只支持这个
https://www.pythonf.cn/read/100405
https://cloud.tencent.com/developer/article/1453361
https://tf.wiki/zh_hans/appendix/distributed.html
- 模型并行
    - 比较难
    - https://www.tensorflow.org/tutorials/distribute/keras?hl=zh-cn
    - https://medium.com/analytics-vidhya/speeding-up-inference-using-parallel-model-runs-d76dcf393567   
- 分布式训练
https://zhuanlan.zhihu.com/p/469541810
https://zhuanlan.zhihu.com/p/56991108
https://www.tensorflow.org/guide/distributed_training?hl=zh-cn
https://github.com/yahoo/TensorFlowOnSpark
https://towardsdatascience.com/distribute-your-pytorch-model-in-less-than-20-lines-of-code-61a786e6e7b0
https://blog.csdn.net/weixin_39589455/article/details/120759372

## 使用Pytorch进行分布式训练
- https://zhuanlan.zhihu.com/p/113694038
- https://pytorch.org/tutorials/beginner/dist_overview.html
- https://pytorch.org/tutorials/intermediate/dist_tuto.html
- https://zhuanlan.zhihu.com/p/361314953

