---
layout:     post
title:      统计学习方法之五
subtitle:   决策树
date:       2019-11-17
author:     bjmsong
header-img: img/machineLearning/machineLearning.png
catalog: true
tags:
    - 机器学习
---
>决策树是一种基本的分类与回归方法。它可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。



### 决策树

- 模型

  - 决策树：
    - 结点
      - 内部结点：表示一个特征或属性
      - 叶子结点：表示一个类
    - 有向边
  - 从根节点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点。如此递归地对实例进行测试并分配，直至达到叶子结点。最后将实例分到叶子结点的类中。

- 策略

  - 损失函数：x正则化的极大似然函数

- 算法

  - 启发式方法，递归地选择最优特征

- 步骤

  - 特征选择
  - 决策树的生成
  - 剪枝

- 优点

  - 可解释性较强
  - 分类速度快
  - 特征选择

- 信息增益

  - 熵（entropy）：表示随机变量不确定性的度量，熵越大，随机变量的不确定性就越大

    $$H(p) = -\sum{p_i}logp_i$$

  - 条件熵：已知随机变量X的条件下随机变量Y的不确定性

    $$H(Y|X) = \sum{p_iH(Y|X=x_i)}$$

    其中，$$p_i=P(X=x_i)$$

  - 信息增益：表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。特征A对训练数据集D的信息增益$$g(D,A)$$，定义为集合D的经验熵$$H(D)$$与特征A给定条件下D的经验条件熵$$H(D|A)$$之差。

    $$g(D,A)=H(D)-H(D|A)$$

  - 信息增益比：

    $$g_R(D,A)=\frac{g(D,A)}{H_A(D)}$$

- 决策树的生成

  - 学习局部的模型
  - ID3
    - 选择信息增益最大的特征
    - 偏向于选择取值较多的特征
  - C4.5
    - 选择信息增益比最大的特征

- 决策树的剪枝（pruning）

  - 学习整体的模型

  - 最小化决策树整体的损失函数

    $$C_{\alpha}(T) = \sum{N_tH_t(T)}+\alpha|T|$$

    其中，|T|是树T的叶子结点的个数，t是树T的叶子结点，该节点由$$N_t$$个样本点，$$H_t(T)$$为叶子结点t熵的经验熵

  - 递归的从树的叶子结点向上回缩

  - 可以由动态规划的算法实现

- CART（classification and regression tree，1984）

  - 假设决策树是二叉树
  - 决策树生成
    - 回归树
      - 平方误差最小化
      - 递归地遍历变量，对固定的切分变量j扫描切分点s
    - 分类树
      - 基尼指数最小化
        - 基尼指数越大，样本的不确定性也就越大
  - 决策树剪枝
    - 从底端开始不断剪枝，直到根结点，形成一个子树序列
    - 通过交叉验证法在独立的验证数据集熵对子树序列进行测试，从中选择最优子树

- 树模型（决策树、RF、xgb）需要归一化特征吗

  - 不需要
- 数值缩放不影响分裂点的位置
  - 非树形结构模型（Adaboost、SVM、LR）需要做归一化
    - 特征值差别很大时，运用梯度下降的时候，损失等高线是椭圆形，需要进行多次迭代才能到达最优点。但是如果进行了归一化，那么等高线就是圆形的，促使SGD往原点迭代，从而导致需要的迭代次数较少
  
- 代码实现

  

   





## 参考资料
