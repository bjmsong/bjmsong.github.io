---
layout:     post
title:      李宏毅机器学习之一
subtitle:   
date:       2020-03-25
author:     bjmsong
header-img: img/machineLearning/machineLearning.png
catalog: true
tags:
    - 机器学习
---

> 入门最佳课程
>
> 本文将介绍Bias & Variance，梯度下降、分类、逻辑回归、神经网络



### Learning Map

<ul> 
<li markdown="1"> 
Structured Learning:除分类、回归以外的机器学习任务，如语音识别、机器翻译
![]({{site.baseurl}}/img/machineLearning/机器学习概貌.png) 
</li> 
</ul> 



### Bias & Variance

<ul> 
<li markdown="1"> 
机器学习的误差来源于两个方面：bias，variance。每次采样一批样本做预测，bias是预测的期望值和真实值的差距，variance是单个预测结果和预测的期望值的差距。
![]({{site.baseurl}}/img/machineLearning/bias_variance.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/bias.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/bias_variance2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
诊断误差的来源，才能找到提升模型的方法    
![]({{site.baseurl}}/img/machineLearning/large_bias.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/large_variance.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/cross_validation.png) 
</li> 
</ul> 



### 梯度下降

<ul> 
<li markdown="1"> 
Loss Function要选凸函数：只有全局最优，没有局部最优
![]({{site.baseurl}}/img/machineLearning/梯度下降.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/梯度下降公式.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
做梯度下降要画loss随参数更新的变化趋势图
![]({{site.baseurl}}/img/machineLearning/learning_rate.png) 
</li> 
</ul> 



#### Tips

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/adaptive_learning_rate.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/adagrad.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/adagrad2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/machineLearning/随机梯度下降.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
通过feature scaling，loss下降更有效率
![]({{site.baseurl}}/img/machineLearning/特征缩放.png) 
</li> 
</ul> 



#### 数学推导

<ul> 
<li markdown="1"> 
初始化点(a,b)，在(a,b)为圆心的一个小圆范围内寻找点，使得loss funcion减小。在(a,b)附近做一阶泰勒展开
![]({{site.baseurl}}/img/machineLearning/梯度下降数学原理.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
loss funcion可以表示成两个向量点积的形式（如图），要使得值最小，两个向量应该相反，且值最大（达到圆的边界）
![]({{site.baseurl}}/img/machineLearning/梯度下降数学原理2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1"> 
要保证这个推导成立，泰勒展开要成立，那么圆的范围应该尽量小，也即learning rate尽量小。因此如果learning rate比较大，每一步迭代后，loss不一定下降。
![]({{site.baseurl}}/img/machineLearning/梯度下降数学原理3.png) 
</li> 
</ul> 

- 牛顿法：二阶泰勒展开

<ul> 
<li markdown="1"> 
对于多维变量，鞍点其实不是个问题
![]({{site.baseurl}}/img/machineLearning/梯度下降法的局限.png) 
</li> 
</ul> 



### 分类: Probabilistic Generative Model(生成式模型)

<ul> 
<li markdown="1"> 
假设每个类别的样本都服从某一种分布（比如高斯分布），通过最大似然求出分布的参数，然后求出样本属于某个类别的概率。
![]({{site.baseurl}}/img/machineLearning/generative_model.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
降低模型的复杂度，假设不同类别的分布的协方差矩阵是一样的
![]({{site.baseurl}}/img/machineLearning/极大似然求分布参数.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
假设不同特征来自于独立的分布，就可以得到Naive Bayes分类器。但是这显然是一个比较强的假设，如果数据不满足这个假设，NB的效果会比较差。
![]({{site.baseurl}}/img/machineLearning/naive_bayes.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
Generative Model -> LR !!!
![]({{site.baseurl}}/img/machineLearning/推导LR.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
中间省略若干步推导，得到Logistic Regression！因此，其实不需要绕一圈去估计分布的参数，直接求解参数w和b即可
![]({{site.baseurl}}/img/machineLearning/推导LR2.png) 
</li> 
</ul> 



### 逻辑回归

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/逻辑回归步骤一.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
最大化似然函数，可以求得最优的w和b。Loss funcion一般是越小越好，因此在前面加负号，再加log，将乘积变成求和。    
![]({{site.baseurl}}/img/machineLearning/逻辑回归步骤二.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
为了统一表达式，将式子转化成以下的形式。这个式子也可以理解为两个伯努利分布的交叉熵，当两个分布越接近，交叉熵越小。   
![]({{site.baseurl}}/img/machineLearning/逻辑回归步骤二2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
逻辑回归和线性回归，梯度下降的式子是完全一样的！
![]({{site.baseurl}}/img/machineLearning/lr对比.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
如果逻辑回归采用平方误差呢
![]({{site.baseurl}}/img/machineLearning/逻辑回归平方误差.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
均方误差的损失函数较为平坦，在离目标值很远的地方，梯度也很小，这样做梯度下降就很困难。交叉熵损失函数则不同，在离目标值很远的地方，梯度很大
![]({{site.baseurl}}/img/machineLearning/交叉熵vs平方误差.png) 
</li> 
</ul> 



#### 判别式算法 vs 生成式算法

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/判别式生成式.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
生成式算法需要预设一些假设，如果样本不满足假设，效果不会好。比如下面的例子：人可以很简单地判断出这个testing data是属于类别1的，但是朴素贝叶斯计算的结果却是属于类别2。
![]({{site.baseurl}}/img/machineLearning/生成式的缺点.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
但是也不是说生成式算法一定比判别式算法差
![]({{site.baseurl}}/img/machineLearning/生成式的优点.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/多类别分类.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
决策边界是直线（线性分类器）
![]({{site.baseurl}}/img/machineLearning/逻辑回归的局限.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
可以通过特征转换（特征工程）来解决这个问题，但是构造特征比较不容易
![]({{site.baseurl}}/img/machineLearning/逻辑回归的局限2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
通过两层逻辑回归，可以达到跟特征转换一样的效果 -- 其实就也是Deep Learning的雏形
![]({{site.baseurl}}/img/machineLearning/逻辑回归的局限3.png) 
</li> 
</ul> 



### Deep Learning Introduction

<ul> 
<li markdown="1">
上个世纪五六十年代就掀起过浪潮，也曾被学术界嗤之以鼻，如今又再次被奉为“显学”,几经起落，令人唏嘘
![]({{site.baseurl}}/img/machineLearning/深度学习的起起伏伏.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
跟传统机器学习一样，深度学习还是三个步骤：
![]({{site.baseurl}}/img/machineLearning/三板斧.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
将神经网络运算过程写成矩阵运算的好处是：可以用GPU加速
![]({{site.baseurl}}/img/machineLearning/神经网络矩阵运算.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/神经网络结构.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/神经网络的loss.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/mini-batch.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
同一个batch的样本，GPU会并行运算，batch size=10比batch size=1运算效率更高。但是batch size如果太高，效果反而不会好，因为丢掉了随机梯度下降的优点。
![]({{site.baseurl}}/img/machineLearning/batch-size.png) 
</li> 
</ul> 



#### Keras

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/keras2.0.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/keras2.02.png) 
</li> 
</ul> 



#### Tips  For Deep Learning

<ul> 
<li markdown="1">
先检查一下模型在训练集上的表现，不要将一切原因归为“过拟合”
![]({{site.baseurl}}/img/machineLearning/recipe.png) 
</li> 
</ul> 



#### New Activation Function

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/recipe2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
sigmoid激活函数会带来梯度消失的问题：在接近输入的地方梯度很小，当接近输出神经元的参数已经收敛，接近输入的神经元参数还是基本random。
![]({{site.baseurl}}/img/machineLearning/vanish.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
背后的原因是sigmoid函数，会把输入进行压缩，而梯度可以理解为输入的变化引起输出变化的大小
![]({{site.baseurl}}/img/machineLearning/vanish2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
ReLU激活函数可以解决这个问题
![]({{site.baseurl}}/img/machineLearning/relu.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/relu2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/maxout.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
maxout可以学出激活函数，每个神经元激活函数不同
![]({{site.baseurl}}/img/machineLearning/maxout2.png) 
</li> 
</ul> 



#### Adaptive Learning Rate

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/adagrad3.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
深度学习的loss function曲面更复杂，需要更灵活的learning rate
![]({{site.baseurl}}/img/machineLearning/rmsprop.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/rmsprop2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
local minmum一般不是问题：loca minmum的条件是每一维特征的梯度都是零，对于高维特征，很难同时满足
![]({{site.baseurl}}/img/machineLearning/局部最优.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/momentum.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
每次参数更新的方向，除了梯度，还会考虑之前的运行速度
![]({{site.baseurl}}/img/machineLearning/momentum2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/adam.png) 
</li> 
</ul> 



#### Early Stopping

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/early_stopping.png) 
</li> 
</ul> 



#### Regularization

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/regularization.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
L1正则在参数迭代时减去一个很小的数，所以可以实现参数稀疏化，L2正则则是在参数迭代时乘一个很小的数
![]({{site.baseurl}}/img/machineLearning/regularization2.png) 
</li> 
</ul> 



#### Dropout

<ul> 
<li markdown="1">
dropout会使训练集效果变差
![]({{site.baseurl}}/img/machineLearning/dropout.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/dropout2.png) 
</li> 
</ul> 

<ul> 
<li markdown="1">
![]({{site.baseurl}}/img/machineLearning/dropout3.png) 
</li> 
</ul> 



### 参考资料

- [课程主页](http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML16.html)

