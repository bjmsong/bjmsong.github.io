---
layout:     post
title:      【转载】SimRank
subtitle:   
date:       2019-09-04
author:     bjmsong
header-img: img/Graph/kg.jpg
catalog: true
tags:
    - 图算法
---

### 基本概念
[PageRank](https://bjmsong.github.io/2019/09/04/PageRank/)，Topic-Senstive PageRank算法计算是是网络节点在整个图中的重要性大小，然而在很多场景中，我们更关心不同结点之间的关联性。比如，推荐算法中我们想知道用户对哪些产品会感兴趣。SimRank算法就是来解决这方面的问题。**SimRank的基本思想很自然，如果两个结点关联的结点是类似的，那么这两个结点就是相似度比较高的。**
SimRank的基本公式是：

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/Graph/SimRank/SimRank.png) 
</li> 
</ul> 

s(a,b)是节点a和b的相似度，当a=b时，s(a,b)=1。Ii(a)表示a的第i个in-neighbor。当I(a)=∅或I(b)=∅时式(1)为0。公式用一句话描述就是：**a和b的相似度等于a的in-neighbors和b的in-neighbors相似度的平均值**。参数C是个阻尼系数，它的含义可以这么理解：假如I(a)=I(b)={A}，按照(1)式计算出sim(a,b)=C*sim(A,A)=C，所以C∈(0,1)。
应用到二部图（如用户-购买-产品）的场景，就是

<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/Graph/SimRank/SimRank_Bipartite.png) 
</li> 
</ul> 

上式的意思是买家A和B的相似度等于他们购买的物品之间相似度的平均值，物品a和b的相似度是购买它们的买家之间相似度的平均值。

对于非二部图的情况，一个节点既可能有in-neighbors也可能有out-neighbors，比如在论文引用的场景下，一篇论文既可能引用其他论文，也可能被其他论文引用。站在引用的角度，两篇论文的相似度为
<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/Graph/SimRank/SimRank_NoBipartite_1.png) 
</li> 
</ul> 


站在被引用的角度，两篇论文的相似度为
<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/Graph/SimRank/SimRank_NoBipartite_2.png) 
</li> 
</ul> 

### Naive SimRank
SimRank迭代算法：
<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/Graph/SimRank/SimRank迭代算法.png) 
</li> 
</ul> 


Rk(∗,∗) 是k的单调不减函数，limk→∞Rk(a,b)=s(a,b)，实践中发现Rk(∗,∗)收敛得很快，k不需要设得太大。

下面给出矩阵的形式，因为直接使用上面的迭代公式很难展开并行计算，数量稍微大一些（比如上十万）时在单机上跑时间和空间开销非常大。
SimRank迭代算法：
<ul> 
<li markdown="1"> 
![]({{site.baseurl}}/img/Graph/SimRank/SimRank迭代矩阵形式.png) 
</li> 
</ul> 

S是相似度矩阵。Q是转移概率矩阵，它的每一列和为1，如果从节点i可以转移到节点j，并且这样的节点i一共有n个，则Qi,j=1n

### 优化算法
- 平方缓存法：加速收敛
- Arnoldi迭代降维法：对矩阵进行降维
- **simRank++**：即使我们通过平方缓存法优化了SimRank的迭代次数，通过对Q进行降维避开了大矩阵相乘带来的时空消耗的困扰，但是到最后还原S的时候还是避不开大矩阵相乘，所以最根本的解决办法还是使用Hadoop/Spark进行矩阵的并行运算



### [spark实现](https://github.com/thunderain-project/examples)



### 参考资料

1. https://www.cnblogs.com/zhangchaoyang/articles/4575809.html
2. SimRank;A Measure of Structural-Context Similarity
3. https://www.cnblogs.com/pinard/p/6362647.html
4. https://blog.csdn.net/dengxing1234/article/details/78933187