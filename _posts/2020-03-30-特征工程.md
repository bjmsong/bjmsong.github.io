---
layout:     post
title:      特征工程
subtitle:   
date:       2020-03-30
author:     bjmsong
header-img: img/machineLearning/machineLearning.png
catalog: true
tags:
    - 机器学习

---

> 特征没做好，参数调到老
>
> 特征工程：简而言之，就是用最好的方法来表示数据的艺术/科学。“最好的”是什么意思？大体上，提供给算法的数据的方式，应该以最有效的方式表示潜在信息的相关结构/属性。当你进行特征工程时，你是在把你的数据属性转化为数据特征。
> 好的特征工程是专业知识，直觉和基础的数学能力的优雅组合。

### 特征工程的目的
旨在去除原始数据中的杂质和冗余，设计更高效的特征以刻画求解的问题与预测模型之间的关系

### 构建特征

- **重要的事情说三遍：基于业务！基于业务！基于业务！不要拍脑袋**
- 可以尝试先用规则去解决这个问题，在这个过程中发现这个项目目标实际影响的因素，作为之后建模参考的变量
- 变量不是越多越好，以下几类变量要警惕加入模型：
  - 缺失值太多的
  - 数据源准确性不佳的
  - 取数过程工作量极大的
  - 与其它变量有较大相关性的
- 归一化：可以让梯度下降更快收敛，树模型不需要
  - min-max scaling
  - z-score normalization
- 好的特征具有如下的特点：
  - 有区分性（Informative）
  - 特征之间相互独立（Independent）
  - 简单易于理解（Simple）


### 做探索性数据分析（EDA）

- **建模前，先分析数据，如果觉得人也没法从这个数据中得到想要的结果，不能指望机器能做到**

- **探索性数据分析（EDA）目的是最大化对数据的直觉，完成这个事情的方法只能是结合统计学的图形以各种形式展现出来。**

- 通过EDA可以实现
  ❏	得到数据的直观表现 
  ❏	发现潜在的结构 
  ❏	提取重要的变量 
  ❏	处理异常值 
  ❏	检验统计假设 
  ❏	建立初步模型 
  ❏	决定最优因子的设置

- 探索性数据分析常用的一些常见问题

  1. 数据的典型值是多少(均值，中位数等)？

  2. 变量的缺失性怎么样？

  3. 一组数据的良好分布拟合是什么？

  4. 一个因子是否有影响？

  5. 最重要的因素是什么？

  6. 我们可以将时间相关数据中的信号与噪声分离吗？

  7. 我们可以从多变量数据中提取任何结构吗？

  8. 数据是否有离群值？ 如t-sne可以实现对高维数据集降维可视化



### 数据清洗

- **数据（变量，目标值）的正确性无比关键，脏数据训练的结果就是garbage in，garbage out**

- 缺失值处理
	- **根据业务经验填充，重要变量尽量填充，避免做决策树模型时因为该值null，分到不合理的节点**
	- 将“是否缺失”作为一个新的变量
	- 模型预测缺失值
		- sknn：最近k个邻（3,5），距离平方的倒数为权，从缺失最少的开始，填充数据可以用于后续计算
		- svd：寻找与数据X同大小填补矩阵Y，最小化Φ=‖X-eTμ-Y‖，SVD迭代求解
		- EM：选取初始簇中心，E步（贝叶斯最大后验簇分配），M步（重新分配簇）
  - https://blog.csdn.net/jingyi130705008/article/details/82670011?utm_source=blogxgwz9
  - http://blog.sina.com.cn/s/blog_670445240102v08m.html
- 异常值处理
	- 基于统计的异常点检测算法
		- 3sigma：与平均值超过3倍标准差，数据需满足正态分布
			- 箱线图：大于QU+1.5IQR,小于QL−1.5IQR.QU:上四分位数,QL:下四分位数,IQR:QU-QL.
	- 基于距离的异常点检测算法
		- 主要通过距离方法来检测异常点，将数据集中与大多数点之间距离大于某个阈值的点视为异常点，主要使用的距离度量方法有绝对距离 ( 曼哈顿距离 ) 、欧氏距离和马氏距离等方法。
	- 基于密度的异常点检测算法
		- 考察当前点周围密度，可以发现局部异常点，例如LOF算法
	- 异常检测算法（孤立森林，多元高斯分布）
	- 异常值的判断和处理
	  - 类别型异常：稀有事件（<1%）
	  - 连续型：3个标准差之外
	  - 需要结合业务逻辑：是真异常，还是另类？是否代表一类有特殊需求的用户？
	  - 删除异常值有利于模型的稳定
	  - 异常值也可以是一个新的业务方向，比如新用户类型、恶意欺诈用户



#### 特征筛选

- 意义
   - 少而精的自变量有助于提高模型的稳定性
   - 有效自变量可以提高模型预测能力
   - 提高运算速度和效率

 - 基础筛选

   - 去除常量
   - 去除缺失率高的变量
   - 取值过泛的变量，比如邮编

 - 结合业务经验的筛选：让业务专家来讨论变量的重要性，但需要用数据再验证
   
 - RF/xgboost等树模型
   
 - L1正则

 - 不推荐以下方法
   
    - PCA不适合用来防止过拟合，适用于数据降维（为了减少存储开销等），可视化
    - 求自变量和因变量的相关性：也许自变量和因变量有非线性的关系，仅根据相关性不能作为筛选依据 
    - sklearn.RFECV：计算量大
          
 - 自变量与因变量  
      - 相关性过低的（对于连续型）
      - 卡方检验不显著（对于离散型）
      - 信息价值IV（information value）过低
        - [IV & WOE](https://blog.csdn.net/kevin7658/article/details/50780391)	
      - 基尼系数（Gini Score）过低
      - 这些检验可能表示自变量不重要，但不能排除交互作用
      
- https://www.cnblogs.com/hhh5460/p/5186226.html  
- https://mp.weixin.qq.com/s?__biz=Mzg5ODAzMTkyMg==&mid=2247486594&idx=1&sn=7800b2aabac7d208921be259478ac620&chksm=c06984dff71e0dc9410037d407393153411294ae7adec11d7bef1b7996c064a854d0909cae6e&mpshare=1&scene=1&srcid=&sharer_sharetime=1564447357577&sharer_shareid=49581f7bdbef8664715f595bc62d7044&key=8a4c04f4ab18b288d2023df92965edae448fa68263e4981c3ff42a601e22dfe6e9751e9dc425f59672be06cf394405fa6b3ba34674b51dd4db9c5e020948f360dd3c91b39c74092b7b5a74bfd58130f6&ascene=1&uin=MjM1OTMwMzkwMA%3D%3D&devicetype=Windows+7&version=62060833&lang=zh_CN&pass_ticket=pBdN32AlyZ77Ty%2BYvZUYJTqAYVrcJZCIivujUOoaFs75nQNfGLTNMuQcGwRf7Pzp

### 组合特征
- gbdt+lr
- FM，FFM


### 文本特征
- bag of words
- tf-idf
- 主题模型
- word2vec
- https://www.jiqizhixin.com/articles/2017-04-30-9
- 张俊林：放弃幻想，全面拥抱Transformer：NLP三大特征抽取器（CNN/RNN/TF）比较


### 类别变量的处理 
- 序号编码：类别之间存在大小关系，如成绩“高”、“中”、“低”
- 独热编码（one-hot encoding）
  - 类别取值较多的情况下可以考虑：用稀疏向量存储、配合特征选择来降低维度
  - 缺点：数据稀疏、所有的类别同样重要，无法体现出不同样本之间的关系
  - multi-hot encoding：可以取到同一个类别中的多个值，如用户看过的电影
- 二进制编码：维度少于独热编码，节省了存储空间
- **embedding**
  - https://mp.weixin.qq.com/s?__biz=Mzg5ODAzMTkyMg==&mid=2247486758&idx=1&sn=1a5b2558511fc52621300ca45f27356c&chksm=c069857bf71e0c6d866b6b8cbef4fed0225f1e4acc3208fd44fbef737a9901704f4b3a89306e&mpshare=1&scene=1&srcid=&sharer_sharetime=1566085108359&sharer_shareid=602b1ccf63ca4ea52755ecd058f6d407&key=8a4c04f4ab18b288a2b1e2103adeeb4e7952007b30884c7484c137968fd745d35a8a4b501869275fe676f69010d14f457a372ef4786f5df8b08b9c956eb27d54b931cd34464b8ba75546b8af6ec2e9e7&ascene=1&uin=MjM1OTMwMzkwMA%3D%3D&devicetype=Windows+10&version=62060833&lang=en&pass_ticket=IUUUfbK16TvO1gSfqeJNL4tVQmZmZcGoL1QKs%2BBeaVEhl7p4xYuuhsscZVM1JWZn
  - https://www.zhihu.com/question/34819617
- Hash成64位：前16位空置，中间12位为Slot_id,最后36位为FeatureID（即原ID的二进制表示）
      
### 特征衍生

- 基于原有特征构造
  - 单变量：x^2、e^x、偏度较大的特征：log、Box－Cox变换
  - 多变量：线性组合、非线性组合、x/y、x*y
  - 核函数
- 定量特征分桶
  ■	之后不需要再做one-hot-encoder
  ■	其同一区间的数字能够代表相同的特征
  ■	减少过拟合
- 连续值排序：df.rank
- 按时间偏移：对于部分时间序列变量存在“时滞”效应
- K-means对样本聚类，以所属类别的哑编码为新特征
- Auto-encoder
  - GBDT算法的特性，在面对高维稀疏矩阵的时候效果并不好。因为为了控制过拟合，GBDT通常会限制叶子节点的样本数目，稀疏矩阵经过少量分支就已经到达了减枝条件，也就无法体现深度挖掘组合关系的特点。
  - 采用autoencoder在没有人工干预的情况下自动将稀疏特征编码
  - 通过一个深度学习网络结构，用特征经过一系列网络结构预测自己。这样的模型设定通过设定最优化目标是是否可以通过自己预测自己，保证在降低数据噪音、降低数据维度的同时，最大限度的保留原有数据的信息量。
- 特征自动生成：`featuretools`


### 其它操作

- [卡方分箱](https://www.cnblogs.com/wqbin/p/10547167.html)
- 抽样
  - 训练集样本至少1000，保证模型的稳定性
  - 验证集可以是训练集的1/10~1/2
  - 训练集必须是自变量数的10倍以上
  - 目标事件必须是自变量数的6~8倍以上
  - 自变量数一般在8~20个，过多、过少都不一定好



### 参考资料

- Coursera How to Win a Data Science Competition: Learn from Top Kagglers
- 百面机器学习，第一章 特征工程
- https://www.zhihu.com/question/29316149/answer/607394337
- https://www.slideshare.net/HJvanVeen/feature-engineering-72376750
- https://zhuanlan.zhihu.com/p/71609765
- Dipanjan Sarkar：不会做特征工程的 AI 研究员不是好数据科学家
https://www.leiphone.com/news/201801/T9JlyTOAMxFZvWly.html
https://www.leiphone.com/news/201801/KTVu68zA6szteVmS.html
- 《特征工程入门与实践》
https://mp.weixin.qq.com/s?__biz=MzI1MzY0MzE4Mg==&mid=2247485232&idx=1&sn=e83004347d6b5c993af3cfa64f533a0c&chksm=e9d017f1dea79ee7f2e399884b90f3e55c33c1c98747fb55f9a4f5257bcce22374465a344f64&scene=0&xtrack=1&key=c86a338f58bd007cdffd48c99e9e2298a13ebf736243b5d96f69a9361632ffa5a9388e60759653767b14c83729bcbdc54fb6f30806f1c212ff1fb14b99f43248d0ece5001c9cbc78052f7ba5cda89774&ascene=1&uin=MjM1OTMwMzkwMA%3D%3D&devicetype=Windows+7&version=62060833&lang=zh_CN&pass_ticket=fZx3R6hAbiq5JXQFJM5ZrEYTbNGY4PKgyOa4uD91VEjM4%2FQUCMtw94n3V1bJeCDB
- https://github.com/Pysamlam/Tips-of-Feature-engineering
