---
layout:     post
title:      机器学习系统：设计和实现
subtitle:   之一
date:       2022-09-02
author:     bjmsong
header-img: 
catalog: true
tags:
    - ML System
---
## 2. 导论
### 机器学习应用
- 学习模式
    - 监督学习：已知输入输出对应关系情况下的学习，比如：给定输入图像和它对应的内容标签，学习图像分类（Classification）。
    - 无监督学习：只有输入数据但不知道输出标签情况下的学习，比如：给定一堆猫和狗的图像，自主学会猫和狗的分类，这种无监督分类也称为聚类（Clustering）。
    - 强化学习：给定一个学习环境和任务目标，算法自主地去不断尝试、改进自己、以实现任务目标 ，比如： AlphaGo围棋就是用强化学习实现的，给定的环境是围棋的规则、而目标则是胜利得分。
- 应用领域
    - 计算机视觉
    - 自然语言处理
    - 智能决策
- 机器学习系统位于机器学习应用和通用处理器/智能加速器之间

### 设计目标
- 支持多种神经网络
- 支持自动微分
- 支持数据管理和处理
- 支持模型的训练和部署
- 高效使用硬件加速器
- 分布式计算

- 机器学习系统：TensorFlow，PyTorch、PaddlePaddle、MindSpore

### 基本组成
- 编程接口
    - 不仅需要高层次简易编程（Python，Julia和Java），同时也需要支持低层次高性能编程（利用C和C++函数调用操作系统和硬件加速器）
- 计算图
    - 利用不同编程接口实现的机器学习程序需要共享一个运行后端。实现这一后端的关键技术是：应用无关的计算图。计算图包含计算节点，节点之间的边表达计算依赖。计算图可以被同步和异步执行。
- 编译器前端
    - 给定一个计算图，机器学习框架会对计算图做一系列优化。和硬件无关的优化由编译器前端实现。编译器前端实现包括：中间表达，自动微分，类型推导和静态分析等等。
- 编译器后端和运行时
    - 机器学习框架利用编译器后端对计算图可以进一步针对硬件的特性（例如说，L2/L3大小，指令流水线长度）进行性能优化。最终优化后的计算图通过运行时执行在通用处理器（CPU）或者是硬件加速器之上。运行时需要实现算子选择和内存分配等技术
- 硬件加速器
    - 现代硬件加速器提供了丰富的编程接口。
- 数据处理
    - 机器学习系统拥有专门的数据处理框架来实现数据读取，存储和预处理的功能由数据处理模块（例如，TensorFlow的tf.data和PyTorch的DataLoader）。这一框架需要针对机器学习应用实现易用性，保序性和高效性等设计目标。
- 模型部署
    - 为了确保模型可以在内存有限的硬件上执行，我们会使用模型转换，量化，蒸馏等模型压缩技术。同时，我们也需要实现针对推理硬件平台（例如，英伟达Jetson）的模型算子优化。最后，为了保证模型的安全（不被黑客窃取），实践者还会对模型进行混淆设计。
- 分布式训练

## 3. 编程接口
- 平衡框架性能和易用性
    - 为了达到最优的性能，开发者需要利用硬件亲和的编程语言如：C和C++来进行开发
        - C和C++的使用使得机器学习框架可以高效调用硬件的底层API，从而最大限度发挥硬件性能
        - 现代操作系统（如Linux和Windows）提供丰富的基于C和C++的编程接口（如文件系统，网络编程，多线程管理等），通过直接调用操作系统API，可以降低框架运行的开销
    - 从易用性的角度分析，机器学习框架的使用者往往具有丰富的行业背景（如数据科学家，生物学家，化学家，物理学家等）。他们常用的编程语言是高层次脚本语言：Python，Matlab，R和Julia
- 核心设计目标
    - 要具有易用编程接口来支持用户用高层次语言如Python来实现机器学习算法
    - 也要具备以C和C++为核心的低层次编程接口，使得框架开发者可以用C和C++实现大量高性能组件，从而在硬件上高效执行。

### 机器学习系统编程模型的演进
- 第一代：高层次编程语言来编写机器学习程序：Lua（Torch），Python（Theano）
- 第二代：以C和C++作为核心编程接口：Caffe
    - 训练深度神经网络需要消耗大量的算力，而这些算力无法被以Lua和Python所主导开发的Torch和Theano所满足
    - 计算加速卡（如英伟达GPU）的通用编程接口（例如CUDA C）日趋成熟，而构建于CPU多核技术之上的多线程库（POSIX Threads）也被广大开发者所接受。因此，许多的机器学习用户希望基于C和C++来开发高性能的深度学习应用
- 第三代：Python作为面向用户的主要前端语言，而利用C和C++实现高性能后端：TensorFlow，PyTorch，MXNet，CNTK
    - 大量基于Python的前端API确保了TensorFlow可以被大量的数据科学家和机器学习科学家接受，同时帮助TensorFlow能够快速融入Python为主导的大数据生态（大量的大数据开发库如Numpy，Pandas，SciPy， Matplotlib和PySpark）
    - Python具有出色的和C语言的互操作性，这种互操作性已经在多个Python库中得到验证
    - Keras和TensorLayer等高层次机器学习开发库提供了更高层次的Python API从而可以快速导入已有的模型， 这些高层次API进一步屏蔽了底层框架的实现细节，因此Keras和TensorLayer可以运行在不同的机器学习框架之上。
    - JAX
    - MindSpore
        - MindSpore在继承了TensorFlow，PyTorch的Python和C/C++的混合接口的基础上，进一步拓展了机器学习编程模型从而可以高效支持多种AI后端芯片（如华为Ascend，英伟达GPU和ARM芯片），实现了机器学习应用在海量异构设备上的快速部署
        - MindSpore进一步完善了机器学习框架的分布式编程模型的能力，从而让单节点的MindSpore程序可以无缝地运行在海量节点上

### 机器学习工作流
- 数据处理：读取、预处理、特征变换
- 模型结构
- 损失函数和优化算法
- 训练过程：给定一个数据集、模型、损失函数和优化器，用户需要训练API来定义一个循环（Loop）从而将数据集中的数据按照小批量（mini-batch）的方式读取出来，反复计算梯度来更新模型。这个反复的过程称为训练
- 测试和调试

### 定义深度神经网络
- 构建深度神经网络结构始终遵循最基本的规则：
1. 承载计算的节点；
2. 可变化的节点权重（节点权重可训练）；
3. 允许数据流动的节点连接
- 机器学习编程库中神经网络是以层为核心，它提供了各类神经网络层基本组件；将神经网络层组件按照网络结构进行堆叠、连接就能构造出神经网络模型。
#### 以层为核心定义神经网络
- 常用层：全连接、卷积、池化、RNN、BatchNorm、dropout...

#### 神经网络层的实现原理
- 机器学习编程库大都提供了更高级用户友好的API，它将神经网络层抽象成一个基类，所有的神经网络层实现都继承基类调用低级API
    - 不需要用户手动管理训练变量
- 神经网络层需要的功能：该层的训练参数（变量，包括初始化方法和训练状态）以及计算过程
- 神经网络模型需要的功能：对神经网络层管理和神经网络层参数的管理
    - MindSpore的Cell、PyTorch的Module
    - 将Layer(负责单个神经网络层的参数构建和前向计算)和Model(负责对神经网络层进行连接组合和神经网络层参数管理)抽象成一个方法，该方法既能表示单层神经网络层也能表示包含多个神经网络层堆叠的模型
- 神经网络抽象方法(以MindSpore的Cell为例)
    - 构造器(__init__): 初始化神经网络层和神经网络层参数的存储
        - self._params = OrderedDict()
            - OrderedDict: 初始化神经网络层和神经网络层参数的存储；它的输出是一个有序的，相比与Dict更适合深度学习这种模型堆叠的模式。
        - self._cells = OrderedDict()
        - self.training = False
    - 设置属性(__setattr__): 参数和神经网络层的管理
        - isinstance(value, parameter)
        - isinstance(value, Cell)
    - 执行计算(__call__): 实现神经网络层时在这里定义计算过程
        - 动态图模式：转发计算并返回结果
        - 静态图模式：编译并运行cell
    - 训练参数(parameters_and_names): 为了给优化器传所有训练参数
        - 遍历神经网络层，返回所有神经网络层参数
    - 神经网络层(cells_and_names)
        - 返回所有神经网络层迭代器
    - 自定义方法


#### 自定义神经网络层

#### 自定义神经网络模型
- 神经网络模型也可以采用SubClass的方法自定义神经网络模型


### C/C++编程接口
- 很多时候，用户需要添加自定义的算子来帮助实现新的模型、优化器
、数据处理函数等。这些自定义算子需要通过C和C++实现，从而获得最优性能。但是为了帮助这些算子被用户使用，他们也需要暴露为Python函数，从而方便用户整合入已有的Python为核心编写的工作流和模型

#### 在Python中调用C/C++函数的原理
- 现代机器学习框架（包括TensorFlow，PyTorch和MindSpore）主要依赖**Pybind11**来将底层的大量C和C++函数自动生成对应的Python函数，这一过程一般被称为Python绑定（Binding）
- 在Pybind11出现以前，将C和C++函数进行Python绑定的手段主要包括：
    - Python的C-API。这种方式要求在一个C++程序中包含Python.h，并使用Python的C-API对Python语言进行操作。使用这套API需要对Python的底层实现有一定了解，比如如何管理引用计数等，具有较高的使用门槛。
        - https://blog.csdn.net/fitzzhang/article/details/79212411
        - https://www.bilibili.com/video/BV1GS4y1E7Az/?spm_id_from=333.999.0.0&vd_source=7798c62f92ce545f56fd00d4daf55e26
            - https://github.com/ttttLarva/cpython-extension-example
    - 简单包装界面产生器（Simplified Wrapper and Interface Generator，SWIG)。SWIG可以将C和C++代码暴露给Python。SWIG是TensorFlow早期使用的方式。这种方式需要用户编写一个复杂的SWIG接口声明文件，并使用SWIG自动生成使用Python C-API的C代码。自动生成的代码可读性很低，因此具有很大代码维护开销。
    - Python的ctypes模块，提供了C语言中的类型，以及直接调用动态链接库的能力。缺点是依赖于C的原生的类型，对自定义类型支持不好。
    - Cython是结合了Python和C语言的一种语言，可以简单的认为就是给Python加上了静态类型后的语法，使用者可以维持大部分的Python语法。Cython编写的函数会被自动转译为C和C++代码，因此在Cython中可以插入对于C/C++函数的调用。
    - Boost::Python是一个C++库。它可以将C++函数暴露为Python函数。其原理和Python C-API类似，但是使用方法更简单。然而，由于引入了Boost库，因此有沉重的第三方依赖。
- 相对于上述的提供Python绑定的手段，Pybind11提供了类似于Boost::Python的简洁性和易用性，但是其通过专注支持C++ 11，并且去除Boost依赖，因此成为了轻量级的Python库，从而特别适合在一个复杂的C++项目中暴露大量的Python函数。

#### 添加C++编写的自定义算子
- 算子是构建神经网络的基础，在前面也称为低级API；通过算子的封装可以实现各类神经网络层，当开发神经网络层遇到内置算子无法满足时，可以通过自定义算子来实现
- 以MindSpore为例，实现一个GPU算子需要如下步骤：
    - Primitive注册：算子原语是构建网络模型的基础单元，用户可以直接或者间接调用算子原语搭建一个神经网络模型。
    - GPU Kernel实现：GPU Kernel用于调用GPU实现加速计算。 
    - GPU Kernel注册：算子注册用于将GPU Kernel及必要信息注册给框架，由框架完成对GPU Kernel的调用。


## 4. 计算图
- 计算图的设计背景和作用
    - 计算框架在后端会将前端语言构建的神经网络模型前向计算与反向梯度计算以计算图的形式来进行表示。
    - 早期神经网络的拓扑结构简单（神经网络层往往通过串行构建）
        - 拓扑结构可以用简单的配置文件来表达（例如Caffe中基于Protocol Buffer格式的模型定义）
    - 模型的拓扑日益复杂（包括混合专家，生成对抗网络，多注意力模型）。这些复杂的模型拓扑结构（例如：分支结构，带有条件的if-else循环）会影响模型算子的执行、自动微分以及训练参数的自动化判断
    - 计算图对于一个机器学习框架提供了以下几个关键作用
        - 作为一个统一的数据结构来表达用户用不同语言编写的训练程序。这个数据结构可以准确表述用户的输入数据、模型所带有的多个算子，以及算子之间的执行顺序。
        - 定义中间状态和模型状态
        - 自动化计算梯度
        - 优化程序执行：通过利用计算图来分析模型中算子的执行关系，机器学习框架可以更好地发现将算子进行异步执行的机会，从而以更快的速度完成模型程序的执行。
- 计算图的基本构成
    - 基本数据结构：张量
        - 计算图中表示为节点间的有向线段
        - 属性：形状、维度、数据类型、存储位置、名字
        - 在特定的环境中，也会使用特殊类型的张量，比如不规则张量和稀疏张量(COO)
    - 基本运算单元：算子
        - 计算图中表示为节点
        - 张量操作: 包括张量的结构操作和张量的数学运算
        - 神经网络操作: 包括特征提取、激活函数、损失函数、优化算法等
        - 数据流操作: 数据的预处理与数据载入相关算子
        - 控制流操作
    - 计算依赖
        - 算子之间存在依赖关系
        - 有向无环图(DAG)
    - 控制流
    - 基于链式法则计算梯度 
- 计算图的生成
    - 静态生成
        - 采用先编译后执行的方式，该模式将计算图的定义和执行进行分离
        - 使用前端语言定义模型形成完整的程序表达后，并不使用前端语言解释器进行执行，而是将前端描述的完整模型交给计算框架。框架在执行模型计算之前会首先对神经网络模型进行分析，获取网络层之间的连接拓扑关系以及参数变量设置、损失函数等信息，接着用一种特殊的静态数据结构来描述拓扑结构及其他神经网络模型组件，这种特殊的静态数据结构通常被称为静态计算图。
    - 动态生成
        - 
- 计算图的调度

## 参考资料
- https://openmlsys.github.io/
- https://zhuanlan.zhihu.com/p/484878657