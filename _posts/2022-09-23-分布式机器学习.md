---
layout:     post
title:      分布式机器学习
subtitle:   
date:       2022-09-23
author:     bjmsong
header-img: 
catalog: true
tags:
    - 并行计算
---
## 分布式深度学习框架的前世今生，从 MapReduce 到 Pathways
https://zhuanlan.zhihu.com/p/533852982?utm_id=0
- 上古时代的分布式机器学习平台：MPI,Hadoop,Spark
- 参数服务器
- Pathways

## 《分布式机器学习》（刘铁岩）
https://zhuanlan.zhihu.com/p/377147020


## Paper
- Scaling Distributed Machine Learning with System and Algorithm Co-design
- LARGE BATCH TRAINING OF CONVOLUTIONAL NETWORKS
- Highly Scalable Deep Learning Training System with Mixed-Precision: Training ImageNet in Four Minutes
- Accurate, large minibatch sgd: Training imagenet in 1 hour
    - https://www.zhihu.com/question/60874090/answer/181413785
    - https://blog.csdn.net/Jing_xian/article/details/79982209
- Imagenet training in minutes
- https://zhuanlan.zhihu.com/p/29032307

## 《Dive into Big Model Training》
https://github.com/qhliu26/BM-Training
https://zhuanlan.zhihu.com/p/506052424?utm_id=0


## 李沐 动手学深度学习
- 分布式学习

## CS583: Deep Learning
https://github.com/wangshusen/DeepLearning
### Parallel Computing
- 并行梯度下降：数据分布在不同处理器上，分别计算梯度
- 通信方式
    - 共享内存(share memory): 没有办法做到大规模并行
    - 消息传递(message passing)：节点之间通信
- 架构
    - client-server：worker node上存储数据、做计算
    - peer-to-peer：没有server，所有的节点都做计算，每个节点都有几个邻居，邻居之间可以通信 
#### MapReduce
-《MapReduce: Simplified Data Processing on Large Clusters》
- 架构：client-server
- 通信：message passing
- 并行：同步（所有的worker完成工作才能进行下一轮）
- google的MapReduce不开源，因此有很多开源实现(Hadoop,Spark)
- 适合做大数据处理，但是来做机器学习不高效 
- broadcast：server可以把信息广播到所有的worker节点上
- map：所有的worker并行做的操作
- reduce：worker把计算结果传回给server，server把结果整合起来 
- 数据并行
- 时间开销
    - computation
    - Communication
        - Communication complexity：how many words are transmitted between server and workers
        - Latency(网络延迟)：
        - Communication time： Communication complexity/bandwith + Latency
    - 同步
        - 所有的worker都必须等最慢的worker
#### Parameter Server
- 当前主流
- 异步
    - 比同步快很多
    - 不同worker之间效率要差不多，不然收敛会有问题 
    - 《hogwild a lock-free approach to parallelizing stochastic gradient descent》
- client-server
- message passing
- 框架：Ray
    - https://github.com/ray-project/ray
- 数据并行


#### Decentralized network
- peer-to-peer：没有server，所有节点都是worker
- message passing
- 数据并行
- 同步 or 异步
- 目前不太流行

#### 使用tensorflow进行分布式计算
- 六个并行计算框架
    - MirroredStrategy：适合一台机器上有多个GPU
        - 同步随机梯度下降
        - 原理：类似MapReduce
            - Ring All-Reduce
    - TPUStrategy
    - MultiWorkerMirroredStrategy
    - CentralStorageStrategy
    - ParameterServerStrategy
    - OneDeviceStrategy
- code
    - learn/MachineLearning/tf_distribute.py
- 《Hands on ml2》Chapter 19
- 业界实践
https://zhuanlan.zhihu.com/p/430383324
- 默认使用全部CPU
https://stackoverflow.com/questions/42845261/does-tensorflow-job-use-multiple-cores-by-default
https://stackoverflow.com/questions/38836269/does-tensorflow-view-all-cpus-of-one-machine-as-one-device
- 数据并行：更容易实现，tensorflow目前只支持这个
https://www.pythonf.cn/read/100405
https://cloud.tencent.com/developer/article/1453361
https://tf.wiki/zh_hans/appendix/distributed.html
- 模型并行
    - 比较难
    - https://www.tensorflow.org/tutorials/distribute/keras?hl=zh-cn
    - https://medium.com/analytics-vidhya/speeding-up-inference-using-parallel-model-runs-d76dcf393567   
- 分布式训练
https://zhuanlan.zhihu.com/p/469541810
https://zhuanlan.zhihu.com/p/56991108
https://www.tensorflow.org/guide/distributed_training?hl=zh-cn
https://github.com/yahoo/TensorFlowOnSpark
https://towardsdatascience.com/distribute-your-pytorch-model-in-less-than-20-lines-of-code-61a786e6e7b0
https://blog.csdn.net/weixin_39589455/article/details/120759372



#### 联邦学习
- https://www.zhihu.com/question/315844487/answer/920036089
- 《A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection》

## Pathways
- 李沐读paper


## ZeRo
https://zhuanlan.zhihu.com/p/394064174?utm_id=0


## MPI
https://zhuanlan.zhihu.com/p/355652501
https://mpitutorial.com/tutorials/mpi-introduction/zh_cn/
https://www.aminer.cn/research_report/5d00a91800eea1f1d521d890
https://www.youtube.com/watch?v=36nCgG40DJo&ab_channel=SharcnetHPC


## ICML'22大模型技术Tutorial
https://zhuanlan.zhihu.com/p/562741952?utm_campaign=shareopn&utm_medium=social&utm_oi=30949685329920&utm_psn=1556101177766563840&utm_source=wechat_session&s_r=0
https://github.com/alpa-projects/alpa

## Horovod
https://zhuanlan.zhihu.com/p/40578792

## DeepSpeed
https://blog.csdn.net/u010751000/article/details/123516433
https://github.com/microsoft/DeepSpeed
https://zhuanlan.zhihu.com/p/414773915

## Jax
https://zhuanlan.zhihu.com/p/468223891
https://www.zhihu.com/question/306496943
https://zhuanlan.zhihu.com/p/536377657


## 其它参考资料
- https://www.zhihu.com/question/24070061/answer/1325421472
- 《高级能计算并行编程技术--MPI并行程序设计》
- https://zhuanlan.zhihu.com/p/450689346
- https://zhuanlan.zhihu.com/p/466002243
- http://www.cnblogs.com/LeftNotEasy/archive/2010/11/27/1889598.html
- paper
    - FlumeJava : Easy,Efficient Data-Parallel Pipelines
    - Large-scale Incremental Processing Using Distributed Transactions and Notifications
    - SPark: CLuster Computing with Working Sets
    - Pregel:A System for Large-Scale Graph Processing