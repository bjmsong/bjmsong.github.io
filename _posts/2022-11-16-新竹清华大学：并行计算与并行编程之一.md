---
layout:     post
title:      新竹清华大学：并行计算与并行编程
subtitle:   Part I Parallel Programming
date:       2022-11-16
author:     bjmsong
header-img: 
catalog: true
tags:
    - 并行计算
---
- 台湾老师的课一直很喜欢（例如李宏毅老师），质量高，中文讲述容易理解，老师很有水平教得又好，国内很少找到相同质量的课程
- https://www.bilibili.com/video/BV1Yt411W7td?p=1&vd_source=7798c62f92ce545f56fd00d4daf55e26
  + 2018 Fall
  + 周志遠 教授
  + 拆成了小段(20-30min)，方便食用
- 课程主页
  + https://ocw.nthu.edu.tw/ocw/index.php?page=course&cid=231
- slides
- 参考书不重要，重要的是动手，learning by doing，代码优化，性能分析
  - Lecture，textbook：Fundamental knowledge，algorithm and theory
  - Project: coding, performance
  - Report: Performance analysis, Presentation

## Intro to Parallel Computing
### Parallel Computing Introduction
1. What is parallel computing
- Solve a single problem by using multiple processors working together 
- parallel computing vs distributed computing
  + parallel computing
    * different activities happen at the same time
    * spread out a single application over many cores/processors/processes to get it done bigger or faster
  + distributed computing
    * activities across systems or distanced servers
    * focus more on concurrency and resource sharing
- The Universe is Parallel
  + 生活中并行无处不在
  
2. Why we need parallel computing
- Save time
- Solve larger problem
- Make better use of the underlying parallel hardware
- The Death of CPU Scaling 

3. Trend of Parallel Computing
- Single-Core Era
- Multi-Core Era
  + Phread -> OpenMP
- Distributed System Era
  + MPI -> MapReduce
- Heterogeneous Systems Era
  + Shader -> CUDA -> OpneCL

### Classifications of Parallel Computers & Programming Models
1. Flynn's classic taxonomy
- SISD: single-core processor
- SIMD: GPU, vector processor
- MISD
- MIMD: multi-core CPU

2. Memory architecture classification
- Shared Memory
  + Single computer with multiple internal multi-core processors
  + 根据内存是否可以直接读取，分为两种
    * Uniform Memory Access (UMA)
      - equal access times to memory
    * Non-Uniform Memory Access (NUMA)
      - memory access across link is slower
- Distributed Memory
  + Connect multiple computers to form a computing platform without sharing memory
    * Cluster,Supercomputer,Datacenter
  + require a communication network to connect inter-processor memory
  + processors have their own memory & address space
  + memory change made by a processor has no effect on the memory other processors
  + programmers and programming tools are responsible to explicitly define how and when data is communicated between processors 

3. Programming model classification
- Parallel programming models exist as an abstraction above hardware & memory architecture
- In general(不绝对) programming models are designed to match the computer architecture
  + Shared memory prog. model for shared memory machine
  + Message passing prog. model for distributed memory machine
- but programming models are not restricted by the machine or memory architecure
- Shared memory programming model
  + A single process can have multiple, concurrent execution paths
  + Threads have local data, but also, shared resources
  + Threads communicate with each other through global memory
  + Threads can come and go, but the main program remains
  + Implementation
    * A library of subroutines called from parallel source code
      - POSIX Thread(Pthread)
    * A set of compiler directives imbedded in either serial or parallel source code
      - OpenMP: 实现更简单，由编译器转换为Pthread的实现
  + Convenient
    * can share data structures
    * just annotate loops
    * Closer to serial code
  + Disadvantages
    * No locality control
    * Does not scale
    * Race conditions
- Message passing programming model
  + A set of tasks that use their own local memory during computation
  + Task exchange data through communications by sending and receiving messages(memory copy) 
  + MPI API: Send,Recv,Bcast,Gather,Scatter,etc
  + Scalable
    * Locality control
    * Communication is all explicit in code
  + Disadvantages
    * Need to rethink ehtire application/data structures
    * Lots of tedious pack/unpack code
    * Don't know when to say "receive" for some problems
- Summary
  + The designs and popularity of programming model and parallel systems are highly influenced by each other
  + openMP,MPI,Pthreads,CUDA are just some of the programming languages for users to do parallel programming
  + In reality, knowing what is parallel computing is more important than knowing how to do parallel programming, because that's how you can...

### Supercomputer & Latest Technology
1. 

### Parallel Program Analysis
1. 

## MPI Programming
- message passing 

## Pthread Programming
- 

## OpenMP Programming
- 
