---
layout:     post
title:      深度模型加速
subtitle:   
date:       2022-07-25
author:     bjmsong
header-img: 
catalog: true
tags:
    - 模型优化与部署
---
>模型压缩和加速是两个不同的话题，有时候压缩并不一定能带来加速的效果，有时候又是相辅相成的。压缩重点在于减少网络参数量，加速则侧重在降低计算复杂度、提升并行能力等。

## 背景
- AI模型越来越复杂，训练和预测需要大量算力，大量时间
- 很多场景下算力、存储、电量等有限，比如移动端AI落地
- 目标：模型尺寸小、计算复杂度低、电池耗电量低、下发更新部署灵活


## 算法层压缩加速


## 框架层加速


## 硬件层加速,异构计算
- 有GPU、FPGA、ASIC等多种方案，各种TPU、NPU就是ASIC这种方案，通过专门为深度学习进行芯片定制，大大加速模型运行速度

## CNN加速：Winograd算法

## 其它
- 读取训练文件
    https://tech.meituan.com/2018/06/07/searchads-dnn.html
    https://www.cnblogs.com/wj-1314/p/11211333.html
    https://www.tensorflow.org/tutorials/load_data/csv?hl=zh-cn
    https://www.tensorflow.org/tutorials/load_data/tfrecord?hl=zh-cn
    - spark输出训练文件（TensorFlow Record格式） 
        https://zhuanlan.zhihu.com/p/352025069
        https://cloud.tencent.com/developer/news/639997  
        - 归一化这步local来完成 
    - 数据预读：用多进程的方式，将HDFS上预处理好的数据拉取到本地磁盘（使用joblib库+shell将HDFS数据用多进程的方式拉取到本地，基本可以打满节点带宽2.4GB/s，所以，拉取数据也可以在10分钟内完成）
    - 程序通过TensorFlow提供的TFrecordReader的方式读取本地磁盘上的数据，这部分的性能提升是最为明显的。原有的程序处理数据的性能大概是1000条/秒，而通过TFrecordReader读取数据并且处理，性能大概是18000条/秒，性能大概提升了18倍



## 参考资料
- https://towardsdatascience.com/how-to-reduce-the-training-time-of-your-neural-network-from-hours-to-minutes-fe7533a3eec5
- https://towardsdatascience.com/the-deep-learning-inference-acceleration-blog-series-part-1-introduction-668e39b8b14b
- https://blog.csdn.net/nature553863/article/details/81083955
- https://github.com/scutan90/DeepLearning-500-questions/blob/master/ch17_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2/%E7%AC%AC%E5%8D%81%E4%B8%83%E7%AB%A0_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%E3%80%81%E5%8A%A0%E9%80%9F%E5%8F%8A%E7%A7%BB%E5%8A%A8%E7%AB%AF%E9%83%A8%E7%BD%B2.md
- https://mp.weixin.qq.com/s/VdmukMxAFTJYQmqXP7rrcg
- https://blog.csdn.net/zeusee/article/details/89601634
- https://www.cnblogs.com/LXP-Never/p/14833772.html