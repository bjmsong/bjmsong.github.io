---
layout:     post
title:      统计学习方法之七
subtitle:   逻辑回归与最大熵模型
date:       2019-12-01
author:     bjmsong
header-img: img/machineLearning/machineLearning.png
catalog: true
tags:
    - 机器学习
---
>逻辑回归虽说名字里面有“回归”，却是经典分类方法。最大熵是概率模型学习的一个准则，将其推广到分类问题得到最大熵模型。逻辑回归与最大熵模型都属于对数线性模型。

### 逻辑斯蒂分布

- 分布函数

  $$F(x) = P(X<=x) = \frac{1}{1+e^{\frac{-(x-\mu)}{\gamma}}}$$

- 概率密度函数（纵轴是随机变量等于横轴的概率）

  $$f(x) = F^{'}(x) = \frac{e^{\frac{-(x-\mu)}{\gamma}}}{\gamma(1+e^{\frac{-(x-\mu)}{\gamma}})^2} $$

  <ul> 
  <li markdown="1"> 
  ![]({{site.baseurl}}/img/machinelearning/逻辑斯蒂分布.jpg) 
  </li> 
  </ul> 

- 逻辑斯蒂分布函数形状类似阶跃函数（因此可以较好地表示二分类问题），同时其连续可导



### 逻辑回归模型

$$P(Y=1|x) = \frac{exp(w*x+b)}{1+exp(w*x+b)}$$

$$P(Y=0|x) = \frac{1}{1+exp(w*x+b)}$$

- 比较两个条件概率值的大小，将实例x分到概率值较大的那一类。（也可以拿其中一个概率值跟0.5比较）

- 事件的几率（odds）：事件发生的概率/事件不发生的概率

  $$logit(p) = log\frac{p}{1-p} = log\frac{P(Y=1|x)}{1-P(Y=1|x)} = w*x$$



### 策略

- 极大似然估计法

  1. 假设

     $$P(Y=1|x)=\pi(x)$$

     $$P(Y=0|x)=1-\pi(x)$$

  2. P满足二项分布，其似然函数（即抽样的期望）为

     $$\prod_{i=1}^N[\pi(x_i)]^{y_i}[1-\pi(x_i)]^{1-y_i}$$

  3. 对似然函数求对数，再代入逻辑斯蒂模型，得到

     $$L(w) = \sum_{i=1}^N[y_i(w*x_i)-log(1+exp(w*x_i))]$$

  4. 对L（w）求极大值，得到w的估计值

     

### 算法

- 梯度下降法
- 改进的迭代尺度算法
- 拟牛顿法



### 应用场景、优缺点



### 衍生

- LR可以用于回归吗
- 跟Ridge，Lasso的关系



### 最大熵模型

- 最大熵原理
  - 在所有可能的概率模型（分布）中，熵最大的模型是最好的模型
- 最大熵模型



### 多分类（多项）逻辑回归

$$P(Y=k|x) = \frac{exp(w_k*x)}{1+\sum_{k=1}^{K-1}{exp(w_k*x)}}, k=1,2,...K-1$$

$$P(Y=K|x) = \frac{1}{1+\sum_{k=1}^{K-1}{exp(w_k*x)}}$$

- 也可以应用极大似然法估计模型参数



### 代码实现





### 最大熵模型


