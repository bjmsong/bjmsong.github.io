---
layout:     post
title:      DLRM
subtitle:   
date:       2022-09-06
author:     bjmsong
header-img: 
catalog: true
tags:
    - 深度模型加速
---
## 官方文档
- https://arxiv.org/abs/1906.00091
- https://ai.facebook.com/blog/dlrm-an-advanced-open-source-deep-learning-recommendation-model/
- https://www.youtube.com/watch?v=DFrCEvPgEcQ&ab_channel=OpenComputeProject

## 解读
- 参考
    - https://zhuanlan.zhihu.com/p/82839874
    - https://www.infoq.cn/article/isbHku4p*C4KZ49JIkLy
- 模型中规中矩
    - 离散特征：embedding
    - 连续特征：通过mlp处理成跟离散特征一样的长度
    - 特征交叉：embedding两两做点积，做完之后在跟之前dense features对应的embedding concat起来
- 并行训练
    - Embedding部分采用模型并行
        - 绝大部分参数在Embedding部分
        - 在一个device或者说计算节点上，仅有一部分Embedding层参数，每个device进行并行mini-batch梯度更新时，仅更新自己节点上的部分Embedding层参数
    - MLP和interactions部分采用数据并行
        - 参数少，计算量大
        - 每个device上已经有了全部模型参数，每个device上利用部分数据计算gradient，再利用allreduce的方法汇总所有梯度进行参数更新
    - 自研：PyTorch/caffe2不支持模型并行+数据并行
- Data通过三种渠道收集
    - Random
    - synthetic
    - public data sets
- Experience
    - 训练平台：CPU+GPU

## 源码
- https://github.com/facebookresearch/dlrm
- https://zhuanlan.zhihu.com/p/480534227


## System Architecture Implication
- https://arxiv.org/abs/1906.03109
### Abstract
- 介绍了生产环境的深度学习(DNN)推荐系统，并进行了评估。开源了一些workload, 通过深入分析这些推荐系统，积累了设计大规模的生产环境推荐系统的经验：
    - Intel三代服务器(Haswell, Broadwell, Skylake)的推理效率差别可以达到60%
    - batching and co-location of inference jobs can drastically improve latency- bounded throughput
    - diversity across recommendation models leads to different optimization strategies
### Conclusion
- 对生产环境的推荐系统进行了详细分析
- 优化DNN模型的执行跟CNN/RNN有不同的挑战
    - 需要更大的存储空间
    - 不规律的内存访问
    - a diverse set of operator-level performance bottlenecks
- 
### Introduction
- 在Facebook，个性化推荐在整个AI Inference处理中占了79%（Training占了的大约占50%）。其中，三类主要的模型(RMC1, RMC2, and RMC3)占了65%
- 已经有很多的工作，致力于optimizing the performance, energy efficiency, and memory consumption of DNNs
    - efficient DNN architectures：GRU, ResNet, Mobilenets
    - reduced precision datatypes：压缩，量化
    - heavily parallelized training/inference：分布式
    - hardware accelerators
- 但是这些优化大部分针对于CNN/RNN, 无法应用到推荐系统模型上
    - 因为模型差异很大,对运算能力(FLOPS)/内存读取的需求都不一样
- 公开的推荐系统基准(MLPerf-NCF)和生产环境的推荐系统有所不同
    - 生产环境的推荐系统需要高吞吐、低延迟地处理用户请求
        - 通过数据并行、任务并行来实现
    - 生产环境的embedding维度要高几个量级：存储空间要求更高，内存读取更不规律
    - MLPerf-NCF使用的全连接层更少，更小
- 

## 参考资料
